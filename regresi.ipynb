{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f3ef054-050e-44e7-9ee5-06a8dd1ca556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdbc73df-6efa-4887-961f-7e1210ada1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>38.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>13.8</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>94.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>177.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6.4</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>283.6</td>\n",
       "      <td>42.0</td>\n",
       "      <td>66.2</td>\n",
       "      <td>25.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>232.1</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TV  Radio  Newspaper  Sales\n",
       "0    230.1   37.8       69.2   22.1\n",
       "1     44.5   39.3       45.1   10.4\n",
       "2     17.2   45.9       69.3    9.3\n",
       "3    151.5   41.3       58.5   18.5\n",
       "4    180.8   10.8       58.4   12.9\n",
       "..     ...    ...        ...    ...\n",
       "195   38.2    3.7       13.8    7.6\n",
       "196   94.2    4.9        8.1    9.7\n",
       "197  177.0    9.3        6.4   12.8\n",
       "198  283.6   42.0       66.2   25.5\n",
       "199  232.1    8.6        8.7   13.4\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Advertising.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8ec20f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV           190\n",
       "Radio        167\n",
       "Newspaper    172\n",
       "Sales        121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d8d83b5-4079-42d4-8c10-d90874a79a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cd99d23-d09b-407b-940a-5a6445c55d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ed4b4a2-7491-4da6-b056-4b4bb9511374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   TV         200 non-null    float64\n",
      " 1   Radio      200 non-null    float64\n",
      " 2   Newspaper  200 non-null    float64\n",
      " 3   Sales      200 non-null    float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 6.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33d4d1dc-84b3-4120-a0d2-de744dbbddaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>147.042500</td>\n",
       "      <td>23.264000</td>\n",
       "      <td>30.554000</td>\n",
       "      <td>14.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>85.854236</td>\n",
       "      <td>14.846809</td>\n",
       "      <td>21.778621</td>\n",
       "      <td>5.217457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.375000</td>\n",
       "      <td>9.975000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>149.750000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>12.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>218.825000</td>\n",
       "      <td>36.525000</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>17.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>296.400000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TV       Radio   Newspaper       Sales\n",
       "count  200.000000  200.000000  200.000000  200.000000\n",
       "mean   147.042500   23.264000   30.554000   14.022500\n",
       "std     85.854236   14.846809   21.778621    5.217457\n",
       "min      0.700000    0.000000    0.300000    1.600000\n",
       "25%     74.375000    9.975000   12.750000   10.375000\n",
       "50%    149.750000   22.900000   25.750000   12.900000\n",
       "75%    218.825000   36.525000   45.100000   17.400000\n",
       "max    296.400000   49.600000  114.000000   27.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd9f9867-680a-4e04-a5a2-c0ffdd46f012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Buat scatter plot utk masing masing variabel terhadap sales\n",
    "fig, axs= plt.subplots(1,3,figsize=(18,5))\n",
    "\n",
    "#Scatterplot antara TV dan Sales\n",
    "axs[0].scatter(df['TV'], df['Sales'], color='gray')\n",
    "axs[0].set_title('TV vs Sales')\n",
    "axs[0].set_xlabel('TV')\n",
    "axs[0].set_ylabel('Sales')\n",
    "\n",
    "#Scatterplot antara Radio dan Sales \n",
    "axs[1].scatter(df['Radio'], df['Sales'], color='gray')\n",
    "axs[1].set_title('Radio vs Sales')\n",
    "axs[1].set_xlabel('Radio')\n",
    "axs[1].set_ylabel('Sales')\n",
    "\n",
    "#Scatterplot antara Newspaper vs Sales\n",
    "axs[2].scatter(df['Newspaper'], df['Sales'], color='gray')\n",
    "axs[2].set_title('Newspaper vs Sales')\n",
    "axs[2].set_xlabel('Newspaper')\n",
    "axs[2].set_ylabel('Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f9609ed-1e8c-4555-a2c6-3c9168eba5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pisahkan X dan y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "315b09a0-11fc-4730-92e2-46e57415b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Sales', axis=1)\n",
    "y = df['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53ef63db-0484-4f2c-bc02-ce5f128e671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train & Test dulu \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0a7e4d7-2e73-4f7f-b9e5-ed8eb5127b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV           0\n",
       "Radio        0\n",
       "Newspaper    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8950d9d-d427-477f-aa7f-a1a9d48aacdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV           0\n",
       "Radio        0\n",
       "Newspaper    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d1c2b78-945a-4f74-9c32-83a093e57ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cf6af26-1e66-492e-8a29-36d2cd8e2e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "002ffbdd-44bc-43c9-ab00-c0e5508bb032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV: 0 data (0.00%) outlier\n",
      "Radio: 0 data (0.00%) outlier\n",
      "Newspaper: 1 data (0.62%) outlier\n"
     ]
    }
   ],
   "source": [
    "#Cek outlier\n",
    "\n",
    "Q1 = X_train.quantile(0.25)\n",
    "Q3 = X_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = ((X_train < (Q1 - 1.5 * IQR)) | (X_train > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "for col in X_train.columns:\n",
    "    jumlah = outliers[col].sum()\n",
    "    persen = (jumlah / len(X_train)) * 100\n",
    "    print(f\"{col}: {jumlah} data ({persen:.2f}%) outlier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8378f68a-0aa3-4492-ae86-4388344db14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV          -0.093150\n",
       "Radio        0.130306\n",
       "Newspaper    0.700071\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Hitung skewness setiap kolom\n",
    "X_train.apply(skew)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf8fc062-c1d1-4e2b-a722-12653fd388d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 4))\n",
    "\n",
    "for i, col in enumerate(X_train.columns):\n",
    "    plt.subplot(1, len(X_train.columns), i + 1)\n",
    "    sns.histplot(X_train[col], kde=True, color='gray')\n",
    "    plt.title(f'{col}\\nSkew: {X_train[col].skew():.2f}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b570d05e-d2a0-4231-8d61-b3a3dcf07fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sebelumnya di coba log transform skewness masih ada\n",
    "#Karena cbrt di train berpengaruuh ke X_test maka kita juga tambhakan kolom tersebut ke X_test\n",
    "X_train['Newspaper_cbrt'] = np.cbrt(X_train['Newspaper'])\n",
    "X_test['Newspaper_cbrt'] = np.cbrt(X_test['Newspaper'])\n",
    "\n",
    "#Pilih atua drop newspaper\n",
    "X_train = X_train.drop(columns='Newspaper')\n",
    "X_test = X_test.drop(columns='Newspaper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cb05b28-0444-4410-839f-54d2eb92e260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV               -0.093150\n",
       "Radio             0.130306\n",
       "Newspaper_cbrt   -0.322630\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Hitung skewness setiap kolom\n",
    "X_train.apply(skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e340a7b7-88aa-4a20-a29a-d0832f59f2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 4))\n",
    "\n",
    "for i, col in enumerate(X_train.columns):\n",
    "    plt.subplot(1, len(X_train.columns), i + 1)\n",
    "    sns.histplot(X_train[col], kde=True, color='gray')\n",
    "    plt.title(f'{col}\\nSkew: {X_train[col].skew():.2f}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Keliatan skewness sudah reda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0332d59b-0202-480b-bcd6-a4afb31bc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV: 0 data (0.00%) outlier\n",
      "Radio: 0 data (0.00%) outlier\n",
      "Newspaper_cbrt: 0 data (0.00%) outlier\n"
     ]
    }
   ],
   "source": [
    "#Cek outlier\n",
    "\n",
    "Q1 = X_train.quantile(0.25)\n",
    "Q3 = X_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = ((X_train < (Q1 - 1.5 * IQR)) | (X_train > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "for col in X_train.columns:\n",
    "    jumlah = outliers[col].sum()\n",
    "    persen = (jumlah / len(X_train)) * 100\n",
    "    print(f\"{col}: {jumlah} data ({persen:.2f}%) outlier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5b6b9f1-7bed-41b8-bcd1-961b49fb4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9632bb1f-08e4-402b-936d-fa73b93ad84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "\n",
    "X_train_scaled= scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Balik ke DataFrame\n",
    "X_train_scaled= pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns= X_test.columns, index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "caeb608c-d4a1-4633-9630-f5ce883c9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "deedf6e9-764a-41d0-bfca-2b62fee72788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(14.1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_pred = np.mean(y_train)\n",
    "baseline_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6bbf72b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Train Evaluasi ===\n",
      "MAE : 4.12\n",
      "MAPE: 0.39326262702499604\n",
      "MSE: 25.93625\n",
      "RMSE: 5.092764475213831\n",
      "R2: 0.0\n",
      "\n",
      "=== Baseline Test Evaluasi ===\n",
      "MAE : 4.865625\n",
      "MAPE: 0.4251706271807656\n",
      "MSE: 31.563593749999995\n",
      "RMSE: 5.6181486051901475\n",
      "R2: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# ===== Baseline Train =====\n",
    "baseline_pred_train_mean = np.mean(y_train)\n",
    "baseline_train_pred = np.ones(len(y_train)) * baseline_pred_train_mean\n",
    "\n",
    "baseline_train_mae = mean_absolute_error(y_train, baseline_train_pred)\n",
    "baseline_train_mape = mean_absolute_percentage_error(y_train, baseline_train_pred)\n",
    "baseline_train_mse = mean_squared_error(y_train, baseline_train_pred)\n",
    "baseline_train_rmse = np.sqrt(baseline_train_mse)\n",
    "baseline_train_r2 = r2_score(y_train, baseline_train_pred)\n",
    "\n",
    "# ===== Baseline Test =====\n",
    "baseline_pred_test_mean = np.mean(y_test)\n",
    "baseline_test_pred = np.ones(len(y_test)) * baseline_pred_test_mean\n",
    "\n",
    "baseline_test_mae = mean_absolute_error(y_test, baseline_test_pred)\n",
    "baseline_test_mape = mean_absolute_percentage_error(y_test, baseline_test_pred)\n",
    "baseline_test_mse = mean_squared_error(y_test, baseline_test_pred)\n",
    "baseline_test_rmse = np.sqrt(baseline_test_mse)\n",
    "baseline_test_r2 = r2_score(y_test, baseline_test_pred)\n",
    "\n",
    "# ===== Tampilkan Hasil =====\n",
    "print(\"=== Baseline Train Evaluasi ===\")\n",
    "print('MAE :', baseline_train_mae)\n",
    "print('MAPE:', baseline_train_mape)\n",
    "print('MSE:', baseline_train_mse)\n",
    "print('RMSE:', baseline_train_rmse)\n",
    "print('R2:', baseline_train_r2)\n",
    "\n",
    "print(\"\\n=== Baseline Test Evaluasi ===\")\n",
    "print('MAE :', baseline_test_mae)\n",
    "print('MAPE:', baseline_test_mape)\n",
    "print('MSE:', baseline_test_mse)\n",
    "print('RMSE:', baseline_test_rmse)\n",
    "print('R2:', baseline_test_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1ea51",
   "metadata": {},
   "source": [
    "# SVM (Support Vector Machine Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a586331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "SVR(\n",
      "    *,\n",
      "    kernel=\u001b[33m'rbf'\u001b[39m,\n",
      "    degree=\u001b[32m3\u001b[39m,\n",
      "    gamma=\u001b[33m'scale'\u001b[39m,\n",
      "    coef0=\u001b[32m0.0\u001b[39m,\n",
      "    tol=\u001b[32m0.001\u001b[39m,\n",
      "    C=\u001b[32m1.0\u001b[39m,\n",
      "    epsilon=\u001b[32m0.1\u001b[39m,\n",
      "    shrinking=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    cache_size=\u001b[32m200\u001b[39m,\n",
      "    verbose=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    max_iter=-\u001b[32m1\u001b[39m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "Epsilon-Support Vector Regression.\n",
      "\n",
      "The free parameters in the model are C and epsilon.\n",
      "\n",
      "The implementation is based on libsvm. The fit time complexity\n",
      "is more than quadratic with the number of samples which makes it hard\n",
      "to scale to datasets with more than a couple of 10000 samples. For large\n",
      "datasets consider using :class:`~sklearn.svm.LinearSVR` or\n",
      ":class:`~sklearn.linear_model.SGDRegressor` instead, possibly after a\n",
      ":class:`~sklearn.kernel_approximation.Nystroem` transformer or\n",
      "other :ref:`kernel_approximation`.\n",
      "\n",
      "Read more in the :ref:`User Guide <svm_regression>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n",
      "     Specifies the kernel type to be used in the algorithm.\n",
      "     If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     used to precompute the kernel matrix.\n",
      "     For an intuitive visualization of different kernel types\n",
      "     see :ref:`sphx_glr_auto_examples_svm_plot_svm_regression.py`\n",
      "\n",
      "degree : int, default=3\n",
      "    Degree of the polynomial kernel function ('poly').\n",
      "    Must be non-negative. Ignored by all other kernels.\n",
      "\n",
      "gamma : {'scale', 'auto'} or float, default='scale'\n",
      "    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "\n",
      "    - if ``gamma='scale'`` (default) is passed then it uses\n",
      "      1 / (n_features * X.var()) as value of gamma,\n",
      "    - if 'auto', uses 1 / n_features\n",
      "    - if float, must be non-negative.\n",
      "\n",
      "    .. versionchanged:: 0.22\n",
      "       The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
      "\n",
      "coef0 : float, default=0.0\n",
      "    Independent term in kernel function.\n",
      "    It is only significant in 'poly' and 'sigmoid'.\n",
      "\n",
      "tol : float, default=1e-3\n",
      "    Tolerance for stopping criterion.\n",
      "\n",
      "C : float, default=1.0\n",
      "    Regularization parameter. The strength of the regularization is\n",
      "    inversely proportional to C. Must be strictly positive.\n",
      "    The penalty is a squared l2. For an intuitive visualization of the\n",
      "    effects of scaling the regularization parameter C, see\n",
      "    :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.\n",
      "\n",
      "epsilon : float, default=0.1\n",
      "     Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n",
      "     within which no penalty is associated in the training loss function\n",
      "     with points predicted within a distance epsilon from the actual\n",
      "     value. Must be non-negative.\n",
      "\n",
      "shrinking : bool, default=True\n",
      "    Whether to use the shrinking heuristic.\n",
      "    See the :ref:`User Guide <shrinking_svm>`.\n",
      "\n",
      "cache_size : float, default=200\n",
      "    Specify the size of the kernel cache (in MB).\n",
      "\n",
      "verbose : bool, default=False\n",
      "    Enable verbose output. Note that this setting takes advantage of a\n",
      "    per-process runtime setting in libsvm that, if enabled, may not work\n",
      "    properly in a multithreaded context.\n",
      "\n",
      "max_iter : int, default=-1\n",
      "    Hard limit on iterations within solver, or -1 for no limit.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "coef_ : ndarray of shape (1, n_features)\n",
      "    Weights assigned to the features (coefficients in the primal\n",
      "    problem). This is only available in the case of a linear kernel.\n",
      "\n",
      "    `coef_` is readonly property derived from `dual_coef_` and\n",
      "    `support_vectors_`.\n",
      "\n",
      "dual_coef_ : ndarray of shape (1, n_SV)\n",
      "    Coefficients of the support vector in the decision function.\n",
      "\n",
      "fit_status_ : int\n",
      "    0 if correctly fitted, 1 otherwise (will raise warning)\n",
      "\n",
      "intercept_ : ndarray of shape (1,)\n",
      "    Constants in decision function.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_iter_ : int\n",
      "    Number of iterations run by the optimization routine to fit the model.\n",
      "\n",
      "    .. versionadded:: 1.1\n",
      "\n",
      "n_support_ : ndarray of shape (1,), dtype=int32\n",
      "    Number of support vectors.\n",
      "\n",
      "shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
      "    Array dimensions of training vector ``X``.\n",
      "\n",
      "support_ : ndarray of shape (n_SV,)\n",
      "    Indices of support vectors.\n",
      "\n",
      "support_vectors_ : ndarray of shape (n_SV, n_features)\n",
      "    Support vectors.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "NuSVR : Support Vector Machine for regression implemented using libsvm\n",
      "    using a parameter to control the number of support vectors.\n",
      "\n",
      "LinearSVR : Scalable Linear Support Vector Machine for regression\n",
      "    implemented using liblinear.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] `LIBSVM: A Library for Support Vector Machines\n",
      "    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
      "\n",
      ".. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n",
      "    Machines and Comparisons to Regularized Likelihood Methods\"\n",
      "    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.svm import SVR\n",
      ">>> from sklearn.pipeline import make_pipeline\n",
      ">>> from sklearn.preprocessing import StandardScaler\n",
      ">>> import numpy as np\n",
      ">>> n_samples, n_features = 10, 5\n",
      ">>> rng = np.random.RandomState(0)\n",
      ">>> y = rng.randn(n_samples)\n",
      ">>> X = rng.randn(n_samples, n_features)\n",
      ">>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
      ">>> regr.fit(X, y)\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svr', SVR(epsilon=0.2))])\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\asus\\anaconda3\\envs\\clustering_env_clean\\lib\\site-packages\\sklearn\\svm\\_classes.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "#Default SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "?SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7673252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(C=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVR</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.svm.SVR.html\">?<span>Documentation for SVR</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">kernel&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;rbf&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('degree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">degree&nbsp;</td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('gamma',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">gamma&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;scale&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('coef0',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">coef0&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('epsilon',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">epsilon&nbsp;</td>\n",
       "            <td class=\"value\">0.1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('shrinking',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">shrinking&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cache_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">cache_size&nbsp;</td>\n",
       "            <td class=\"value\">200</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "SVR(C=1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Default SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_model = SVR(kernel='rbf', C=1, epsilon=0.1)\n",
    "svr_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0953af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediksi pada data training dan data testing\n",
    "y_train_pred_svr = svr_model.predict(X_train_scaled)\n",
    "y_test_pred_svr = svr_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "114476de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVR Train ===\n",
      "MAE: 0.8116445865138306\n",
      "MAPE: 11.03115428020934\n",
      "MSE: 2.2947766227028796\n",
      "RMSE: 1.5148520134662922\n",
      "R2: 0.9115224204461756\n",
      "\n",
      "=== SVR Test ===\n",
      "MAE: 1.2175234014922005\n",
      "MAPE: 12.899385188928111\n",
      "MSE: 3.3742268471216335\n",
      "RMSE: 1.8369068694742348\n",
      "R2: 0.8930975074052956\n",
      "R2 selisih: 0.01842491304088001\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "# === Train ===\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred_svr)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred_svr)\n",
    "rmse_train = sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_train_pred_svr)\n",
    "mape_train = np.mean(np.abs((y_train - y_train_pred_svr) / y_train)) * 100\n",
    "\n",
    "# === Test ===\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred_svr)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred_svr)\n",
    "rmse_test = sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_test_pred_svr)\n",
    "mape_test = np.mean(np.abs((y_test - y_test_pred_svr) / y_test)) * 100\n",
    "\n",
    "# Selisih R2\n",
    "r2_selisih = r2_train - r2_test\n",
    "\n",
    "# Print hasil\n",
    "print(\"=== SVR Train ===\")\n",
    "print(\"MAE:\", mae_train)\n",
    "print(\"MAPE:\", mape_train)\n",
    "print(\"MSE:\", mse_train)\n",
    "print(\"RMSE:\", rmse_train)\n",
    "print(\"R2:\", r2_train)\n",
    "\n",
    "print(\"\\n=== SVR Test ===\")\n",
    "print(\"MAE:\", mae_test)\n",
    "print(\"MAPE:\", mape_test)\n",
    "print(\"MSE:\", mse_test)\n",
    "print(\"RMSE:\", rmse_test)\n",
    "print(\"R2:\", r2_test)\n",
    "print(\"R2 selisih:\", r2_selisih)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9569f",
   "metadata": {},
   "source": [
    "| Parameter   | Rule of Thumb                               |\n",
    "| ----------- | ------------------------------------------- |\n",
    "| **gamma**   | \\~ 1 / (p \\* Var(X)) atau `'scale'`         |\n",
    "| **epsilon** | \\~ range(y) / sqrt(n)                       |\n",
    "| **C**       | dataset kecil â†’ 0.1â€“1, dataset besar â†’ 1â€“10 |\n",
    "| kernel      | `rbf` default, linear kalau p >> n          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e9f72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# Parameter grid manual (rule-of-thumb)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [2, 4, 8],\n",
    "    'gamma': ['scale', 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# List untuk menyimpan semua hasil\n",
    "results_list = []\n",
    "\n",
    "# Loop semua kombinasi\n",
    "for C_val, epsilon_val, gamma_val in product(param_grid['C'], param_grid['epsilon'], param_grid['gamma']):\n",
    "    svr = SVR(kernel='rbf', C=C_val, epsilon=epsilon_val, gamma=gamma_val)\n",
    "    svr.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prediksi\n",
    "    y_train_pred = svr.predict(X_train_scaled)\n",
    "    y_test_pred = svr.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluasi\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_train = sqrt(mse_train)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    r2_selisih = r2_train - r2_test\n",
    "    \n",
    "    # Simpan hasil\n",
    "    results_list.append({\n",
    "        'C': C_val,\n",
    "        'epsilon': epsilon_val,\n",
    "        'gamma': gamma_val,\n",
    "        'MAE_train': mae_train,\n",
    "        'MAE_test': mae_test,\n",
    "        'MAPE_train': mape_train,\n",
    "        'MAPE_test': mape_test,\n",
    "        'MSE_train': mse_train,\n",
    "        'MSE_test': mse_test,\n",
    "        'RMSE_train': rmse_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'R2_train': r2_train,\n",
    "        'R2_test': r2_test,\n",
    "        'R2_selisih': r2_selisih\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b96ab313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>gamma</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.064649</td>\n",
       "      <td>1.111496</td>\n",
       "      <td>10.285420</td>\n",
       "      <td>9.276836</td>\n",
       "      <td>1.583362</td>\n",
       "      <td>1.717821</td>\n",
       "      <td>1.258317</td>\n",
       "      <td>1.310657</td>\n",
       "      <td>0.938952</td>\n",
       "      <td>0.945576</td>\n",
       "      <td>-0.006624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>scale</td>\n",
       "      <td>1.038649</td>\n",
       "      <td>1.228983</td>\n",
       "      <td>10.361203</td>\n",
       "      <td>10.640363</td>\n",
       "      <td>1.576401</td>\n",
       "      <td>2.015055</td>\n",
       "      <td>1.255548</td>\n",
       "      <td>1.419526</td>\n",
       "      <td>0.939220</td>\n",
       "      <td>0.936159</td>\n",
       "      <td>0.003061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.175417</td>\n",
       "      <td>1.484098</td>\n",
       "      <td>12.907411</td>\n",
       "      <td>14.584435</td>\n",
       "      <td>2.608440</td>\n",
       "      <td>3.373033</td>\n",
       "      <td>1.615066</td>\n",
       "      <td>1.836582</td>\n",
       "      <td>0.899429</td>\n",
       "      <td>0.893135</td>\n",
       "      <td>0.006293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.619598</td>\n",
       "      <td>1.635998</td>\n",
       "      <td>15.467767</td>\n",
       "      <td>14.415510</td>\n",
       "      <td>3.968636</td>\n",
       "      <td>4.198588</td>\n",
       "      <td>1.992144</td>\n",
       "      <td>2.049046</td>\n",
       "      <td>0.846985</td>\n",
       "      <td>0.866980</td>\n",
       "      <td>-0.019995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>scale</td>\n",
       "      <td>1.556639</td>\n",
       "      <td>1.668896</td>\n",
       "      <td>14.646213</td>\n",
       "      <td>14.451638</td>\n",
       "      <td>3.821092</td>\n",
       "      <td>4.218916</td>\n",
       "      <td>1.954761</td>\n",
       "      <td>2.054000</td>\n",
       "      <td>0.852674</td>\n",
       "      <td>0.866336</td>\n",
       "      <td>-0.013662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.196584</td>\n",
       "      <td>1.609301</td>\n",
       "      <td>14.379806</td>\n",
       "      <td>16.380141</td>\n",
       "      <td>3.259242</td>\n",
       "      <td>4.517289</td>\n",
       "      <td>1.805337</td>\n",
       "      <td>2.125392</td>\n",
       "      <td>0.874336</td>\n",
       "      <td>0.856883</td>\n",
       "      <td>0.017453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>scale</td>\n",
       "      <td>1.305188</td>\n",
       "      <td>1.707587</td>\n",
       "      <td>15.783805</td>\n",
       "      <td>17.792458</td>\n",
       "      <td>3.759520</td>\n",
       "      <td>5.294108</td>\n",
       "      <td>1.938948</td>\n",
       "      <td>2.300893</td>\n",
       "      <td>0.855048</td>\n",
       "      <td>0.832272</td>\n",
       "      <td>0.022776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.840500</td>\n",
       "      <td>2.154830</td>\n",
       "      <td>19.108959</td>\n",
       "      <td>20.380819</td>\n",
       "      <td>5.586921</td>\n",
       "      <td>7.035692</td>\n",
       "      <td>2.363667</td>\n",
       "      <td>2.652488</td>\n",
       "      <td>0.784590</td>\n",
       "      <td>0.777095</td>\n",
       "      <td>0.007496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.112788</td>\n",
       "      <td>2.592731</td>\n",
       "      <td>22.925728</td>\n",
       "      <td>25.474987</td>\n",
       "      <td>7.430061</td>\n",
       "      <td>9.576565</td>\n",
       "      <td>2.725814</td>\n",
       "      <td>3.094603</td>\n",
       "      <td>0.713526</td>\n",
       "      <td>0.696595</td>\n",
       "      <td>0.016931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>scale</td>\n",
       "      <td>2.214813</td>\n",
       "      <td>2.734568</td>\n",
       "      <td>24.866055</td>\n",
       "      <td>27.761788</td>\n",
       "      <td>8.293360</td>\n",
       "      <td>11.054456</td>\n",
       "      <td>2.879819</td>\n",
       "      <td>3.324824</td>\n",
       "      <td>0.680241</td>\n",
       "      <td>0.649772</td>\n",
       "      <td>0.030469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.873373</td>\n",
       "      <td>3.169778</td>\n",
       "      <td>28.145788</td>\n",
       "      <td>29.010649</td>\n",
       "      <td>12.165273</td>\n",
       "      <td>13.758323</td>\n",
       "      <td>3.487875</td>\n",
       "      <td>3.709221</td>\n",
       "      <td>0.530955</td>\n",
       "      <td>0.564108</td>\n",
       "      <td>-0.033153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.675625</td>\n",
       "      <td>3.260036</td>\n",
       "      <td>26.020821</td>\n",
       "      <td>29.232436</td>\n",
       "      <td>11.721135</td>\n",
       "      <td>14.667129</td>\n",
       "      <td>3.423614</td>\n",
       "      <td>3.829769</td>\n",
       "      <td>0.548079</td>\n",
       "      <td>0.535315</td>\n",
       "      <td>0.012764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>scale</td>\n",
       "      <td>2.894374</td>\n",
       "      <td>3.340898</td>\n",
       "      <td>28.922549</td>\n",
       "      <td>31.156324</td>\n",
       "      <td>12.944623</td>\n",
       "      <td>15.171156</td>\n",
       "      <td>3.597864</td>\n",
       "      <td>3.895017</td>\n",
       "      <td>0.500906</td>\n",
       "      <td>0.519346</td>\n",
       "      <td>-0.018440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.206179</td>\n",
       "      <td>3.876779</td>\n",
       "      <td>31.959099</td>\n",
       "      <td>35.910851</td>\n",
       "      <td>15.619166</td>\n",
       "      <td>19.408811</td>\n",
       "      <td>3.952109</td>\n",
       "      <td>4.405543</td>\n",
       "      <td>0.397786</td>\n",
       "      <td>0.385089</td>\n",
       "      <td>0.012698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>scale</td>\n",
       "      <td>3.043951</td>\n",
       "      <td>3.745593</td>\n",
       "      <td>29.393019</td>\n",
       "      <td>33.329121</td>\n",
       "      <td>16.049945</td>\n",
       "      <td>19.894027</td>\n",
       "      <td>4.006238</td>\n",
       "      <td>4.460272</td>\n",
       "      <td>0.381177</td>\n",
       "      <td>0.369716</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.152642</td>\n",
       "      <td>3.827868</td>\n",
       "      <td>30.183449</td>\n",
       "      <td>33.988658</td>\n",
       "      <td>16.320903</td>\n",
       "      <td>20.107007</td>\n",
       "      <td>4.039914</td>\n",
       "      <td>4.484084</td>\n",
       "      <td>0.370730</td>\n",
       "      <td>0.362968</td>\n",
       "      <td>0.007762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.391474</td>\n",
       "      <td>4.052310</td>\n",
       "      <td>35.033413</td>\n",
       "      <td>39.129177</td>\n",
       "      <td>16.478554</td>\n",
       "      <td>20.319539</td>\n",
       "      <td>4.059379</td>\n",
       "      <td>4.507720</td>\n",
       "      <td>0.364652</td>\n",
       "      <td>0.356235</td>\n",
       "      <td>0.008417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.309687</td>\n",
       "      <td>4.000858</td>\n",
       "      <td>34.026157</td>\n",
       "      <td>38.215529</td>\n",
       "      <td>16.539294</td>\n",
       "      <td>20.424341</td>\n",
       "      <td>4.066853</td>\n",
       "      <td>4.519330</td>\n",
       "      <td>0.362310</td>\n",
       "      <td>0.352915</td>\n",
       "      <td>0.009395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>scale</td>\n",
       "      <td>3.304772</td>\n",
       "      <td>4.024834</td>\n",
       "      <td>33.734014</td>\n",
       "      <td>38.122948</td>\n",
       "      <td>16.672752</td>\n",
       "      <td>20.836184</td>\n",
       "      <td>4.083228</td>\n",
       "      <td>4.564667</td>\n",
       "      <td>0.357164</td>\n",
       "      <td>0.339867</td>\n",
       "      <td>0.017298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.556688</td>\n",
       "      <td>4.344153</td>\n",
       "      <td>35.294426</td>\n",
       "      <td>40.093763</td>\n",
       "      <td>19.488819</td>\n",
       "      <td>24.192374</td>\n",
       "      <td>4.414614</td>\n",
       "      <td>4.918574</td>\n",
       "      <td>0.248588</td>\n",
       "      <td>0.233535</td>\n",
       "      <td>0.015052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C  epsilon  gamma  MAE_train  MAE_test  MAPE_train  MAPE_test  \\\n",
       "0   10.0        2    0.1   1.064649  1.111496   10.285420   9.276836   \n",
       "1   10.0        2  scale   1.038649  1.228983   10.361203  10.640363   \n",
       "2   10.0        2   0.01   1.175417  1.484098   12.907411  14.584435   \n",
       "3   10.0        4    0.1   1.619598  1.635998   15.467767  14.415510   \n",
       "4   10.0        4  scale   1.556639  1.668896   14.646213  14.451638   \n",
       "5    1.0        2    0.1   1.196584  1.609301   14.379806  16.380141   \n",
       "6    1.0        2  scale   1.305188  1.707587   15.783805  17.792458   \n",
       "7   10.0        4   0.01   1.840500  2.154830   19.108959  20.380819   \n",
       "8    1.0        4    0.1   2.112788  2.592731   22.925728  25.474987   \n",
       "9    1.0        4  scale   2.214813  2.734568   24.866055  27.761788   \n",
       "10  10.0        8    0.1   2.873373  3.169778   28.145788  29.010649   \n",
       "11   1.0        2   0.01   2.675625  3.260036   26.020821  29.232436   \n",
       "12  10.0        8  scale   2.894374  3.340898   28.922549  31.156324   \n",
       "13   1.0        4   0.01   3.206179  3.876779   31.959099  35.910851   \n",
       "14   0.1        2  scale   3.043951  3.745593   29.393019  33.329121   \n",
       "15   0.1        2    0.1   3.152642  3.827868   30.183449  33.988658   \n",
       "16  10.0        8   0.01   3.391474  4.052310   35.033413  39.129177   \n",
       "17   1.0        8    0.1   3.309687  4.000858   34.026157  38.215529   \n",
       "18   1.0        8  scale   3.304772  4.024834   33.734014  38.122948   \n",
       "19   0.1        4    0.1   3.556688  4.344153   35.294426  40.093763   \n",
       "\n",
       "    MSE_train   MSE_test  RMSE_train  RMSE_test  R2_train   R2_test  \\\n",
       "0    1.583362   1.717821    1.258317   1.310657  0.938952  0.945576   \n",
       "1    1.576401   2.015055    1.255548   1.419526  0.939220  0.936159   \n",
       "2    2.608440   3.373033    1.615066   1.836582  0.899429  0.893135   \n",
       "3    3.968636   4.198588    1.992144   2.049046  0.846985  0.866980   \n",
       "4    3.821092   4.218916    1.954761   2.054000  0.852674  0.866336   \n",
       "5    3.259242   4.517289    1.805337   2.125392  0.874336  0.856883   \n",
       "6    3.759520   5.294108    1.938948   2.300893  0.855048  0.832272   \n",
       "7    5.586921   7.035692    2.363667   2.652488  0.784590  0.777095   \n",
       "8    7.430061   9.576565    2.725814   3.094603  0.713526  0.696595   \n",
       "9    8.293360  11.054456    2.879819   3.324824  0.680241  0.649772   \n",
       "10  12.165273  13.758323    3.487875   3.709221  0.530955  0.564108   \n",
       "11  11.721135  14.667129    3.423614   3.829769  0.548079  0.535315   \n",
       "12  12.944623  15.171156    3.597864   3.895017  0.500906  0.519346   \n",
       "13  15.619166  19.408811    3.952109   4.405543  0.397786  0.385089   \n",
       "14  16.049945  19.894027    4.006238   4.460272  0.381177  0.369716   \n",
       "15  16.320903  20.107007    4.039914   4.484084  0.370730  0.362968   \n",
       "16  16.478554  20.319539    4.059379   4.507720  0.364652  0.356235   \n",
       "17  16.539294  20.424341    4.066853   4.519330  0.362310  0.352915   \n",
       "18  16.672752  20.836184    4.083228   4.564667  0.357164  0.339867   \n",
       "19  19.488819  24.192374    4.414614   4.918574  0.248588  0.233535   \n",
       "\n",
       "    R2_selisih  \n",
       "0    -0.006624  \n",
       "1     0.003061  \n",
       "2     0.006293  \n",
       "3    -0.019995  \n",
       "4    -0.013662  \n",
       "5     0.017453  \n",
       "6     0.022776  \n",
       "7     0.007496  \n",
       "8     0.016931  \n",
       "9     0.030469  \n",
       "10   -0.033153  \n",
       "11    0.012764  \n",
       "12   -0.018440  \n",
       "13    0.012698  \n",
       "14    0.011461  \n",
       "15    0.007762  \n",
       "16    0.008417  \n",
       "17    0.009395  \n",
       "18    0.017298  \n",
       "19    0.015052  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buat dataframe hasil\n",
    "df_svr = pd.DataFrame(results_list)\n",
    "\n",
    "# Tampilkan 5 kombinasi terbaik berdasarkan R2_test\n",
    "df_svr.sort_values('R2_test', ascending=False).head(20).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "30386a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index 0 adalah yang terbaik di SVR karena MAE test terendah (1.111496), MAPE test terendah (9.276836),  RMSE test (1.310657), R2 test tertinggi (0.945576), meski selisihnya bukan yang paling rendah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "612be648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearch karena dataset masih kecil 1-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5fad2b84-084f-4da1-9906-805c01eea40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30136038-15bd-4b3f-a76b-57f1bd94c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "abc465a9-52a5-4c55-8aec-4385bf1027d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "LinearRegression(\n",
      "    *,\n",
      "    fit_intercept=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    copy_X=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    tol=\u001b[32m1e-06\u001b[39m,\n",
      "    n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    positive=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "Ordinary least squares Linear Regression.\n",
      "\n",
      "LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      "to minimize the residual sum of squares between the observed targets in\n",
      "the dataset, and the targets predicted by the linear approximation.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "fit_intercept : bool, default=True\n",
      "    Whether to calculate the intercept for this model. If set\n",
      "    to False, no intercept will be used in calculations\n",
      "    (i.e. data is expected to be centered).\n",
      "\n",
      "copy_X : bool, default=True\n",
      "    If True, X will be copied; else, it may be overwritten.\n",
      "\n",
      "tol : float, default=1e-6\n",
      "    The precision of the solution (`coef_`) is determined by `tol` which\n",
      "    specifies a different convergence criterion for the `lsqr` solver.\n",
      "    `tol` is set as `atol` and `btol` of `scipy.sparse.linalg.lsqr` when\n",
      "    fitting on sparse training data. This parameter has no effect when fitting\n",
      "    on dense data.\n",
      "\n",
      "    .. versionadded:: 1.7\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    The number of jobs to use for the computation. This will only provide\n",
      "    speedup in case of sufficiently large problems, that is if firstly\n",
      "    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n",
      "    to `True`. ``None`` means 1 unless in a\n",
      "    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "    processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "\n",
      "positive : bool, default=False\n",
      "    When set to ``True``, forces the coefficients to be positive. This\n",
      "    option is only supported for dense arrays.\n",
      "\n",
      "    For a comparison between a linear regression model with positive constraints\n",
      "    on the regression coefficients and a linear regression without such constraints,\n",
      "    see :ref:`sphx_glr_auto_examples_linear_model_plot_nnls.py`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      "    Estimated coefficients for the linear regression problem.\n",
      "    If multiple targets are passed during the fit (y 2D), this\n",
      "    is a 2D array of shape (n_targets, n_features), while if only\n",
      "    one target is passed, this is a 1D array of length n_features.\n",
      "\n",
      "rank_ : int\n",
      "    Rank of matrix `X`. Only available when `X` is dense.\n",
      "\n",
      "singular_ : array of shape (min(X, y),)\n",
      "    Singular values of `X`. Only available when `X` is dense.\n",
      "\n",
      "intercept_ : float or array of shape (n_targets,)\n",
      "    Independent term in the linear model. Set to 0.0 if\n",
      "    `fit_intercept = False`.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "See Also\n",
      "--------\n",
      "Ridge : Ridge regression addresses some of the\n",
      "    problems of Ordinary Least Squares by imposing a penalty on the\n",
      "    size of the coefficients with l2 regularization.\n",
      "Lasso : The Lasso is a linear model that estimates\n",
      "    sparse coefficients with l1 regularization.\n",
      "ElasticNet : Elastic-Net is a linear regression\n",
      "    model trained with both l1 and l2 -norm regularization of the\n",
      "    coefficients.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "From the implementation point of view, this is just plain Ordinary\n",
      "Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
      "(scipy.optimize.nnls) wrapped as a predictor object.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> import numpy as np\n",
      ">>> from sklearn.linear_model import LinearRegression\n",
      ">>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      ">>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      ">>> y = np.dot(X, np.array([1, 2])) + 3\n",
      ">>> reg = LinearRegression().fit(X, y)\n",
      ">>> reg.score(X, y)\n",
      "1.0\n",
      ">>> reg.coef_\n",
      "array([1., 2.])\n",
      ">>> reg.intercept_\n",
      "np.float64(3.0)\n",
      ">>> reg.predict(np.array([[3, 5]]))\n",
      "array([16.])\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\asus\\anaconda3\\envs\\clustering_env_clean\\lib\\site-packages\\sklearn\\linear_model\\_base.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "?LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "994bf308-1fb0-42eb-a8ed-ea6b37eb4700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy_X',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy_X&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">1e-06</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('positive',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">positive&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buat model Linear Regression biasa\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f4841a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediksi pada data training dan data testing\n",
    "y_train_pred_linreg = linreg.predict(X_train_scaled)\n",
    "y_test_pred_linreg = linreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7de21676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Linear Regression Train ==\n",
      "MAE : 1.1998932527386414\n",
      "MAPE: 0.13650808035036396\n",
      "MSE : 2.7017636973937265\n",
      "RMSE: 1.6437042609282628\n",
      "R2  : 0.8958305962737972\n",
      "\n",
      "== Linear Regression Test ==\n",
      "MAE : 1.4658248899137323\n",
      "MAPE: 0.15206156168565768\n",
      "MSE : 3.171732680591278\n",
      "RMSE: 0.8995129418496182\n",
      "R2  : 0.8995129418496182\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "#Evaluasi pada data training \n",
    "mae_train_linreg = mean_absolute_error(y_train, y_train_pred_linreg)\n",
    "mape_train_linreg = mean_absolute_percentage_error(y_train, y_train_pred_linreg)\n",
    "mse_train_linreg = mean_squared_error(y_train, y_train_pred_linreg)\n",
    "rmse_train_linreg = sqrt(mse_train_linreg)\n",
    "r2_train_linreg = r2_score(y_train, y_train_pred_linreg)\n",
    "\n",
    "#Evaluasi Test\n",
    "mae_test_linreg = mean_absolute_error(y_test, y_test_pred_linreg)\n",
    "mape_test_linreg = mean_absolute_percentage_error(y_test, y_test_pred_linreg)\n",
    "mse_test_linreg= mean_squared_error(y_test, y_test_pred_linreg)\n",
    "rmse_test_linreg= r2_score(y_test, y_test_pred_linreg)\n",
    "r2_test_linreg = r2_score(y_test, y_test_pred_linreg)\n",
    "\n",
    "# ===== Tampilkan hasil =====\n",
    "print(\"== Linear Regression Train ==\")\n",
    "print(\"MAE :\", mae_train_linreg)\n",
    "print(\"MAPE:\", mape_train_linreg)\n",
    "print(\"MSE :\", mse_train_linreg)\n",
    "print(\"RMSE:\", rmse_train_linreg)\n",
    "print(\"R2  :\", r2_train_linreg)\n",
    "\n",
    "print(\"\\n== Linear Regression Test ==\")\n",
    "print(\"MAE :\", mae_test_linreg)\n",
    "print(\"MAPE:\", mape_test_linreg)\n",
    "print(\"MSE :\", mse_test_linreg)\n",
    "print(\"RMSE:\", rmse_test_linreg)\n",
    "print(\"R2  :\", r2_test_linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84e8faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Peningkatan Train vs Baseline (dalam %) ==\n",
      "MAE Improvement : 70.88%\n",
      "MAPE Improvement: 65.29%\n",
      "RMSE Improvement: 67.72%\n",
      "\n",
      "== Peningkatan Test vs Baseline (dalam %) ==\n",
      "MAE Improvement : 69.87%\n",
      "MAPE Improvement: 64.24%\n",
      "RMSE Improvement: 83.99%\n"
     ]
    }
   ],
   "source": [
    "# === Hitung Improvement Train ===\n",
    "mae_improve_train  = (baseline_train_mae - mae_train_linreg) / baseline_train_mae * 100\n",
    "mape_improve_train = (baseline_train_mape - mape_train_linreg) / baseline_train_mape * 100\n",
    "rmse_improve_train = (baseline_train_rmse - rmse_train_linreg) / baseline_train_rmse * 100\n",
    "r2_improve_train   = (r2_train_linreg - baseline_train_r2) / (abs(baseline_train_r2) + 1e-10) * 100\n",
    "\n",
    "# === Hitung Improvement Test ===\n",
    "mae_improve_test  = (baseline_test_mae - mae_test_linreg) / baseline_test_mae * 100\n",
    "mape_improve_test = (baseline_test_mape - mape_test_linreg) / baseline_test_mape * 100\n",
    "rmse_improve_test = (baseline_test_rmse - rmse_test_linreg) / baseline_test_rmse * 100\n",
    "r2_improve_test   = (r2_test_linreg - baseline_test_r2) / (abs(baseline_test_r2) + 1e-10) * 100\n",
    "\n",
    "# === Print hasil ===\n",
    "print(\"== Peningkatan Train vs Baseline (dalam %) ==\")\n",
    "print(f\"MAE Improvement : {mae_improve_train:.2f}%\")\n",
    "print(f\"MAPE Improvement: {mape_improve_train:.2f}%\")\n",
    "print(f\"RMSE Improvement: {rmse_improve_train:.2f}%\")\n",
    "\n",
    "\n",
    "print(\"\\n== Peningkatan Test vs Baseline (dalam %) ==\")\n",
    "print(f\"MAE Improvement : {mae_improve_test:.2f}%\")\n",
    "print(f\"MAPE Improvement: {mape_improve_test:.2f}%\")\n",
    "print(f\"RMSE Improvement: {rmse_improve_test:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f89ecf",
   "metadata": {},
   "source": [
    "# Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62abb532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fitur -> MAE Train: 2.5831, MAE Test: 2.4444, MAPE Train: 21.08%, MAPE Test: 18.67%, MSE Train: 10.6036, MSE Test: 10.2047, RMSE Train: 3.2563, RMSE Test: 3.1945, R2 Train: 0.5912, R2 Test: 0.6767, Fitur: ['TV']\n",
      "2 fitur -> MAE Train: 1.1965, MAE Test: 1.4443, MAPE Train: 13.66%, MAPE Test: 15.09%, MSE Train: 2.7078, MSE Test: 3.1379, RMSE Train: 1.6455, RMSE Test: 1.7714, R2 Train: 0.8956, R2 Test: 0.9006, Fitur: ['TV', 'Radio']\n",
      "3 fitur -> MAE Train: 1.1999, MAE Test: 1.4658, MAPE Train: 13.65%, MAPE Test: 15.21%, MSE Train: 2.7018, MSE Test: 3.1717, RMSE Train: 1.6437, RMSE Test: 1.7809, R2 Train: 0.8958, R2 Test: 0.8995, Fitur: ['TV', 'Radio', 'Newspaper_cbrt']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# List utk menyimpan metric\n",
    "mae_train_list, mape_train_list, mse_train_list, rmse_train_list, r2_train_list = [], [], [], [], []\n",
    "mae_test_list, mape_test_list, mse_test_list, rmse_test_list, r2_test_list = [], [], [], [], []\n",
    "selected_features_list = []\n",
    "\n",
    "total_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Forward Selection looping\n",
    "for k in range(1, total_features):\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        LinearRegression(),\n",
    "        n_features_to_select=k,\n",
    "        direction='forward'\n",
    "    )\n",
    "    sfs.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Fitur Terpilih\n",
    "    selected_features = X_train.columns[sfs.get_support()]\n",
    "    selected_features_list.append(list(selected_features))\n",
    "    \n",
    "    # Latih model dengan fitur terpilih\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled[selected_features], y_train)\n",
    "    \n",
    "    # Evaluasi train\n",
    "    y_train_pred = lr.predict(X_train_scaled[selected_features])\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = sqrt(mse_train)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    mae_train_list.append(mae_train)\n",
    "    mape_train_list.append(mape_train)\n",
    "    mse_train_list.append(mse_train)\n",
    "    rmse_train_list.append(rmse_train)\n",
    "    r2_train_list.append(r2_train)\n",
    "    \n",
    "    # Evaluasi test\n",
    "    y_test_pred = lr.predict(X_test_scaled[selected_features])\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    mae_test_list.append(mae_test)\n",
    "    mape_test_list.append(mape_test)\n",
    "    mse_test_list.append(mse_test)\n",
    "    rmse_test_list.append(rmse_test)\n",
    "    r2_test_list.append(r2_test)\n",
    "\n",
    "# Model full fitur\n",
    "lr_full = LinearRegression()\n",
    "lr_full.fit(X_train_scaled, y_train)\n",
    "selected_features_list.append(list(X_train.columns))\n",
    "\n",
    "y_train_pred = lr_full.predict(X_train_scaled)\n",
    "y_test_pred = lr_full.predict(X_test_scaled)\n",
    "\n",
    "mae_train_list.append(mean_absolute_error(y_train, y_train_pred))\n",
    "mape_train_list.append(np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100)\n",
    "mse_train_list.append(mean_squared_error(y_train, y_train_pred))\n",
    "rmse_train_list.append(sqrt(mse_train_list[-1]))\n",
    "r2_train_list.append(r2_score(y_train, y_train_pred))\n",
    "\n",
    "mae_test_list.append(mean_absolute_error(y_test, y_test_pred))\n",
    "mape_test_list.append(np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100)\n",
    "mse_test_list.append(mean_squared_error(y_test, y_test_pred))\n",
    "rmse_test_list.append(sqrt(mse_test_list[-1]))\n",
    "r2_test_list.append(r2_score(y_test, y_test_pred))\n",
    "\n",
    "# Tampilkan hasil\n",
    "for i, feats in enumerate(selected_features_list, 1):\n",
    "    print(f\"{i} fitur -> \"\n",
    "          f\"MAE Train: {mae_train_list[i-1]:.4f}, MAE Test: {mae_test_list[i-1]:.4f}, \"\n",
    "          f\"MAPE Train: {mape_train_list[i-1]:.2f}%, MAPE Test: {mape_test_list[i-1]:.2f}%, \"\n",
    "          f\"MSE Train: {mse_train_list[i-1]:.4f}, MSE Test: {mse_test_list[i-1]:.4f}, \"\n",
    "          f\"RMSE Train: {rmse_train_list[i-1]:.4f}, RMSE Test: {rmse_test_list[i-1]:.4f}, \"\n",
    "          f\"R2 Train: {r2_train_list[i-1]:.4f}, R2 Test: {r2_test_list[i-1]:.4f}, \"\n",
    "          f\"Fitur: {feats}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a032f",
   "metadata": {},
   "source": [
    "# Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf0aa6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 fitur -> MAE Train: 1.1999, MAE Test: 1.4658, MAPE Train: 13.65%, MAPE Test: 15.21%, MSE Train: 2.7018, MSE Test: 3.1717, RMSE Train: 1.6437, RMSE Test: 1.7809, R2 Train: 0.8958, R2 Test: 0.8995, Fitur: ['TV', 'Radio', 'Newspaper_cbrt']\n",
      "2 fitur -> MAE Train: 1.1965, MAE Test: 1.4443, MAPE Train: 13.66%, MAPE Test: 15.09%, MSE Train: 2.7078, MSE Test: 3.1379, RMSE Train: 1.6455, RMSE Test: 1.7714, R2 Train: 0.8956, R2 Test: 0.9006, Fitur: ['TV', 'Radio']\n",
      "1 fitur -> MAE Train: 2.5831, MAE Test: 2.4444, MAPE Train: 21.08%, MAPE Test: 18.67%, MSE Train: 10.6036, MSE Test: 10.2047, RMSE Train: 3.2563, RMSE Test: 3.1945, R2 Train: 0.5912, R2 Test: 0.6767, Fitur: ['TV']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# List untuk menyimpan metric\n",
    "mae_train_list, mape_train_list, mse_train_list, rmse_train_list, r2_train_list = [], [], [], [], []\n",
    "mae_test_list, mape_test_list, mse_test_list, rmse_test_list, r2_test_list = [], [], [], [], []\n",
    "selected_features_list = []\n",
    "\n",
    "total_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Backward elimination mulai dari total_features-1 sampai 1\n",
    "for k in range(total_features - 1, 0, -1):\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        LinearRegression(),\n",
    "        n_features_to_select=k,\n",
    "        direction='backward'\n",
    "    )\n",
    "    \n",
    "    sfs.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Fitur terpilih\n",
    "    selected_features = X_train_scaled.columns[sfs.get_support()]\n",
    "    selected_features_list.append(list(selected_features))\n",
    "\n",
    "    # Train model\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled[selected_features], y_train)\n",
    "\n",
    "    # Evaluasi train\n",
    "    y_train_pred = lr.predict(X_train_scaled[selected_features])\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = sqrt(mse_train)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    mae_train_list.append(mae_train)\n",
    "    mape_train_list.append(mape_train)\n",
    "    mse_train_list.append(mse_train)\n",
    "    rmse_train_list.append(rmse_train)\n",
    "    r2_train_list.append(r2_train)\n",
    "\n",
    "    # Evaluasi test\n",
    "    y_test_pred = lr.predict(X_test_scaled[selected_features])\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    mae_test_list.append(mae_test)\n",
    "    mape_test_list.append(mape_test)\n",
    "    mse_test_list.append(mse_test)\n",
    "    rmse_test_list.append(rmse_test)\n",
    "    r2_test_list.append(r2_test)\n",
    "\n",
    "# Tambahkan model full fitur awal\n",
    "lr_full = LinearRegression()\n",
    "lr_full.fit(X_train_scaled, y_train)\n",
    "selected_features_list.insert(0, list(X_train_scaled.columns))\n",
    "\n",
    "y_train_pred = lr_full.predict(X_train_scaled)\n",
    "y_test_pred = lr_full.predict(X_test_scaled)\n",
    "\n",
    "mae_train_list.insert(0, mean_absolute_error(y_train, y_train_pred))\n",
    "mape_train_list.insert(0, np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100)\n",
    "mse_train_list.insert(0, mean_squared_error(y_train, y_train_pred))\n",
    "rmse_train_list.insert(0, sqrt(mse_train_list[0]))\n",
    "r2_train_list.insert(0, r2_score(y_train, y_train_pred))\n",
    "\n",
    "mae_test_list.insert(0, mean_absolute_error(y_test, y_test_pred))\n",
    "mape_test_list.insert(0, np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100)\n",
    "mse_test_list.insert(0, mean_squared_error(y_test, y_test_pred))\n",
    "rmse_test_list.insert(0, sqrt(mse_test_list[0]))\n",
    "r2_test_list.insert(0, r2_score(y_test, y_test_pred))\n",
    "\n",
    "# Tampilkan hasil\n",
    "for i, feats in enumerate(selected_features_list, 1):\n",
    "    print(f\"{len(feats)} fitur -> \"\n",
    "          f\"MAE Train: {mae_train_list[i-1]:.4f}, MAE Test: {mae_test_list[i-1]:.4f}, \"\n",
    "          f\"MAPE Train: {mape_train_list[i-1]:.2f}%, MAPE Test: {mape_test_list[i-1]:.2f}%, \"\n",
    "          f\"MSE Train: {mse_train_list[i-1]:.4f}, MSE Test: {mse_test_list[i-1]:.4f}, \"\n",
    "          f\"RMSE Train: {rmse_train_list[i-1]:.4f}, RMSE Test: {rmse_test_list[i-1]:.4f}, \"\n",
    "          f\"R2 Train: {r2_train_list[i-1]:.4f}, R2 Test: {r2_test_list[i-1]:.4f}, \"\n",
    "          f\"Fitur: {feats}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386e47c",
   "metadata": {},
   "source": [
    "# Lasso Regresi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "64453a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule of Thumb Lasso (ringkas):\n",
    "# - Data sedikit (n < 1.000), fitur banyak (p > 50) â†’ alpha besar (0.1 â€“ 10)\n",
    "# - Data banyak (n > 10.000), fitur sedikit (p < 20) â†’ alpha kecil (0.0001 â€“ 0.1)\n",
    "# - Data banyak (n > 10.000), fitur banyak (p > 50) â†’ coba rentang luas (0.0001 â€“ 10), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6129141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lasso dengan GridSearchCV ===\n",
      "Alpha terbaik: 0.1\n",
      "\n",
      "=== Train ===\n",
      "MAE: 1.2013, MAPE: 13.53%, MSE: 2.7264, RMSE: 1.6512, R2: 0.8949\n",
      "\n",
      "=== Test ===\n",
      "MAE: 1.4617, MAPE: 15.03%, MSE: 3.2095, RMSE: 1.7915, R2: 0.8983\n",
      "\n",
      "Koefisien Lasso: [3.66931756e+00 2.71825744e+00 2.65069167e-03]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Daftar alpha yang mau dicoba\n",
    "param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1,10]}\n",
    "\n",
    "# GridSearchCV\n",
    "lasso = Lasso(max_iter=10000, random_state=42)\n",
    "grid = GridSearchCV(lasso, param_grid, cv=5, scoring='r2')\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model terbaik\n",
    "best_lasso = grid.best_estimator_\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred = best_lasso.predict(X_train_scaled)\n",
    "y_test_pred = best_lasso.predict(X_test_scaled)\n",
    "\n",
    "# Evaluasi Train\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Evaluasi Test\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Hasil\n",
    "print(\"=== Lasso dengan GridSearchCV ===\")\n",
    "print(\"Alpha terbaik:\", grid.best_params_['alpha'])\n",
    "\n",
    "print(\"\\n=== Train ===\")\n",
    "print(f\"MAE: {mae_train:.4f}, MAPE: {mape_train:.2f}%, \"\n",
    "      f\"MSE: {mse_train:.4f}, RMSE: {rmse_train:.4f}, R2: {r2_train:.4f}\")\n",
    "\n",
    "print(\"\\n=== Test ===\")\n",
    "print(f\"MAE: {mae_test:.4f}, MAPE: {mape_test:.2f}%, \"\n",
    "      f\"MSE: {mse_test:.4f}, RMSE: {rmse_test:.4f}, R2: {r2_test:.4f}\")\n",
    "\n",
    "print(\"\\nKoefisien Lasso:\", best_lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db365f",
   "metadata": {},
   "source": [
    "# Ridge Regresi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02f85d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule of Thumb Lasso (ringkas):\n",
    "# - Data sedikit (n < 1.000), fitur banyak (p > 50) â†’ alpha besar (0.1 â€“ 10)\n",
    "# - Data banyak (n > 10.000), fitur sedikit (p < 20) â†’ alpha kecil (0.0001 â€“ 0.1)\n",
    "# - Data banyak (n > 10.000), fitur banyak (p > 50) â†’ coba rentang luas (0.0001 â€“ 10), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5d2ba36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ridge dengan GridSearchCV ===\n",
      "Alpha terbaik: 1\n",
      "\n",
      "=== Train ===\n",
      "MAE: 1.2002, MAPE: 13.62%, MSE: 2.7026, RMSE: 1.6440, R2: 0.8958\n",
      "\n",
      "=== Test ===\n",
      "MAE: 1.4684, MAPE: 15.20%, MSE: 3.1884, RMSE: 1.7856, R2: 0.8990\n",
      "\n",
      "Koefisien Ridge: [3.7448478  2.76940915 0.08695204]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Daftar alpha yang mau dicoba\n",
    "param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# GridSearchCV\n",
    "ridge = Ridge(max_iter=10000, random_state=42)\n",
    "grid = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model terbaik\n",
    "best_ridge = grid.best_estimator_\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred = best_ridge.predict(X_train_scaled)\n",
    "y_test_pred = best_ridge.predict(X_test_scaled)\n",
    "\n",
    "# Evaluasi Train\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Evaluasi Test\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Hasil\n",
    "print(\"=== Ridge dengan GridSearchCV ===\")\n",
    "print(\"Alpha terbaik:\", grid.best_params_['alpha'])\n",
    "\n",
    "print(\"\\n=== Train ===\")\n",
    "print(f\"MAE: {mae_train:.4f}, MAPE: {mape_train:.2f}%, \"\n",
    "      f\"MSE: {mse_train:.4f}, RMSE: {rmse_train:.4f}, R2: {r2_train:.4f}\")\n",
    "\n",
    "print(\"\\n=== Test ===\")\n",
    "print(f\"MAE: {mae_test:.4f}, MAPE: {mape_test:.2f}%, \"\n",
    "      f\"MSE: {mse_test:.4f}, RMSE: {rmse_test:.4f}, R2: {r2_test:.4f}\")\n",
    "\n",
    "print(\"\\nKoefisien Ridge:\", best_ridge.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266176d",
   "metadata": {},
   "source": [
    "Karena perbedaan keduanya sangat kecil (selisih < 0.01), secara praktik keduanya sama-sama bagus.\n",
    "Biasanya, dalam kasus seperti ini, pemilihan lebih ditentukan oleh:\n",
    "\n",
    "Jumlah fitur: kalau banyak & ingin seleksi fitur â†’ pilih Lasso.\n",
    "\n",
    "Fokus ke stabilitas koefisien: kalau tidak ingin ada koefisien di-nolkan â†’ pilih Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d565ac52",
   "metadata": {},
   "source": [
    "# KNN Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f781153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e4aa966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "KNeighborsRegressor(\n",
      "    n_neighbors=\u001b[32m5\u001b[39m,\n",
      "    *,\n",
      "    weights=\u001b[33m'uniform'\u001b[39m,\n",
      "    algorithm=\u001b[33m'auto'\u001b[39m,\n",
      "    leaf_size=\u001b[32m30\u001b[39m,\n",
      "    p=\u001b[32m2\u001b[39m,\n",
      "    metric=\u001b[33m'minkowski'\u001b[39m,\n",
      "    metric_params=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "Regression based on k-nearest neighbors.\n",
      "\n",
      "The target is predicted by local interpolation of the targets\n",
      "associated of the nearest neighbors in the training set.\n",
      "\n",
      "Read more in the :ref:`User Guide <regression>`.\n",
      "\n",
      ".. versionadded:: 0.9\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "n_neighbors : int, default=5\n",
      "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "\n",
      "weights : {'uniform', 'distance'}, callable or None, default='uniform'\n",
      "    Weight function used in prediction.  Possible values:\n",
      "\n",
      "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "      are weighted equally.\n",
      "    - 'distance' : weight points by the inverse of their distance.\n",
      "      in this case, closer neighbors of a query point will have a\n",
      "      greater influence than neighbors which are further away.\n",
      "    - [callable] : a user-defined function which accepts an\n",
      "      array of distances, and returns an array of the same shape\n",
      "      containing the weights.\n",
      "\n",
      "    Uniform weights are used by default.\n",
      "\n",
      "    See the following example for a demonstration of the impact of\n",
      "    different weighting schemes on predictions:\n",
      "    :ref:`sphx_glr_auto_examples_neighbors_plot_regression.py`.\n",
      "\n",
      "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "    Algorithm used to compute the nearest neighbors:\n",
      "\n",
      "    - 'ball_tree' will use :class:`BallTree`\n",
      "    - 'kd_tree' will use :class:`KDTree`\n",
      "    - 'brute' will use a brute-force search.\n",
      "    - 'auto' will attempt to decide the most appropriate algorithm\n",
      "      based on the values passed to :meth:`fit` method.\n",
      "\n",
      "    Note: fitting on sparse input will override the setting of\n",
      "    this parameter, using brute force.\n",
      "\n",
      "leaf_size : int, default=30\n",
      "    Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "    speed of the construction and query, as well as the memory\n",
      "    required to store the tree.  The optimal value depends on the\n",
      "    nature of the problem.\n",
      "\n",
      "p : float, default=2\n",
      "    Power parameter for the Minkowski metric. When p = 1, this is\n",
      "    equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "\n",
      "metric : str, DistanceMetric object or callable, default='minkowski'\n",
      "    Metric to use for distance computation. Default is \"minkowski\", which\n",
      "    results in the standard Euclidean distance when p = 2. See the\n",
      "    documentation of `scipy.spatial.distance\n",
      "    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n",
      "    the metrics listed in\n",
      "    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n",
      "    values.\n",
      "\n",
      "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "    must be square during fit. X may be a :term:`sparse graph`, in which\n",
      "    case only \"nonzero\" elements may be considered neighbors.\n",
      "\n",
      "    If metric is a callable function, it takes two arrays representing 1D\n",
      "    vectors as inputs and must return one value indicating the distance\n",
      "    between those vectors. This works for Scipy's metrics, but is less\n",
      "    efficient than passing the metric name as a string.\n",
      "\n",
      "    If metric is a DistanceMetric object, it will be passed directly to\n",
      "    the underlying computation routines.\n",
      "\n",
      "metric_params : dict, default=None\n",
      "    Additional keyword arguments for the metric function.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    The number of parallel jobs to run for neighbors search.\n",
      "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "    for more details.\n",
      "    Doesn't affect :meth:`fit` method.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "effective_metric_ : str or callable\n",
      "    The distance metric to use. It will be same as the `metric` parameter\n",
      "    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "    'minkowski' and `p` parameter set to 2.\n",
      "\n",
      "effective_metric_params_ : dict\n",
      "    Additional keyword arguments for the metric function. For most metrics\n",
      "    will be same with `metric_params` parameter, but may also contain the\n",
      "    `p` parameter value if the `effective_metric_` attribute is set to\n",
      "    'minkowski'.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_samples_fit_ : int\n",
      "    Number of samples in the fitted data.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n",
      "RadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\n",
      "KNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\n",
      "RadiusNeighborsClassifier : Classifier implementing\n",
      "    a vote among neighbors within a given radius.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "\n",
      ".. warning::\n",
      "\n",
      "   Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      "   neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      "   different labels, the results will depend on the ordering of the\n",
      "   training data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> X = [[0], [1], [2], [3]]\n",
      ">>> y = [0, 0, 1, 1]\n",
      ">>> from sklearn.neighbors import KNeighborsRegressor\n",
      ">>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      ">>> neigh.fit(X, y)\n",
      "KNeighborsRegressor(...)\n",
      ">>> print(neigh.predict([[1.5]]))\n",
      "[0.5]\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\asus\\anaconda3\\envs\\clustering_env_clean\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "?KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "431c4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KNN Regressor Train ===\n",
      "MAE: 0.7339, MAPE: 9.10%, MSE: 1.2229, RMSE: 1.1058, R2: 0.9528\n",
      "\n",
      "=== KNN Regressor Test ===\n",
      "MAE: 1.0180, MAPE: 9.29%, MSE: 1.9871, RMSE: 1.4096, R2: 0.9370\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Buat model KNN Regressor\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Latih model\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred_knn = knn.predict(X_train_scaled)\n",
    "y_test_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluasi Train\n",
    "mae_train_knn = mean_absolute_error(y_train, y_train_pred_knn)\n",
    "mape_train_knn = np.mean(np.abs((y_train - y_train_pred_knn) / y_train)) * 100\n",
    "mse_train_knn = mean_squared_error(y_train, y_train_pred_knn)\n",
    "rmse_train_knn = sqrt(mse_train_knn)\n",
    "r2_train_knn = r2_score(y_train, y_train_pred_knn)\n",
    "\n",
    "# Evaluasi Test\n",
    "mae_test_knn = mean_absolute_error(y_test, y_test_pred_knn)\n",
    "mape_test_knn = np.mean(np.abs((y_test - y_test_pred_knn) / y_test)) * 100\n",
    "mse_test_knn = mean_squared_error(y_test, y_test_pred_knn)\n",
    "rmse_test_knn = sqrt(mse_test_knn)\n",
    "r2_test_knn = r2_score(y_test, y_test_pred_knn)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"=== KNN Regressor Train ===\")\n",
    "print(f\"MAE: {mae_train_knn:.4f}, MAPE: {mape_train_knn:.2f}%, \"\n",
    "      f\"MSE: {mse_train_knn:.4f}, RMSE: {rmse_train_knn:.4f}, R2: {r2_train_knn:.4f}\")\n",
    "\n",
    "print(\"\\n=== KNN Regressor Test ===\")\n",
    "print(f\"MAE: {mae_test_knn:.4f}, MAPE: {mape_test_knn:.2f}%, \"\n",
    "      f\"MSE: {mse_test_knn:.4f}, RMSE: {rmse_test_knn:.4f}, R2: {r2_test_knn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db882e8b",
   "metadata": {},
   "source": [
    "# Tunning Metric Minkowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "01809cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List untuk menyimpan hasil\n",
    "results_knn = []\n",
    "\n",
    "# Range n_neighbors (sesuaikan jumlah baris data)\n",
    "neighbors = range(10, 17)\n",
    "weights_options = ['uniform', 'distance']\n",
    "p_values = [1, 2, 3]\n",
    "\n",
    "# Loop semua kombinasi\n",
    "for k in neighbors:\n",
    "    for w in weights_options:\n",
    "        for p_val in p_values:\n",
    "            knn = KNeighborsRegressor(\n",
    "                n_neighbors=k,\n",
    "                weights=w,\n",
    "                metric='minkowski',\n",
    "                p=p_val\n",
    "            )\n",
    "            knn.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_train_pred = knn.predict(X_train_scaled)\n",
    "            y_test_pred = knn.predict(X_test_scaled)\n",
    "            \n",
    "            # Hitung metrik evaluasi\n",
    "            mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "            mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "            # Tambahkan MAPE (hindari pembagian nol)\n",
    "            mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "            mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "\n",
    "            mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "            mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "            rmse_train = sqrt(mse_train)\n",
    "            rmse_test = sqrt(mse_test)\n",
    "\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            \n",
    "            # Simpan hasil ke list (dengan urutan sesuai permintaan)\n",
    "            results_knn.append({\n",
    "                'n_neighbors': k,\n",
    "                'weights': w,\n",
    "                'metric': 'minkowski',\n",
    "                'p': p_val,\n",
    "                'MAE_train': mae_train,\n",
    "                'MAE_test': mae_test,\n",
    "                'MAPE_train': mape_train,\n",
    "                'MAPE_test': mape_test,\n",
    "                'MSE_train': mse_train,\n",
    "                'MSE_test': mse_test,\n",
    "                'RMSE_train': rmse_train,\n",
    "                'RMSE_test': rmse_test,\n",
    "                'R2_train': r2_train,\n",
    "                'R2_test': r2_test,\n",
    "                'R2_selisih': r2_train-r2_test\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "064c30b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>weights</th>\n",
       "      <th>metric</th>\n",
       "      <th>p</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.006789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.653230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.842868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.357523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941614</td>\n",
       "      <td>0.058386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.027288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.875027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.887234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.373766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940209</td>\n",
       "      <td>0.059791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.012459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.946707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.894164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.376286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.939989</td>\n",
       "      <td>0.060011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.017284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.192384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.969144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.403262</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937613</td>\n",
       "      <td>0.062387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.044585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.290502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.016536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.420048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936112</td>\n",
       "      <td>0.063888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.070847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.603427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.126280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.458177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.932635</td>\n",
       "      <td>0.067365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.804625</td>\n",
       "      <td>1.080750</td>\n",
       "      <td>10.115601</td>\n",
       "      <td>10.480666</td>\n",
       "      <td>1.594315</td>\n",
       "      <td>2.174948</td>\n",
       "      <td>1.262662</td>\n",
       "      <td>1.474770</td>\n",
       "      <td>0.938529</td>\n",
       "      <td>0.931093</td>\n",
       "      <td>0.007436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.079678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.797265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.179028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.476153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930964</td>\n",
       "      <td>0.069036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.082827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.700695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.240359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.496783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929021</td>\n",
       "      <td>0.070979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.089724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.116170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.250728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500243</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928692</td>\n",
       "      <td>0.071308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.933019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.251180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500393</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928678</td>\n",
       "      <td>0.071322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.839659</td>\n",
       "      <td>1.102045</td>\n",
       "      <td>10.724059</td>\n",
       "      <td>10.699324</td>\n",
       "      <td>1.806536</td>\n",
       "      <td>2.275436</td>\n",
       "      <td>1.344074</td>\n",
       "      <td>1.508455</td>\n",
       "      <td>0.930347</td>\n",
       "      <td>0.927909</td>\n",
       "      <td>0.002438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.864323</td>\n",
       "      <td>1.104167</td>\n",
       "      <td>10.742483</td>\n",
       "      <td>10.967911</td>\n",
       "      <td>1.897225</td>\n",
       "      <td>2.313292</td>\n",
       "      <td>1.377398</td>\n",
       "      <td>1.520951</td>\n",
       "      <td>0.926850</td>\n",
       "      <td>0.926710</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.788945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.340672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.529926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925843</td>\n",
       "      <td>0.074157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.103144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.089359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.348941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.532625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925581</td>\n",
       "      <td>0.074419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820750</td>\n",
       "      <td>1.126500</td>\n",
       "      <td>10.695757</td>\n",
       "      <td>11.131092</td>\n",
       "      <td>1.740767</td>\n",
       "      <td>2.411105</td>\n",
       "      <td>1.319381</td>\n",
       "      <td>1.552773</td>\n",
       "      <td>0.932883</td>\n",
       "      <td>0.923611</td>\n",
       "      <td>0.009272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.107839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.289336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.411780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.552991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923590</td>\n",
       "      <td>0.076410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.920288</td>\n",
       "      <td>1.114231</td>\n",
       "      <td>11.248638</td>\n",
       "      <td>11.312152</td>\n",
       "      <td>2.033172</td>\n",
       "      <td>2.442820</td>\n",
       "      <td>1.425893</td>\n",
       "      <td>1.562952</td>\n",
       "      <td>0.921609</td>\n",
       "      <td>0.922606</td>\n",
       "      <td>-0.000998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.129036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.370270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.514202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.585623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920345</td>\n",
       "      <td>0.079655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.137724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.339200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.561612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.600504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918843</td>\n",
       "      <td>0.081157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.874261</td>\n",
       "      <td>1.163409</td>\n",
       "      <td>10.991265</td>\n",
       "      <td>11.562378</td>\n",
       "      <td>1.876738</td>\n",
       "      <td>2.582209</td>\n",
       "      <td>1.369941</td>\n",
       "      <td>1.606925</td>\n",
       "      <td>0.927640</td>\n",
       "      <td>0.918190</td>\n",
       "      <td>0.009450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.123266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.382653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.595288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.610990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917776</td>\n",
       "      <td>0.082224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.626050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.640897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.625084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916331</td>\n",
       "      <td>0.083669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.152520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.668797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.667524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.633256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915487</td>\n",
       "      <td>0.084513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.140534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.591526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.675939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.635830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915221</td>\n",
       "      <td>0.084779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.161247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.846885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.714123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.647460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914011</td>\n",
       "      <td>0.085989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.865125</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>11.007851</td>\n",
       "      <td>11.871883</td>\n",
       "      <td>1.825330</td>\n",
       "      <td>2.726695</td>\n",
       "      <td>1.351048</td>\n",
       "      <td>1.651271</td>\n",
       "      <td>0.929622</td>\n",
       "      <td>0.913613</td>\n",
       "      <td>0.016010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>0.973973</td>\n",
       "      <td>1.203571</td>\n",
       "      <td>11.757213</td>\n",
       "      <td>12.167429</td>\n",
       "      <td>2.205048</td>\n",
       "      <td>2.766995</td>\n",
       "      <td>1.484940</td>\n",
       "      <td>1.663429</td>\n",
       "      <td>0.914982</td>\n",
       "      <td>0.912336</td>\n",
       "      <td>0.002646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16</td>\n",
       "      <td>distance</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.157560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.790610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.768487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.663877</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.912289</td>\n",
       "      <td>0.087711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.919323</td>\n",
       "      <td>1.220208</td>\n",
       "      <td>11.518154</td>\n",
       "      <td>12.041103</td>\n",
       "      <td>2.078137</td>\n",
       "      <td>2.827887</td>\n",
       "      <td>1.441574</td>\n",
       "      <td>1.681632</td>\n",
       "      <td>0.919875</td>\n",
       "      <td>0.910407</td>\n",
       "      <td>0.009469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>15</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>1.002792</td>\n",
       "      <td>1.208167</td>\n",
       "      <td>11.922555</td>\n",
       "      <td>12.498489</td>\n",
       "      <td>2.367913</td>\n",
       "      <td>2.862159</td>\n",
       "      <td>1.538802</td>\n",
       "      <td>1.691792</td>\n",
       "      <td>0.908703</td>\n",
       "      <td>0.909321</td>\n",
       "      <td>-0.000618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.884375</td>\n",
       "      <td>1.210455</td>\n",
       "      <td>11.265682</td>\n",
       "      <td>12.238059</td>\n",
       "      <td>1.985891</td>\n",
       "      <td>2.898955</td>\n",
       "      <td>1.409216</td>\n",
       "      <td>1.702632</td>\n",
       "      <td>0.923432</td>\n",
       "      <td>0.908155</td>\n",
       "      <td>0.015277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>13</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>0.977740</td>\n",
       "      <td>1.232308</td>\n",
       "      <td>12.115135</td>\n",
       "      <td>12.196808</td>\n",
       "      <td>2.287258</td>\n",
       "      <td>2.956956</td>\n",
       "      <td>1.512368</td>\n",
       "      <td>1.719580</td>\n",
       "      <td>0.911812</td>\n",
       "      <td>0.906318</td>\n",
       "      <td>0.005495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>16</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>1</td>\n",
       "      <td>1.035742</td>\n",
       "      <td>1.256406</td>\n",
       "      <td>12.311976</td>\n",
       "      <td>12.915194</td>\n",
       "      <td>2.523584</td>\n",
       "      <td>3.123347</td>\n",
       "      <td>1.588579</td>\n",
       "      <td>1.767299</td>\n",
       "      <td>0.902701</td>\n",
       "      <td>0.901046</td>\n",
       "      <td>0.001655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925521</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>11.598217</td>\n",
       "      <td>12.666802</td>\n",
       "      <td>2.114836</td>\n",
       "      <td>3.146368</td>\n",
       "      <td>1.454248</td>\n",
       "      <td>1.773800</td>\n",
       "      <td>0.918460</td>\n",
       "      <td>0.900317</td>\n",
       "      <td>0.018144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>14</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>1.014509</td>\n",
       "      <td>1.301786</td>\n",
       "      <td>12.459006</td>\n",
       "      <td>12.910536</td>\n",
       "      <td>2.409994</td>\n",
       "      <td>3.316309</td>\n",
       "      <td>1.552416</td>\n",
       "      <td>1.821073</td>\n",
       "      <td>0.907080</td>\n",
       "      <td>0.894932</td>\n",
       "      <td>0.012148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>15</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>1.020667</td>\n",
       "      <td>1.288167</td>\n",
       "      <td>12.517693</td>\n",
       "      <td>13.069212</td>\n",
       "      <td>2.501194</td>\n",
       "      <td>3.372519</td>\n",
       "      <td>1.581516</td>\n",
       "      <td>1.836442</td>\n",
       "      <td>0.903564</td>\n",
       "      <td>0.893152</td>\n",
       "      <td>0.010412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971587</td>\n",
       "      <td>1.302115</td>\n",
       "      <td>12.078324</td>\n",
       "      <td>13.267330</td>\n",
       "      <td>2.320081</td>\n",
       "      <td>3.381262</td>\n",
       "      <td>1.523181</td>\n",
       "      <td>1.838821</td>\n",
       "      <td>0.910547</td>\n",
       "      <td>0.892875</td>\n",
       "      <td>0.017672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>15</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>1.039875</td>\n",
       "      <td>1.308667</td>\n",
       "      <td>12.740306</td>\n",
       "      <td>13.273388</td>\n",
       "      <td>2.597145</td>\n",
       "      <td>3.428449</td>\n",
       "      <td>1.611566</td>\n",
       "      <td>1.851607</td>\n",
       "      <td>0.899864</td>\n",
       "      <td>0.891380</td>\n",
       "      <td>0.008485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>1.017500</td>\n",
       "      <td>1.324464</td>\n",
       "      <td>12.588630</td>\n",
       "      <td>13.408991</td>\n",
       "      <td>2.496553</td>\n",
       "      <td>3.468552</td>\n",
       "      <td>1.580048</td>\n",
       "      <td>1.862405</td>\n",
       "      <td>0.903743</td>\n",
       "      <td>0.890109</td>\n",
       "      <td>0.013634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>16</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>3</td>\n",
       "      <td>1.080586</td>\n",
       "      <td>1.333281</td>\n",
       "      <td>13.108869</td>\n",
       "      <td>13.568354</td>\n",
       "      <td>2.732538</td>\n",
       "      <td>3.526646</td>\n",
       "      <td>1.653039</td>\n",
       "      <td>1.877937</td>\n",
       "      <td>0.894644</td>\n",
       "      <td>0.888269</td>\n",
       "      <td>0.006375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>16</td>\n",
       "      <td>uniform</td>\n",
       "      <td>minkowski</td>\n",
       "      <td>2</td>\n",
       "      <td>1.043867</td>\n",
       "      <td>1.338125</td>\n",
       "      <td>12.851976</td>\n",
       "      <td>13.693005</td>\n",
       "      <td>2.678764</td>\n",
       "      <td>3.533605</td>\n",
       "      <td>1.636693</td>\n",
       "      <td>1.879789</td>\n",
       "      <td>0.896717</td>\n",
       "      <td>0.888048</td>\n",
       "      <td>0.008669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_neighbors   weights     metric  p  MAE_train  MAE_test  MAPE_train  \\\n",
       "0            10  distance  minkowski  1   0.000000  1.006789    0.000000   \n",
       "1            11  distance  minkowski  1   0.000000  1.027288    0.000000   \n",
       "2            12  distance  minkowski  1   0.000000  1.012459    0.000000   \n",
       "3            13  distance  minkowski  1   0.000000  1.017284    0.000000   \n",
       "4            10  distance  minkowski  2   0.000000  1.044585    0.000000   \n",
       "5            11  distance  minkowski  2   0.000000  1.070847    0.000000   \n",
       "6            10   uniform  minkowski  1   0.804625  1.080750   10.115601   \n",
       "7            14  distance  minkowski  1   0.000000  1.079678    0.000000   \n",
       "8            12  distance  minkowski  2   0.000000  1.082827    0.000000   \n",
       "9            15  distance  minkowski  1   0.000000  1.089724    0.000000   \n",
       "10           10  distance  minkowski  3   0.000000  1.100176    0.000000   \n",
       "11           11   uniform  minkowski  1   0.839659  1.102045   10.724059   \n",
       "12           12   uniform  minkowski  1   0.864323  1.104167   10.742483   \n",
       "13           13  distance  minkowski  2   0.000000  1.083795    0.000000   \n",
       "14           11  distance  minkowski  3   0.000000  1.103144    0.000000   \n",
       "15           10   uniform  minkowski  2   0.820750  1.126500   10.695757   \n",
       "16           16  distance  minkowski  1   0.000000  1.107839    0.000000   \n",
       "17           13   uniform  minkowski  1   0.920288  1.114231   11.248638   \n",
       "18           12  distance  minkowski  3   0.000000  1.129036    0.000000   \n",
       "19           14  distance  minkowski  2   0.000000  1.137724    0.000000   \n",
       "20           11   uniform  minkowski  2   0.874261  1.163409   10.991265   \n",
       "21           15  distance  minkowski  2   0.000000  1.123266    0.000000   \n",
       "22           13  distance  minkowski  3   0.000000  1.142910    0.000000   \n",
       "23           14  distance  minkowski  3   0.000000  1.152520    0.000000   \n",
       "24           15  distance  minkowski  3   0.000000  1.140534    0.000000   \n",
       "25           16  distance  minkowski  2   0.000000  1.161247    0.000000   \n",
       "26           10   uniform  minkowski  3   0.865125  1.191000   11.007851   \n",
       "27           14   uniform  minkowski  1   0.973973  1.203571   11.757213   \n",
       "28           16  distance  minkowski  3   0.000000  1.157560    0.000000   \n",
       "29           12   uniform  minkowski  2   0.919323  1.220208   11.518154   \n",
       "30           15   uniform  minkowski  1   1.002792  1.208167   11.922555   \n",
       "31           11   uniform  minkowski  3   0.884375  1.210455   11.265682   \n",
       "32           13   uniform  minkowski  2   0.977740  1.232308   12.115135   \n",
       "33           16   uniform  minkowski  1   1.035742  1.256406   12.311976   \n",
       "34           12   uniform  minkowski  3   0.925521  1.250000   11.598217   \n",
       "35           14   uniform  minkowski  2   1.014509  1.301786   12.459006   \n",
       "36           15   uniform  minkowski  2   1.020667  1.288167   12.517693   \n",
       "37           13   uniform  minkowski  3   0.971587  1.302115   12.078324   \n",
       "38           15   uniform  minkowski  3   1.039875  1.308667   12.740306   \n",
       "39           14   uniform  minkowski  3   1.017500  1.324464   12.588630   \n",
       "40           16   uniform  minkowski  3   1.080586  1.333281   13.108869   \n",
       "41           16   uniform  minkowski  2   1.043867  1.338125   12.851976   \n",
       "\n",
       "    MAPE_test  MSE_train  MSE_test  RMSE_train  RMSE_test  R2_train   R2_test  \\\n",
       "0    9.653230   0.000000  1.842868    0.000000   1.357523  1.000000  0.941614   \n",
       "1    9.875027   0.000000  1.887234    0.000000   1.373766  1.000000  0.940209   \n",
       "2    9.946707   0.000000  1.894164    0.000000   1.376286  1.000000  0.939989   \n",
       "3   10.192384   0.000000  1.969144    0.000000   1.403262  1.000000  0.937613   \n",
       "4   10.290502   0.000000  2.016536    0.000000   1.420048  1.000000  0.936112   \n",
       "5   10.603427   0.000000  2.126280    0.000000   1.458177  1.000000  0.932635   \n",
       "6   10.480666   1.594315  2.174948    1.262662   1.474770  0.938529  0.931093   \n",
       "7   10.797265   0.000000  2.179028    0.000000   1.476153  1.000000  0.930964   \n",
       "8   10.700695   0.000000  2.240359    0.000000   1.496783  1.000000  0.929021   \n",
       "9   11.116170   0.000000  2.250728    0.000000   1.500243  1.000000  0.928692   \n",
       "10  10.933019   0.000000  2.251180    0.000000   1.500393  1.000000  0.928678   \n",
       "11  10.699324   1.806536  2.275436    1.344074   1.508455  0.930347  0.927909   \n",
       "12  10.967911   1.897225  2.313292    1.377398   1.520951  0.926850  0.926710   \n",
       "13  10.788945   0.000000  2.340672    0.000000   1.529926  1.000000  0.925843   \n",
       "14  11.089359   0.000000  2.348941    0.000000   1.532625  1.000000  0.925581   \n",
       "15  11.131092   1.740767  2.411105    1.319381   1.552773  0.932883  0.923611   \n",
       "16  11.289336   0.000000  2.411780    0.000000   1.552991  1.000000  0.923590   \n",
       "17  11.312152   2.033172  2.442820    1.425893   1.562952  0.921609  0.922606   \n",
       "18  11.370270   0.000000  2.514202    0.000000   1.585623  1.000000  0.920345   \n",
       "19  11.339200   0.000000  2.561612    0.000000   1.600504  1.000000  0.918843   \n",
       "20  11.562378   1.876738  2.582209    1.369941   1.606925  0.927640  0.918190   \n",
       "21  11.382653   0.000000  2.595288    0.000000   1.610990  1.000000  0.917776   \n",
       "22  11.626050   0.000000  2.640897    0.000000   1.625084  1.000000  0.916331   \n",
       "23  11.668797   0.000000  2.667524    0.000000   1.633256  1.000000  0.915487   \n",
       "24  11.591526   0.000000  2.675939    0.000000   1.635830  1.000000  0.915221   \n",
       "25  11.846885   0.000000  2.714123    0.000000   1.647460  1.000000  0.914011   \n",
       "26  11.871883   1.825330  2.726695    1.351048   1.651271  0.929622  0.913613   \n",
       "27  12.167429   2.205048  2.766995    1.484940   1.663429  0.914982  0.912336   \n",
       "28  11.790610   0.000000  2.768487    0.000000   1.663877  1.000000  0.912289   \n",
       "29  12.041103   2.078137  2.827887    1.441574   1.681632  0.919875  0.910407   \n",
       "30  12.498489   2.367913  2.862159    1.538802   1.691792  0.908703  0.909321   \n",
       "31  12.238059   1.985891  2.898955    1.409216   1.702632  0.923432  0.908155   \n",
       "32  12.196808   2.287258  2.956956    1.512368   1.719580  0.911812  0.906318   \n",
       "33  12.915194   2.523584  3.123347    1.588579   1.767299  0.902701  0.901046   \n",
       "34  12.666802   2.114836  3.146368    1.454248   1.773800  0.918460  0.900317   \n",
       "35  12.910536   2.409994  3.316309    1.552416   1.821073  0.907080  0.894932   \n",
       "36  13.069212   2.501194  3.372519    1.581516   1.836442  0.903564  0.893152   \n",
       "37  13.267330   2.320081  3.381262    1.523181   1.838821  0.910547  0.892875   \n",
       "38  13.273388   2.597145  3.428449    1.611566   1.851607  0.899864  0.891380   \n",
       "39  13.408991   2.496553  3.468552    1.580048   1.862405  0.903743  0.890109   \n",
       "40  13.568354   2.732538  3.526646    1.653039   1.877937  0.894644  0.888269   \n",
       "41  13.693005   2.678764  3.533605    1.636693   1.879789  0.896717  0.888048   \n",
       "\n",
       "    R2_selisih  \n",
       "0     0.058386  \n",
       "1     0.059791  \n",
       "2     0.060011  \n",
       "3     0.062387  \n",
       "4     0.063888  \n",
       "5     0.067365  \n",
       "6     0.007436  \n",
       "7     0.069036  \n",
       "8     0.070979  \n",
       "9     0.071308  \n",
       "10    0.071322  \n",
       "11    0.002438  \n",
       "12    0.000140  \n",
       "13    0.074157  \n",
       "14    0.074419  \n",
       "15    0.009272  \n",
       "16    0.076410  \n",
       "17   -0.000998  \n",
       "18    0.079655  \n",
       "19    0.081157  \n",
       "20    0.009450  \n",
       "21    0.082224  \n",
       "22    0.083669  \n",
       "23    0.084513  \n",
       "24    0.084779  \n",
       "25    0.085989  \n",
       "26    0.016010  \n",
       "27    0.002646  \n",
       "28    0.087711  \n",
       "29    0.009469  \n",
       "30   -0.000618  \n",
       "31    0.015277  \n",
       "32    0.005495  \n",
       "33    0.001655  \n",
       "34    0.018144  \n",
       "35    0.012148  \n",
       "36    0.010412  \n",
       "37    0.017672  \n",
       "38    0.008485  \n",
       "39    0.013634  \n",
       "40    0.006375  \n",
       "41    0.008669  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Konversi ke DataFrame biar lebih enak dibaca\n",
    "df_knn_minkowski = pd.DataFrame(results_knn)\n",
    "\n",
    "#Tampilkan haisl \n",
    "df_knn_minkowski.sort_values(by='R2_test', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5351e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hasilnya indeks ke 6 yg terbaik dgn tidak overfit di r2 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "380e2315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params manhattan: {'metric': 'minkowski', 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "Best CV Score (R2) manhattan: 0.9039904626375618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definisi parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': range(10, 16),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['minkowski'],\n",
    "    'p': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# GridSearchCV dengan 5-fold cross-validation\n",
    "grid_search_manhattan = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',   # bisa ganti 'neg_mean_absolute_error', 'neg_mean_squared_error', dll\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit ke data train\n",
    "grid_search_manhattan.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Params manhattan:\", grid_search_manhattan.best_params_)\n",
    "print(\"Best CV Score (R2) manhattan:\", grid_search_manhattan.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aa423669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train Metrics ===\n",
      "MAE  : 0.0000\n",
      "MAPE : 0.0000%\n",
      "MSE  : 0.0000\n",
      "RMSE : 0.0000\n",
      "RÂ²   : 1.0000\n",
      "\n",
      "=== Test Metrics ===\n",
      "MAE  : 1.0068\n",
      "MAPE : 9.6532%\n",
      "MSE  : 1.8429\n",
      "RMSE : 1.3575\n",
      "RÂ²   : 0.9416\n"
     ]
    }
   ],
   "source": [
    "# Ambil best params dari grid search\n",
    "best_params = grid_search_manhattan.best_params_\n",
    "\n",
    "# Fit ulang model dengan best params\n",
    "best_knn = KNeighborsRegressor(**best_params)\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred = best_knn.predict(X_train_scaled)\n",
    "y_test_pred = best_knn.predict(X_test_scaled)\n",
    "\n",
    "# Custom MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    nonzero_idx = y_true != 0\n",
    "    return np.mean(np.abs((y_true[nonzero_idx] - y_pred[nonzero_idx]) / y_true[nonzero_idx])) * 100\n",
    "\n",
    "# Hitung metrik Train\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Hitung metrik Test\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"=== Train Metrics ===\")\n",
    "print(f\"MAE  : {mae_train:.4f}\")\n",
    "print(f\"MAPE : {mape_train:.4f}%\")\n",
    "print(f\"MSE  : {mse_train:.4f}\")\n",
    "print(f\"RMSE : {rmse_train:.4f}\")\n",
    "print(f\"RÂ²   : {r2_train:.4f}\")\n",
    "\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "print(f\"MAE  : {mae_test:.4f}\")\n",
    "print(f\"MAPE : {mape_test:.4f}%\")\n",
    "print(f\"MSE  : {mse_test:.4f}\")\n",
    "print(f\"RMSE : {rmse_test:.4f}\")\n",
    "print(f\"RÂ²   : {r2_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6487ae",
   "metadata": {},
   "source": [
    "#overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8945e525",
   "metadata": {},
   "source": [
    "# Metricsnya Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de4b4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List untuk menyimpan hasil\n",
    "results_knn = []\n",
    "\n",
    "# Range n_neighbors (sesuaikan jumlah baris data training)\n",
    "neighbors = range(10, 17)\n",
    "weights_options = ['uniform', 'distance']\n",
    "\n",
    "# Loop semua kombinasi k dan weights\n",
    "for k in neighbors:\n",
    "    for w in weights_options:\n",
    "        knn = KNeighborsRegressor(\n",
    "            n_neighbors=k,\n",
    "            weights=w,\n",
    "            metric='euclidean'   # langsung euclidean, tidak perlu p\n",
    "        )\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "        y_train_pred = knn.predict(X_train_scaled)\n",
    "        y_test_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "        # Hitung metrik evaluasi\n",
    "        mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "        mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        # Tambahkan MAPE (hindari pembagian nol)\n",
    "        mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "        mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "\n",
    "        mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "        mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        rmse_train = sqrt(mse_train)\n",
    "        rmse_test = sqrt(mse_test)\n",
    "\n",
    "        r2_train = r2_score(y_train, y_train_pred)\n",
    "        r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "        # Simpan hasil ke list (dengan urutan sesuai permintaan: MAE, MAPE, MSE, RMSE, R2)\n",
    "        results_knn.append({\n",
    "            'n_neighbors': k,\n",
    "            'weights': w,\n",
    "            'metric': 'euclidean',\n",
    "            'MAE_train': mae_train,\n",
    "            'MAE_test': mae_test,\n",
    "            'MAPE_train': mape_train,\n",
    "            'MAPE_test': mape_test,\n",
    "            'MSE_train': mse_train,\n",
    "            'MSE_test': mse_test,\n",
    "            'RMSE_train': rmse_train,\n",
    "            'RMSE_test': rmse_test,\n",
    "            'R2_train': r2_train,\n",
    "            'R2_test': r2_test,\n",
    "            'R2_selisih': r2_train-r2_test\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "462b43ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>weights</th>\n",
       "      <th>metric</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.044585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.290502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.016536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.420048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936112</td>\n",
       "      <td>0.063888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.070847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.603427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.126280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.458177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.932635</td>\n",
       "      <td>0.067365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.082827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.700695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.240359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.496783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929021</td>\n",
       "      <td>0.070979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.788945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.340672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.529926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925843</td>\n",
       "      <td>0.074157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.820750</td>\n",
       "      <td>1.126500</td>\n",
       "      <td>10.695757</td>\n",
       "      <td>11.131092</td>\n",
       "      <td>1.740767</td>\n",
       "      <td>2.411105</td>\n",
       "      <td>1.319381</td>\n",
       "      <td>1.552773</td>\n",
       "      <td>0.932883</td>\n",
       "      <td>0.923611</td>\n",
       "      <td>0.009272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.137724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.339200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.561612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.600504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918843</td>\n",
       "      <td>0.081157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.874261</td>\n",
       "      <td>1.163409</td>\n",
       "      <td>10.991265</td>\n",
       "      <td>11.562378</td>\n",
       "      <td>1.876738</td>\n",
       "      <td>2.582209</td>\n",
       "      <td>1.369941</td>\n",
       "      <td>1.606925</td>\n",
       "      <td>0.927640</td>\n",
       "      <td>0.918190</td>\n",
       "      <td>0.009450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.123266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.382653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.595288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.610990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917776</td>\n",
       "      <td>0.082224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.161247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.846885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.714123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.647460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914011</td>\n",
       "      <td>0.085989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.919323</td>\n",
       "      <td>1.220208</td>\n",
       "      <td>11.518154</td>\n",
       "      <td>12.041103</td>\n",
       "      <td>2.078137</td>\n",
       "      <td>2.827887</td>\n",
       "      <td>1.441574</td>\n",
       "      <td>1.681632</td>\n",
       "      <td>0.919875</td>\n",
       "      <td>0.910407</td>\n",
       "      <td>0.009469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.977740</td>\n",
       "      <td>1.232308</td>\n",
       "      <td>12.115135</td>\n",
       "      <td>12.196808</td>\n",
       "      <td>2.287258</td>\n",
       "      <td>2.956956</td>\n",
       "      <td>1.512368</td>\n",
       "      <td>1.719580</td>\n",
       "      <td>0.911812</td>\n",
       "      <td>0.906318</td>\n",
       "      <td>0.005495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>1.014509</td>\n",
       "      <td>1.301786</td>\n",
       "      <td>12.459006</td>\n",
       "      <td>12.910536</td>\n",
       "      <td>2.409994</td>\n",
       "      <td>3.316309</td>\n",
       "      <td>1.552416</td>\n",
       "      <td>1.821073</td>\n",
       "      <td>0.907080</td>\n",
       "      <td>0.894932</td>\n",
       "      <td>0.012148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>1.020667</td>\n",
       "      <td>1.288167</td>\n",
       "      <td>12.517693</td>\n",
       "      <td>13.069212</td>\n",
       "      <td>2.501194</td>\n",
       "      <td>3.372519</td>\n",
       "      <td>1.581516</td>\n",
       "      <td>1.836442</td>\n",
       "      <td>0.903564</td>\n",
       "      <td>0.893152</td>\n",
       "      <td>0.010412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>1.043867</td>\n",
       "      <td>1.338125</td>\n",
       "      <td>12.851976</td>\n",
       "      <td>13.693005</td>\n",
       "      <td>2.678764</td>\n",
       "      <td>3.533605</td>\n",
       "      <td>1.636693</td>\n",
       "      <td>1.879789</td>\n",
       "      <td>0.896717</td>\n",
       "      <td>0.888048</td>\n",
       "      <td>0.008669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_neighbors   weights     metric  MAE_train  MAE_test  MAPE_train  \\\n",
       "0            10  distance  euclidean   0.000000  1.044585    0.000000   \n",
       "1            11  distance  euclidean   0.000000  1.070847    0.000000   \n",
       "2            12  distance  euclidean   0.000000  1.082827    0.000000   \n",
       "3            13  distance  euclidean   0.000000  1.083795    0.000000   \n",
       "4            10   uniform  euclidean   0.820750  1.126500   10.695757   \n",
       "5            14  distance  euclidean   0.000000  1.137724    0.000000   \n",
       "6            11   uniform  euclidean   0.874261  1.163409   10.991265   \n",
       "7            15  distance  euclidean   0.000000  1.123266    0.000000   \n",
       "8            16  distance  euclidean   0.000000  1.161247    0.000000   \n",
       "9            12   uniform  euclidean   0.919323  1.220208   11.518154   \n",
       "10           13   uniform  euclidean   0.977740  1.232308   12.115135   \n",
       "11           14   uniform  euclidean   1.014509  1.301786   12.459006   \n",
       "12           15   uniform  euclidean   1.020667  1.288167   12.517693   \n",
       "13           16   uniform  euclidean   1.043867  1.338125   12.851976   \n",
       "\n",
       "    MAPE_test  MSE_train  MSE_test  RMSE_train  RMSE_test  R2_train   R2_test  \\\n",
       "0   10.290502   0.000000  2.016536    0.000000   1.420048  1.000000  0.936112   \n",
       "1   10.603427   0.000000  2.126280    0.000000   1.458177  1.000000  0.932635   \n",
       "2   10.700695   0.000000  2.240359    0.000000   1.496783  1.000000  0.929021   \n",
       "3   10.788945   0.000000  2.340672    0.000000   1.529926  1.000000  0.925843   \n",
       "4   11.131092   1.740767  2.411105    1.319381   1.552773  0.932883  0.923611   \n",
       "5   11.339200   0.000000  2.561612    0.000000   1.600504  1.000000  0.918843   \n",
       "6   11.562378   1.876738  2.582209    1.369941   1.606925  0.927640  0.918190   \n",
       "7   11.382653   0.000000  2.595288    0.000000   1.610990  1.000000  0.917776   \n",
       "8   11.846885   0.000000  2.714123    0.000000   1.647460  1.000000  0.914011   \n",
       "9   12.041103   2.078137  2.827887    1.441574   1.681632  0.919875  0.910407   \n",
       "10  12.196808   2.287258  2.956956    1.512368   1.719580  0.911812  0.906318   \n",
       "11  12.910536   2.409994  3.316309    1.552416   1.821073  0.907080  0.894932   \n",
       "12  13.069212   2.501194  3.372519    1.581516   1.836442  0.903564  0.893152   \n",
       "13  13.693005   2.678764  3.533605    1.636693   1.879789  0.896717  0.888048   \n",
       "\n",
       "    R2_selisih  \n",
       "0     0.063888  \n",
       "1     0.067365  \n",
       "2     0.070979  \n",
       "3     0.074157  \n",
       "4     0.009272  \n",
       "5     0.081157  \n",
       "6     0.009450  \n",
       "7     0.082224  \n",
       "8     0.085989  \n",
       "9     0.009469  \n",
       "10    0.005495  \n",
       "11    0.012148  \n",
       "12    0.010412  \n",
       "13    0.008669  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Konversi ke DataFrame biar lebih enak dibaca\n",
    "df_knn_euclidean = pd.DataFrame(results_knn)\n",
    "\n",
    "#Tampilkan haisl \n",
    "df_knn_euclidean.sort_values(by='R2_test', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1fee737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indeks ke 4 menarik karena gak overfit di train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0667fc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'metric': 'euclidean', 'n_neighbors': 10, 'weights': 'distance'}\n",
      "Best CV Score (R2): 0.8989821433523085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definisi parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': range(10, 16),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean'],\n",
    "}\n",
    "\n",
    "# Model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# GridSearchCV dengan 5-fold cross-validation\n",
    "grid_search_euclidean = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',   # bisa ganti 'neg_mean_absolute_error', 'neg_mean_squared_error', dll\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit ke data train\n",
    "grid_search_euclidean.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid_search_euclidean.best_params_)\n",
    "print(\"Best CV Score (R2):\", grid_search_euclidean.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a4fa156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train Metrics ===\n",
      "MAE  : 0.0000\n",
      "MAPE : 0.0000%\n",
      "MSE  : 0.0000\n",
      "RMSE : 0.0000\n",
      "RÂ²   : 1.0000\n",
      "\n",
      "=== Test Metrics ===\n",
      "MAE  : 1.0446\n",
      "MAPE : 10.2905%\n",
      "MSE  : 2.0165\n",
      "RMSE : 1.4200\n",
      "RÂ²   : 0.9361\n"
     ]
    }
   ],
   "source": [
    "# Ambil best params dari grid search\n",
    "best_params = grid_search_euclidean.best_params_\n",
    "\n",
    "# Fit ulang model dengan best params\n",
    "best_knn = KNeighborsRegressor(**best_params)\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred = best_knn.predict(X_train_scaled)\n",
    "y_test_pred = best_knn.predict(X_test_scaled)\n",
    "\n",
    "# Custom MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    nonzero_idx = y_true != 0\n",
    "    return np.mean(np.abs((y_true[nonzero_idx] - y_pred[nonzero_idx]) / y_true[nonzero_idx])) * 100\n",
    "\n",
    "# Hitung metrik Train\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Hitung metrik Test\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"=== Train Metrics ===\")\n",
    "print(f\"MAE  : {mae_train:.4f}\")\n",
    "print(f\"MAPE : {mape_train:.4f}%\")\n",
    "print(f\"MSE  : {mse_train:.4f}\")\n",
    "print(f\"RMSE : {rmse_train:.4f}\")\n",
    "print(f\"RÂ²   : {r2_train:.4f}\")\n",
    "\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "print(f\"MAE  : {mae_test:.4f}\")\n",
    "print(f\"MAPE : {mape_test:.4f}%\")\n",
    "print(f\"MSE  : {mse_test:.4f}\")\n",
    "print(f\"RMSE : {rmse_test:.4f}\")\n",
    "print(f\"RÂ²   : {r2_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d108fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26afecd",
   "metadata": {},
   "source": [
    "# Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "79445118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List untuk menyimpan hasil\n",
    "results_knn = []\n",
    "\n",
    "# Range n_neighbors (sesuaikan jumlah baris data training)\n",
    "neighbors = range(10, 17)\n",
    "weights_options = ['uniform', 'distance']\n",
    "\n",
    "# Loop semua kombinasi k dan weights\n",
    "for k in neighbors:\n",
    "    for w in weights_options:\n",
    "        knn = KNeighborsRegressor(\n",
    "            n_neighbors=k,\n",
    "            weights=w,\n",
    "            metric='manhattan'   # langsung manhattan tidak perlu p \n",
    "        )\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "        y_train_pred = knn.predict(X_train_scaled)\n",
    "        y_test_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "        # Hitung metrik evaluasi\n",
    "        mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "        mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        # Tambahkan MAPE (hindari pembagian nol)\n",
    "        mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "        mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "\n",
    "        mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "        mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        rmse_train = sqrt(mse_train)\n",
    "        rmse_test = sqrt(mse_test)\n",
    "\n",
    "        r2_train = r2_score(y_train, y_train_pred)\n",
    "        r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "        # Simpan hasil ke list (dengan urutan sesuai permintaan: MAE, MAPE, MSE, RMSE, R2)\n",
    "        results_knn.append({\n",
    "            'n_neighbors': k,\n",
    "            'weights': w,\n",
    "            'metric': 'manhattan', \n",
    "            'MAE_train': mae_train,\n",
    "            'MAE_test': mae_test,\n",
    "            'MAPE_train': mape_train,\n",
    "            'MAPE_test': mape_test,\n",
    "            'MSE_train': mse_train,\n",
    "            'MSE_test': mse_test,\n",
    "            'RMSE_train': rmse_train,\n",
    "            'RMSE_test': rmse_test,\n",
    "            'R2_train': r2_train,\n",
    "            'R2_test': r2_test,\n",
    "            'R2_selisih': r2_train-r2_test\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ad01384c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>weights</th>\n",
       "      <th>metric</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.006789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.653230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.842868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.357523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941614</td>\n",
       "      <td>0.058386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.027288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.875027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.887234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.373766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940209</td>\n",
       "      <td>0.059791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.012459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.946707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.894164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.376286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.939989</td>\n",
       "      <td>0.060011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.017284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.192384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.969144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.403262</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937613</td>\n",
       "      <td>0.062387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.804625</td>\n",
       "      <td>1.080750</td>\n",
       "      <td>10.115601</td>\n",
       "      <td>10.480666</td>\n",
       "      <td>1.594315</td>\n",
       "      <td>2.174948</td>\n",
       "      <td>1.262662</td>\n",
       "      <td>1.474770</td>\n",
       "      <td>0.938529</td>\n",
       "      <td>0.931093</td>\n",
       "      <td>0.007436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.079678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.797265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.179028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.476153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930964</td>\n",
       "      <td>0.069036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.089724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.116170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.250728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500243</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928692</td>\n",
       "      <td>0.071308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.839659</td>\n",
       "      <td>1.102045</td>\n",
       "      <td>10.724059</td>\n",
       "      <td>10.699324</td>\n",
       "      <td>1.806536</td>\n",
       "      <td>2.275436</td>\n",
       "      <td>1.344074</td>\n",
       "      <td>1.508455</td>\n",
       "      <td>0.930347</td>\n",
       "      <td>0.927909</td>\n",
       "      <td>0.002438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.864323</td>\n",
       "      <td>1.104167</td>\n",
       "      <td>10.742483</td>\n",
       "      <td>10.967911</td>\n",
       "      <td>1.897225</td>\n",
       "      <td>2.313292</td>\n",
       "      <td>1.377398</td>\n",
       "      <td>1.520951</td>\n",
       "      <td>0.926850</td>\n",
       "      <td>0.926710</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.107839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.289336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.411780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.552991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923590</td>\n",
       "      <td>0.076410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.920288</td>\n",
       "      <td>1.114231</td>\n",
       "      <td>11.248638</td>\n",
       "      <td>11.312152</td>\n",
       "      <td>2.033172</td>\n",
       "      <td>2.442820</td>\n",
       "      <td>1.425893</td>\n",
       "      <td>1.562952</td>\n",
       "      <td>0.921609</td>\n",
       "      <td>0.922606</td>\n",
       "      <td>-0.000998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.973973</td>\n",
       "      <td>1.203571</td>\n",
       "      <td>11.757213</td>\n",
       "      <td>12.167429</td>\n",
       "      <td>2.205048</td>\n",
       "      <td>2.766995</td>\n",
       "      <td>1.484940</td>\n",
       "      <td>1.663429</td>\n",
       "      <td>0.914982</td>\n",
       "      <td>0.912336</td>\n",
       "      <td>0.002646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>1.002792</td>\n",
       "      <td>1.208167</td>\n",
       "      <td>11.922555</td>\n",
       "      <td>12.498489</td>\n",
       "      <td>2.367913</td>\n",
       "      <td>2.862159</td>\n",
       "      <td>1.538802</td>\n",
       "      <td>1.691792</td>\n",
       "      <td>0.908703</td>\n",
       "      <td>0.909321</td>\n",
       "      <td>-0.000618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>1.035742</td>\n",
       "      <td>1.256406</td>\n",
       "      <td>12.311976</td>\n",
       "      <td>12.915194</td>\n",
       "      <td>2.523584</td>\n",
       "      <td>3.123347</td>\n",
       "      <td>1.588579</td>\n",
       "      <td>1.767299</td>\n",
       "      <td>0.902701</td>\n",
       "      <td>0.901046</td>\n",
       "      <td>0.001655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_neighbors   weights     metric  MAE_train  MAE_test  MAPE_train  \\\n",
       "0            10  distance  manhattan   0.000000  1.006789    0.000000   \n",
       "1            11  distance  manhattan   0.000000  1.027288    0.000000   \n",
       "2            12  distance  manhattan   0.000000  1.012459    0.000000   \n",
       "3            13  distance  manhattan   0.000000  1.017284    0.000000   \n",
       "4            10   uniform  manhattan   0.804625  1.080750   10.115601   \n",
       "5            14  distance  manhattan   0.000000  1.079678    0.000000   \n",
       "6            15  distance  manhattan   0.000000  1.089724    0.000000   \n",
       "7            11   uniform  manhattan   0.839659  1.102045   10.724059   \n",
       "8            12   uniform  manhattan   0.864323  1.104167   10.742483   \n",
       "9            16  distance  manhattan   0.000000  1.107839    0.000000   \n",
       "10           13   uniform  manhattan   0.920288  1.114231   11.248638   \n",
       "11           14   uniform  manhattan   0.973973  1.203571   11.757213   \n",
       "12           15   uniform  manhattan   1.002792  1.208167   11.922555   \n",
       "13           16   uniform  manhattan   1.035742  1.256406   12.311976   \n",
       "\n",
       "    MAPE_test  MSE_train  MSE_test  RMSE_train  RMSE_test  R2_train   R2_test  \\\n",
       "0    9.653230   0.000000  1.842868    0.000000   1.357523  1.000000  0.941614   \n",
       "1    9.875027   0.000000  1.887234    0.000000   1.373766  1.000000  0.940209   \n",
       "2    9.946707   0.000000  1.894164    0.000000   1.376286  1.000000  0.939989   \n",
       "3   10.192384   0.000000  1.969144    0.000000   1.403262  1.000000  0.937613   \n",
       "4   10.480666   1.594315  2.174948    1.262662   1.474770  0.938529  0.931093   \n",
       "5   10.797265   0.000000  2.179028    0.000000   1.476153  1.000000  0.930964   \n",
       "6   11.116170   0.000000  2.250728    0.000000   1.500243  1.000000  0.928692   \n",
       "7   10.699324   1.806536  2.275436    1.344074   1.508455  0.930347  0.927909   \n",
       "8   10.967911   1.897225  2.313292    1.377398   1.520951  0.926850  0.926710   \n",
       "9   11.289336   0.000000  2.411780    0.000000   1.552991  1.000000  0.923590   \n",
       "10  11.312152   2.033172  2.442820    1.425893   1.562952  0.921609  0.922606   \n",
       "11  12.167429   2.205048  2.766995    1.484940   1.663429  0.914982  0.912336   \n",
       "12  12.498489   2.367913  2.862159    1.538802   1.691792  0.908703  0.909321   \n",
       "13  12.915194   2.523584  3.123347    1.588579   1.767299  0.902701  0.901046   \n",
       "\n",
       "    R2_selisih  \n",
       "0     0.058386  \n",
       "1     0.059791  \n",
       "2     0.060011  \n",
       "3     0.062387  \n",
       "4     0.007436  \n",
       "5     0.069036  \n",
       "6     0.071308  \n",
       "7     0.002438  \n",
       "8     0.000140  \n",
       "9     0.076410  \n",
       "10   -0.000998  \n",
       "11    0.002646  \n",
       "12   -0.000618  \n",
       "13    0.001655  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Konversi ke DataFrame biar lebih enak dibaca\n",
    "df_knn_manhattan = pd.DataFrame(results_knn)\n",
    "\n",
    "#Tampilkan haisl \n",
    "df_knn_manhattan.sort_values(by='R2_test', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3984ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indeks ke 4 menarik karena gak overfit di train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a3992cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'metric': 'minkowski', 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "Best CV Score (R2): 0.9039904626375618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definisi parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': range(10, 16),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['manhattan'],\n",
    "}\n",
    "\n",
    "# Model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# GridSearchCV dengan 5-fold cross-validation\n",
    "grid_search_euclidean = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',   # bisa ganti 'neg_mean_absolute_error', 'neg_mean_squared_error', dll\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit ke data train\n",
    "grid_search_manhattan.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid_search_manhattan.best_params_)\n",
    "print(\"Best CV Score (R2):\", grid_search_manhattan.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "db74c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train Metrics ===\n",
      "MAE  : 0.0000\n",
      "MAPE : 0.0000%\n",
      "MSE  : 0.0000\n",
      "RMSE : 0.0000\n",
      "RÂ²   : 1.0000\n",
      "\n",
      "=== Test Metrics ===\n",
      "MAE  : 1.0068\n",
      "MAPE : 9.6532%\n",
      "MSE  : 1.8429\n",
      "RMSE : 1.3575\n",
      "RÂ²   : 0.9416\n"
     ]
    }
   ],
   "source": [
    "# Ambil best params dari grid search\n",
    "best_params = grid_search_manhattan.best_params_\n",
    "\n",
    "# Fit ulang model dengan best params\n",
    "best_knn = KNeighborsRegressor(**best_params)\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred = best_knn.predict(X_train_scaled)\n",
    "y_test_pred = best_knn.predict(X_test_scaled)\n",
    "\n",
    "# Custom MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    nonzero_idx = y_true != 0\n",
    "    return np.mean(np.abs((y_true[nonzero_idx] - y_pred[nonzero_idx]) / y_true[nonzero_idx])) * 100\n",
    "\n",
    "# Hitung metrik Train\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Hitung metrik Test\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"=== Train Metrics ===\")\n",
    "print(f\"MAE  : {mae_train:.4f}\")\n",
    "print(f\"MAPE : {mape_train:.4f}%\")\n",
    "print(f\"MSE  : {mse_train:.4f}\")\n",
    "print(f\"RMSE : {rmse_train:.4f}\")\n",
    "print(f\"RÂ²   : {r2_train:.4f}\")\n",
    "\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "print(f\"MAE  : {mae_test:.4f}\")\n",
    "print(f\"MAPE : {mape_test:.4f}%\")\n",
    "print(f\"MSE  : {mse_test:.4f}\")\n",
    "print(f\"RMSE : {rmse_test:.4f}\")\n",
    "print(f\"RÂ²   : {r2_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8a6da98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c8150c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kesimpulan dari KNN Regressor nilai yang terbaik adalah nilai terbaik dari manhattan dan minkowski sama baiknya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ef8f1",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bd2c4cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "DecisionTreeRegressor(\n",
      "    *,\n",
      "    criterion=\u001b[33m'squared_error'\u001b[39m,\n",
      "    splitter=\u001b[33m'best'\u001b[39m,\n",
      "    max_depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    min_samples_split=\u001b[32m2\u001b[39m,\n",
      "    min_samples_leaf=\u001b[32m1\u001b[39m,\n",
      "    min_weight_fraction_leaf=\u001b[32m0.0\u001b[39m,\n",
      "    max_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    max_leaf_nodes=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    min_impurity_decrease=\u001b[32m0.0\u001b[39m,\n",
      "    ccp_alpha=\u001b[32m0.0\u001b[39m,\n",
      "    monotonic_cst=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "A decision tree regressor.\n",
      "\n",
      "Read more in the :ref:`User Guide <tree>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "criterion : {\"squared_error\", \"friedman_mse\", \"absolute_error\",             \"poisson\"}, default=\"squared_error\"\n",
      "    The function to measure the quality of a split. Supported criteria\n",
      "    are \"squared_error\" for the mean squared error, which is equal to\n",
      "    variance reduction as feature selection criterion and minimizes the L2\n",
      "    loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
      "    mean squared error with Friedman's improvement score for potential\n",
      "    splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
      "    the L1 loss using the median of each terminal node, and \"poisson\" which\n",
      "    uses reduction in the half mean Poisson deviance to find splits.\n",
      "\n",
      "    .. versionadded:: 0.18\n",
      "       Mean Absolute Error (MAE) criterion.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "        Poisson deviance criterion.\n",
      "\n",
      "splitter : {\"best\", \"random\"}, default=\"best\"\n",
      "    The strategy used to choose the split at each node. Supported\n",
      "    strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "    the best random split.\n",
      "\n",
      "max_depth : int, default=None\n",
      "    The maximum depth of the tree. If None, then nodes are expanded until\n",
      "    all leaves are pure or until all leaves contain less than\n",
      "    min_samples_split samples.\n",
      "\n",
      "    For an example of how ``max_depth`` influences the model, see\n",
      "    :ref:`sphx_glr_auto_examples_tree_plot_tree_regression.py`.\n",
      "\n",
      "min_samples_split : int or float, default=2\n",
      "    The minimum number of samples required to split an internal node:\n",
      "\n",
      "    - If int, then consider `min_samples_split` as the minimum number.\n",
      "    - If float, then `min_samples_split` is a fraction and\n",
      "      `ceil(min_samples_split * n_samples)` are the minimum\n",
      "      number of samples for each split.\n",
      "\n",
      "    .. versionchanged:: 0.18\n",
      "       Added float values for fractions.\n",
      "\n",
      "min_samples_leaf : int or float, default=1\n",
      "    The minimum number of samples required to be at a leaf node.\n",
      "    A split point at any depth will only be considered if it leaves at\n",
      "    least ``min_samples_leaf`` training samples in each of the left and\n",
      "    right branches.  This may have the effect of smoothing the model,\n",
      "    especially in regression.\n",
      "\n",
      "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "    - If float, then `min_samples_leaf` is a fraction and\n",
      "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "      number of samples for each node.\n",
      "\n",
      "    .. versionchanged:: 0.18\n",
      "       Added float values for fractions.\n",
      "\n",
      "min_weight_fraction_leaf : float, default=0.0\n",
      "    The minimum weighted fraction of the sum total of weights (of all\n",
      "    the input samples) required to be at a leaf node. Samples have\n",
      "    equal weight when sample_weight is not provided.\n",
      "\n",
      "max_features : int, float or {\"sqrt\", \"log2\"}, default=None\n",
      "    The number of features to consider when looking for the best split:\n",
      "\n",
      "    - If int, then consider `max_features` features at each split.\n",
      "    - If float, then `max_features` is a fraction and\n",
      "      `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      "      split.\n",
      "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "    - If \"log2\", then `max_features=log2(n_features)`.\n",
      "    - If None, then `max_features=n_features`.\n",
      "\n",
      "    Note: the search for a split does not stop until at least one\n",
      "    valid partition of the node samples is found, even if it requires to\n",
      "    effectively inspect more than ``max_features`` features.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Controls the randomness of the estimator. The features are always\n",
      "    randomly permuted at each split, even if ``splitter`` is set to\n",
      "    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      "    select ``max_features`` at random at each split before finding the best\n",
      "    split among them. But the best found split may vary across different\n",
      "    runs, even if ``max_features=n_features``. That is the case, if the\n",
      "    improvement of the criterion is identical for several splits and one\n",
      "    split has to be selected at random. To obtain a deterministic behaviour\n",
      "    during fitting, ``random_state`` has to be fixed to an integer.\n",
      "    See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "max_leaf_nodes : int, default=None\n",
      "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "    Best nodes are defined as relative reduction in impurity.\n",
      "    If None then unlimited number of leaf nodes.\n",
      "\n",
      "min_impurity_decrease : float, default=0.0\n",
      "    A node will be split if this split induces a decrease of the impurity\n",
      "    greater than or equal to this value.\n",
      "\n",
      "    The weighted impurity decrease equation is the following::\n",
      "\n",
      "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                            - N_t_L / N_t * left_impurity)\n",
      "\n",
      "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "    if ``sample_weight`` is passed.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "\n",
      "ccp_alpha : non-negative float, default=0.0\n",
      "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "    subtree with the largest cost complexity that is smaller than\n",
      "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "    :ref:`minimal_cost_complexity_pruning` for details. See\n",
      "    :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
      "    for an example of such pruning.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\n",
      "monotonic_cst : array-like of int of shape (n_features), default=None\n",
      "    Indicates the monotonicity constraint to enforce on each feature.\n",
      "      - 1: monotonic increase\n",
      "      - 0: no constraint\n",
      "      - -1: monotonic decrease\n",
      "\n",
      "    If monotonic_cst is None, no constraints are applied.\n",
      "\n",
      "    Monotonicity constraints are not supported for:\n",
      "      - multioutput regressions (i.e. when `n_outputs_ > 1`),\n",
      "      - regressions trained on data with missing values.\n",
      "\n",
      "    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "\n",
      "    .. versionadded:: 1.4\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "feature_importances_ : ndarray of shape (n_features,)\n",
      "    The feature importances.\n",
      "    The higher, the more important the feature.\n",
      "    The importance of a feature is computed as the\n",
      "    (normalized) total reduction of the criterion brought\n",
      "    by that feature. It is also known as the Gini importance [4]_.\n",
      "\n",
      "    Warning: impurity-based feature importances can be misleading for\n",
      "    high cardinality features (many unique values). See\n",
      "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "max_features_ : int\n",
      "    The inferred value of max_features.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_outputs_ : int\n",
      "    The number of outputs when ``fit`` is performed.\n",
      "\n",
      "tree_ : Tree instance\n",
      "    The underlying Tree object. Please refer to\n",
      "    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      "    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      "    for basic usage of these attributes.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "DecisionTreeClassifier : A decision tree classifier.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The default values for the parameters controlling the size of the trees\n",
      "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "unpruned trees which can potentially be very large on some data sets. To\n",
      "reduce memory consumption, the complexity and size of the trees should be\n",
      "controlled by setting those parameter values.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      ".. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "\n",
      ".. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "\n",
      ".. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "       Learning\", Springer, 2009.\n",
      "\n",
      ".. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.datasets import load_diabetes\n",
      ">>> from sklearn.model_selection import cross_val_score\n",
      ">>> from sklearn.tree import DecisionTreeRegressor\n",
      ">>> X, y = load_diabetes(return_X_y=True)\n",
      ">>> regressor = DecisionTreeRegressor(random_state=0)\n",
      ">>> cross_val_score(regressor, X, y, cv=10)\n",
      "...                    # doctest: +SKIP\n",
      "...\n",
      "array([-0.39, -0.46,  0.02,  0.06, -0.50,\n",
      "       0.16,  0.11, -0.73, -0.30, -0.00])\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\asus\\anaconda3\\envs\\clustering_env_clean\\lib\\site-packages\\sklearn\\tree\\_classes.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     ExtraTreeRegressor"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "?DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1ca6c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decision Tree Regressor Train ===\n",
      "MAE: 0.0\n",
      "MAPE: 0.0\n",
      "MSE: 0.0\n",
      "RMSE: 0.0\n",
      "R2: 1.0\n",
      "\n",
      "=== Decision Tree Regressor Test ===\n",
      "MAE: 0.9850000000000001\n",
      "MAPE: 8.885090290205047\n",
      "MSE: 2.175\n",
      "RMSE: 1.4747881203752624\n",
      "R2: 0.9310914968293178\n"
     ]
    }
   ],
   "source": [
    "# Buat model Decision Tree\n",
    "tree = DecisionTreeRegressor(random_state=42)  \n",
    "tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred_tree = tree.predict(X_train_scaled)\n",
    "y_test_pred_tree = tree.predict(X_test_scaled)\n",
    "\n",
    "# Evaluasi Train\n",
    "mse_train_tree = mean_squared_error(y_train, y_train_pred_tree)\n",
    "rmse_train_tree = sqrt(mse_train_tree)\n",
    "mae_train_tree = mean_absolute_error(y_train, y_train_pred_tree)\n",
    "r2_train_tree = r2_score(y_train, y_train_pred_tree)\n",
    "mape_train_tree = np.mean(np.abs((y_train - y_train_pred_tree) / y_train)) * 100\n",
    "\n",
    "# Evaluasi Test\n",
    "mse_test_tree = mean_squared_error(y_test, y_test_pred_tree)\n",
    "rmse_test_tree = sqrt(mse_test_tree)\n",
    "mae_test_tree = mean_absolute_error(y_test, y_test_pred_tree)\n",
    "r2_test_tree = r2_score(y_test, y_test_pred_tree)\n",
    "mape_test_tree = np.mean(np.abs((y_test - y_test_pred_tree) / y_test)) * 100\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"=== Decision Tree Regressor Train ===\")\n",
    "print(\"MAE:\", mae_train_tree)\n",
    "print(\"MAPE:\", mape_train_tree)\n",
    "print(\"MSE:\", mse_train_tree)\n",
    "print(\"RMSE:\", rmse_train_tree)\n",
    "print(\"R2:\", r2_train_tree)\n",
    "\n",
    "print(\"\\n=== Decision Tree Regressor Test ===\")\n",
    "print(\"MAE:\", mae_test_tree)\n",
    "print(\"MAPE:\", mape_test_tree)\n",
    "print(\"MSE:\", mse_test_tree)\n",
    "print(\"RMSE:\", rmse_test_tree)\n",
    "print(\"R2:\", r2_test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5c692e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0fcab458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285af026",
   "metadata": {},
   "source": [
    "max_depth â‰ˆ logâ‚‚(n) s/d logâ‚‚(n)/2 â†’ supaya pohon tidak terlalu dalam (overfit) atau terlalu dangkal (underfit).\n",
    "\n",
    "min_samples_split â‰ˆ 2 s/d 0.01Ã—n â†’ mencegah percabangan terlalu kecil.\n",
    "\n",
    "min_samples_leaf â‰ˆ 1 s/d 0.01Ã—n â†’ menjaga daun tidak berisi terlalu sedikit sampel.\n",
    "\n",
    "Criterion: squared error, absolute error, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6506951",
   "metadata": {},
   "source": [
    "#  Criterion squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f6d1441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Grid parameter\n",
    "param_grid = {\n",
    "    'max_depth': list(range(1, 8)),          # 1 sampai 7\n",
    "    'min_samples_split': list(range(2, 11)), # 2 sampai 10\n",
    "    'min_samples_leaf': list(range(1, 6))    # 1 sampai 5\n",
    "}\n",
    "\n",
    "# List untuk menyimpan hasil\n",
    "results = []\n",
    "\n",
    "# Loop semua kombinasi parameter\n",
    "for max_depth, min_samples_split, min_samples_leaf in product(\n",
    "    param_grid['max_depth'], \n",
    "    param_grid['min_samples_split'], \n",
    "    param_grid['min_samples_leaf']\n",
    "):\n",
    "    # Buat model\n",
    "    tree = DecisionTreeRegressor(\n",
    "        criterion='squared_error',\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Latih model\n",
    "    tree.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prediksi\n",
    "    y_train_pred = tree.predict(X_train_scaled)\n",
    "    y_test_pred = tree.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluasi\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    \n",
    "    # Simpan hasil dengan urutan kolom sesuai permintaan\n",
    "    results.append({\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'criterion': 'squared_error',\n",
    "        'MAE_train': mae_train,\n",
    "        'MAE_test': mae_test,\n",
    "        'MAPE_train': mape_train,\n",
    "        'MAPE_test': mape_test,\n",
    "        'MSE_train': mse_train,\n",
    "        'MSE_test': mse_test,\n",
    "        'RMSE_train': rmse_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'R2_train': r2_train,\n",
    "        'R2_test': r2_test,\n",
    "        'R2_selisih': r2_train-r2_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f34a9d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>criterion</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.548717</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>5.704430</td>\n",
       "      <td>7.658700</td>\n",
       "      <td>0.515518</td>\n",
       "      <td>1.663544</td>\n",
       "      <td>0.717996</td>\n",
       "      <td>1.289784</td>\n",
       "      <td>0.980124</td>\n",
       "      <td>0.947295</td>\n",
       "      <td>0.032828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.548717</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>5.704430</td>\n",
       "      <td>7.658700</td>\n",
       "      <td>0.515518</td>\n",
       "      <td>1.663544</td>\n",
       "      <td>0.717996</td>\n",
       "      <td>1.289784</td>\n",
       "      <td>0.980124</td>\n",
       "      <td>0.947295</td>\n",
       "      <td>0.032828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.548717</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>5.704430</td>\n",
       "      <td>7.658700</td>\n",
       "      <td>0.515518</td>\n",
       "      <td>1.663544</td>\n",
       "      <td>0.717996</td>\n",
       "      <td>1.289784</td>\n",
       "      <td>0.980124</td>\n",
       "      <td>0.947295</td>\n",
       "      <td>0.032828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_samples_split  min_samples_leaf      criterion  MAE_train  \\\n",
       "0           7                 10                 5  squared_error   0.611429   \n",
       "1           7                  9                 5  squared_error   0.611429   \n",
       "2           7                  5                 5  squared_error   0.611429   \n",
       "3           6                 10                 5  squared_error   0.611429   \n",
       "4           7                  3                 5  squared_error   0.611429   \n",
       "5           7                  4                 5  squared_error   0.611429   \n",
       "6           6                  9                 5  squared_error   0.611429   \n",
       "7           7                  2                 5  squared_error   0.611429   \n",
       "8           6                  4                 5  squared_error   0.611429   \n",
       "9           6                  3                 5  squared_error   0.611429   \n",
       "10          6                  6                 5  squared_error   0.611429   \n",
       "11          6                  5                 5  squared_error   0.611429   \n",
       "12          7                  7                 5  squared_error   0.611429   \n",
       "13          7                  8                 5  squared_error   0.611429   \n",
       "14          7                  6                 5  squared_error   0.611429   \n",
       "15          6                  2                 5  squared_error   0.611429   \n",
       "16          6                  7                 5  squared_error   0.611429   \n",
       "17          6                  8                 5  squared_error   0.611429   \n",
       "18          5                  6                 5  squared_error   0.626025   \n",
       "19          5                  8                 5  squared_error   0.626025   \n",
       "20          5                  7                 5  squared_error   0.626025   \n",
       "21          5                  9                 5  squared_error   0.626025   \n",
       "22          5                  4                 5  squared_error   0.626025   \n",
       "23          5                  2                 5  squared_error   0.626025   \n",
       "24          5                  3                 5  squared_error   0.626025   \n",
       "25          5                 10                 5  squared_error   0.626025   \n",
       "26          5                  5                 5  squared_error   0.626025   \n",
       "27          7                  7                 4  squared_error   0.548717   \n",
       "28          7                  6                 4  squared_error   0.548717   \n",
       "29          7                  8                 4  squared_error   0.548717   \n",
       "\n",
       "    MAE_test  MAPE_train  MAPE_test  MSE_train  MSE_test  RMSE_train  \\\n",
       "0   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "1   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "2   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "3   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "4   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "5   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "6   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "7   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "8   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "9   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "10  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "11  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "12  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "13  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "14  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "15  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "16  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "17  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "18  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "19  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "20  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "21  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "22  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "23  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "24  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "25  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "26  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "27  0.988494    5.704430   7.658700   0.515518  1.663544    0.717996   \n",
       "28  0.988494    5.704430   7.658700   0.515518  1.663544    0.717996   \n",
       "29  0.988494    5.704430   7.658700   0.515518  1.663544    0.717996   \n",
       "\n",
       "    RMSE_test  R2_train   R2_test  R2_selisih  \n",
       "0    1.256104  0.974891  0.950012    0.024879  \n",
       "1    1.256104  0.974891  0.950012    0.024879  \n",
       "2    1.256104  0.974891  0.950012    0.024879  \n",
       "3    1.256104  0.974891  0.950012    0.024879  \n",
       "4    1.256104  0.974891  0.950012    0.024879  \n",
       "5    1.256104  0.974891  0.950012    0.024879  \n",
       "6    1.256104  0.974891  0.950012    0.024879  \n",
       "7    1.256104  0.974891  0.950012    0.024879  \n",
       "8    1.256104  0.974891  0.950012    0.024879  \n",
       "9    1.256104  0.974891  0.950012    0.024879  \n",
       "10   1.256104  0.974891  0.950012    0.024879  \n",
       "11   1.256104  0.974891  0.950012    0.024879  \n",
       "12   1.256104  0.974891  0.950012    0.024879  \n",
       "13   1.256104  0.974891  0.950012    0.024879  \n",
       "14   1.256104  0.974891  0.950012    0.024879  \n",
       "15   1.256104  0.974891  0.950012    0.024879  \n",
       "16   1.256104  0.974891  0.950012    0.024879  \n",
       "17   1.256104  0.974891  0.950012    0.024879  \n",
       "18   1.281675  0.973926  0.947956    0.025970  \n",
       "19   1.281675  0.973926  0.947956    0.025970  \n",
       "20   1.281675  0.973926  0.947956    0.025970  \n",
       "21   1.281675  0.973926  0.947956    0.025970  \n",
       "22   1.281675  0.973926  0.947956    0.025970  \n",
       "23   1.281675  0.973926  0.947956    0.025970  \n",
       "24   1.281675  0.973926  0.947956    0.025970  \n",
       "25   1.281675  0.973926  0.947956    0.025970  \n",
       "26   1.281675  0.973926  0.947956    0.025970  \n",
       "27   1.289784  0.980124  0.947295    0.032828  \n",
       "28   1.289784  0.980124  0.947295    0.032828  \n",
       "29   1.289784  0.980124  0.947295    0.032828  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ubah jadi DataFrame\n",
    "df_dt_squared_error = pd.DataFrame(results)\n",
    "\n",
    "# Urutkan berdasarkan R2_test (model terbaik di atas)\n",
    "df_dt_squared_error = df_dt_squared_error.sort_values(by=\"R2_test\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_dt_squared_error.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f4f60b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index ke 0 yang terbaik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256114c",
   "metadata": {},
   "source": [
    "# Criterion: friedman mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad4e4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Grid parameter\n",
    "param_grid = {\n",
    "    'max_depth': list(range(1, 8)),          # 1 sampai 7\n",
    "    'min_samples_split': list(range(2, 11)), # 2 sampai 10\n",
    "    'min_samples_leaf': list(range(1, 6))    # 1 sampai 5\n",
    "}\n",
    "\n",
    "# List untuk menyimpan hasil\n",
    "results = []\n",
    "\n",
    "# Loop semua kombinasi parameter\n",
    "for max_depth, min_samples_split, min_samples_leaf in product(\n",
    "    param_grid['max_depth'], \n",
    "    param_grid['min_samples_split'], \n",
    "    param_grid['min_samples_leaf']\n",
    "):\n",
    "    # Buat model\n",
    "    tree = DecisionTreeRegressor(\n",
    "        criterion='friedman_mse',\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Latih model\n",
    "    tree.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prediksi\n",
    "    y_train_pred = tree.predict(X_train_scaled)\n",
    "    y_test_pred = tree.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluasi\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    \n",
    "    # Simpan hasil dengan urutan kolom sesuai permintaan\n",
    "    results.append({\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'criterion': 'friedman_mse',\n",
    "        'MAE_train': mae_train,\n",
    "        'MAE_test': mae_test,\n",
    "        'MAPE_train': mape_train,\n",
    "        'MAPE_test': mape_test,\n",
    "        'MSE_train': mse_train,\n",
    "        'MSE_test': mse_test,\n",
    "        'RMSE_train': rmse_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'R2_train': r2_train,\n",
    "        'R2_test': r2_test,\n",
    "        'R2_selisih': r2_train-r2_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d46fefa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>criterion</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>6.163415</td>\n",
       "      <td>7.207968</td>\n",
       "      <td>0.651229</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>0.806988</td>\n",
       "      <td>1.256104</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.024879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>6.277148</td>\n",
       "      <td>7.602636</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>1.642691</td>\n",
       "      <td>0.822344</td>\n",
       "      <td>1.281675</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.548717</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>5.704430</td>\n",
       "      <td>7.658700</td>\n",
       "      <td>0.515518</td>\n",
       "      <td>1.663544</td>\n",
       "      <td>0.717996</td>\n",
       "      <td>1.289784</td>\n",
       "      <td>0.980124</td>\n",
       "      <td>0.947295</td>\n",
       "      <td>0.032828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.548717</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>5.704430</td>\n",
       "      <td>7.658700</td>\n",
       "      <td>0.515518</td>\n",
       "      <td>1.663544</td>\n",
       "      <td>0.717996</td>\n",
       "      <td>1.289784</td>\n",
       "      <td>0.980124</td>\n",
       "      <td>0.947295</td>\n",
       "      <td>0.032828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.548717</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>5.704430</td>\n",
       "      <td>7.658700</td>\n",
       "      <td>0.515518</td>\n",
       "      <td>1.663544</td>\n",
       "      <td>0.717996</td>\n",
       "      <td>1.289784</td>\n",
       "      <td>0.980124</td>\n",
       "      <td>0.947295</td>\n",
       "      <td>0.032828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_samples_split  min_samples_leaf     criterion  MAE_train  \\\n",
       "0           7                 10                 5  friedman_mse   0.611429   \n",
       "1           7                  9                 5  friedman_mse   0.611429   \n",
       "2           7                  5                 5  friedman_mse   0.611429   \n",
       "3           7                  2                 5  friedman_mse   0.611429   \n",
       "4           6                 10                 5  friedman_mse   0.611429   \n",
       "5           7                  3                 5  friedman_mse   0.611429   \n",
       "6           7                  4                 5  friedman_mse   0.611429   \n",
       "7           6                  9                 5  friedman_mse   0.611429   \n",
       "8           6                  2                 5  friedman_mse   0.611429   \n",
       "9           6                  4                 5  friedman_mse   0.611429   \n",
       "10          6                  3                 5  friedman_mse   0.611429   \n",
       "11          6                  6                 5  friedman_mse   0.611429   \n",
       "12          6                  5                 5  friedman_mse   0.611429   \n",
       "13          7                  7                 5  friedman_mse   0.611429   \n",
       "14          7                  8                 5  friedman_mse   0.611429   \n",
       "15          7                  6                 5  friedman_mse   0.611429   \n",
       "16          6                  8                 5  friedman_mse   0.611429   \n",
       "17          6                  7                 5  friedman_mse   0.611429   \n",
       "18          5                  5                 5  friedman_mse   0.626025   \n",
       "19          5                  6                 5  friedman_mse   0.626025   \n",
       "20          5                  8                 5  friedman_mse   0.626025   \n",
       "21          5                  7                 5  friedman_mse   0.626025   \n",
       "22          5                  9                 5  friedman_mse   0.626025   \n",
       "23          5                  4                 5  friedman_mse   0.626025   \n",
       "24          5                  2                 5  friedman_mse   0.626025   \n",
       "25          5                  3                 5  friedman_mse   0.626025   \n",
       "26          5                 10                 5  friedman_mse   0.626025   \n",
       "27          7                  6                 4  friedman_mse   0.548717   \n",
       "28          7                  7                 4  friedman_mse   0.548717   \n",
       "29          7                  2                 4  friedman_mse   0.548717   \n",
       "\n",
       "    MAE_test  MAPE_train  MAPE_test  MSE_train  MSE_test  RMSE_train  \\\n",
       "0   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "1   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "2   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "3   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "4   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "5   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "6   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "7   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "8   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "9   0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "10  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "11  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "12  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "13  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "14  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "15  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "16  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "17  0.952278    6.163415   7.207968   0.651229  1.577796    0.806988   \n",
       "18  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "19  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "20  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "21  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "22  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "23  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "24  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "25  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "26  0.986116    6.277148   7.602636   0.676250  1.642691    0.822344   \n",
       "27  0.988494    5.704430   7.658700   0.515518  1.663544    0.717996   \n",
       "28  0.988494    5.704430   7.658700   0.515518  1.663544    0.717996   \n",
       "29  0.988494    5.704430   7.658700   0.515518  1.663544    0.717996   \n",
       "\n",
       "    RMSE_test  R2_train   R2_test  R2_selisih  \n",
       "0    1.256104  0.974891  0.950012    0.024879  \n",
       "1    1.256104  0.974891  0.950012    0.024879  \n",
       "2    1.256104  0.974891  0.950012    0.024879  \n",
       "3    1.256104  0.974891  0.950012    0.024879  \n",
       "4    1.256104  0.974891  0.950012    0.024879  \n",
       "5    1.256104  0.974891  0.950012    0.024879  \n",
       "6    1.256104  0.974891  0.950012    0.024879  \n",
       "7    1.256104  0.974891  0.950012    0.024879  \n",
       "8    1.256104  0.974891  0.950012    0.024879  \n",
       "9    1.256104  0.974891  0.950012    0.024879  \n",
       "10   1.256104  0.974891  0.950012    0.024879  \n",
       "11   1.256104  0.974891  0.950012    0.024879  \n",
       "12   1.256104  0.974891  0.950012    0.024879  \n",
       "13   1.256104  0.974891  0.950012    0.024879  \n",
       "14   1.256104  0.974891  0.950012    0.024879  \n",
       "15   1.256104  0.974891  0.950012    0.024879  \n",
       "16   1.256104  0.974891  0.950012    0.024879  \n",
       "17   1.256104  0.974891  0.950012    0.024879  \n",
       "18   1.281675  0.973926  0.947956    0.025970  \n",
       "19   1.281675  0.973926  0.947956    0.025970  \n",
       "20   1.281675  0.973926  0.947956    0.025970  \n",
       "21   1.281675  0.973926  0.947956    0.025970  \n",
       "22   1.281675  0.973926  0.947956    0.025970  \n",
       "23   1.281675  0.973926  0.947956    0.025970  \n",
       "24   1.281675  0.973926  0.947956    0.025970  \n",
       "25   1.281675  0.973926  0.947956    0.025970  \n",
       "26   1.281675  0.973926  0.947956    0.025970  \n",
       "27   1.289784  0.980124  0.947295    0.032828  \n",
       "28   1.289784  0.980124  0.947295    0.032828  \n",
       "29   1.289784  0.980124  0.947295    0.032828  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ubah jadi DataFrame\n",
    "df_dt_friedman_mse = pd.DataFrame(results)\n",
    "\n",
    "# Urutkan berdasarkan R2_test (model terbaik di atas)\n",
    "df_dt_friedman_mse = df_dt_friedman_mse.sort_values(by=\"R2_test\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_dt_friedman_mse.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "588d8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index ke 0 yg terbaik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d954596",
   "metadata": {},
   "source": [
    "# Criterion: absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dc4d1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Grid parameter\n",
    "param_grid = {\n",
    "    'max_depth': list(range(1, 8)),          # 1 sampai 7\n",
    "    'min_samples_split': list(range(2, 11)), # 2 sampai 10\n",
    "    'min_samples_leaf': list(range(1, 6))    # 1 sampai 5\n",
    "}\n",
    "\n",
    "# List untuk menyimpan hasil\n",
    "results = []\n",
    "\n",
    "# Loop semua kombinasi parameter\n",
    "for max_depth, min_samples_split, min_samples_leaf in product(\n",
    "    param_grid['max_depth'], \n",
    "    param_grid['min_samples_split'], \n",
    "    param_grid['min_samples_leaf']\n",
    "):\n",
    "    # Buat model\n",
    "    tree = DecisionTreeRegressor(\n",
    "        criterion='absolute_error',\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Latih model\n",
    "    tree.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prediksi\n",
    "    y_train_pred = tree.predict(X_train_scaled)\n",
    "    y_test_pred = tree.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluasi\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    \n",
    "    # Simpan hasil dengan urutan kolom sesuai permintaan\n",
    "    results.append({\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'criterion': 'absolute_error',\n",
    "        'MAE_train': mae_train,\n",
    "        'MAE_test': mae_test,\n",
    "        'MAPE_train': mape_train,\n",
    "        'MAPE_test': mape_test,\n",
    "        'MSE_train': mse_train,\n",
    "        'MSE_test': mse_test,\n",
    "        'RMSE_train': rmse_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'R2_train': r2_train,\n",
    "        'R2_test': r2_test,\n",
    "        'R2_selisih': r2_train-r2_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1e84a79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>criterion</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.551250</td>\n",
       "      <td>0.91250</td>\n",
       "      <td>5.743865</td>\n",
       "      <td>7.147378</td>\n",
       "      <td>0.640781</td>\n",
       "      <td>1.289625</td>\n",
       "      <td>0.800488</td>\n",
       "      <td>1.135617</td>\n",
       "      <td>0.975294</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>0.016152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.551250</td>\n",
       "      <td>0.91250</td>\n",
       "      <td>5.743865</td>\n",
       "      <td>7.147378</td>\n",
       "      <td>0.640781</td>\n",
       "      <td>1.289625</td>\n",
       "      <td>0.800488</td>\n",
       "      <td>1.135617</td>\n",
       "      <td>0.975294</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>0.016152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.551250</td>\n",
       "      <td>0.91250</td>\n",
       "      <td>5.743865</td>\n",
       "      <td>7.147378</td>\n",
       "      <td>0.640781</td>\n",
       "      <td>1.289625</td>\n",
       "      <td>0.800488</td>\n",
       "      <td>1.135617</td>\n",
       "      <td>0.975294</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>0.016152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.551250</td>\n",
       "      <td>0.91250</td>\n",
       "      <td>5.743865</td>\n",
       "      <td>7.147378</td>\n",
       "      <td>0.640781</td>\n",
       "      <td>1.289625</td>\n",
       "      <td>0.800488</td>\n",
       "      <td>1.135617</td>\n",
       "      <td>0.975294</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>0.016152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.551250</td>\n",
       "      <td>0.91250</td>\n",
       "      <td>5.743865</td>\n",
       "      <td>7.147378</td>\n",
       "      <td>0.640781</td>\n",
       "      <td>1.289625</td>\n",
       "      <td>0.800488</td>\n",
       "      <td>1.135617</td>\n",
       "      <td>0.975294</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>0.016152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.551250</td>\n",
       "      <td>0.91250</td>\n",
       "      <td>5.743865</td>\n",
       "      <td>7.147378</td>\n",
       "      <td>0.640781</td>\n",
       "      <td>1.289625</td>\n",
       "      <td>0.800488</td>\n",
       "      <td>1.135617</td>\n",
       "      <td>0.975294</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>0.016152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.551250</td>\n",
       "      <td>0.91250</td>\n",
       "      <td>5.743865</td>\n",
       "      <td>7.147378</td>\n",
       "      <td>0.640781</td>\n",
       "      <td>1.289625</td>\n",
       "      <td>0.800488</td>\n",
       "      <td>1.135617</td>\n",
       "      <td>0.975294</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>0.016152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>0.89750</td>\n",
       "      <td>5.771117</td>\n",
       "      <td>7.068044</td>\n",
       "      <td>0.665844</td>\n",
       "      <td>1.298625</td>\n",
       "      <td>0.815992</td>\n",
       "      <td>1.139572</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>0.958857</td>\n",
       "      <td>0.015471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.569375</td>\n",
       "      <td>0.91250</td>\n",
       "      <td>5.868897</td>\n",
       "      <td>7.170468</td>\n",
       "      <td>0.673906</td>\n",
       "      <td>1.324125</td>\n",
       "      <td>0.820918</td>\n",
       "      <td>1.150706</td>\n",
       "      <td>0.974017</td>\n",
       "      <td>0.958049</td>\n",
       "      <td>0.015968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.95875</td>\n",
       "      <td>5.898688</td>\n",
       "      <td>7.240266</td>\n",
       "      <td>0.784969</td>\n",
       "      <td>1.482563</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>1.217605</td>\n",
       "      <td>0.969735</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.95875</td>\n",
       "      <td>5.898688</td>\n",
       "      <td>7.240266</td>\n",
       "      <td>0.784969</td>\n",
       "      <td>1.482563</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>1.217605</td>\n",
       "      <td>0.969735</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.95875</td>\n",
       "      <td>5.898688</td>\n",
       "      <td>7.240266</td>\n",
       "      <td>0.784969</td>\n",
       "      <td>1.482563</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>1.217605</td>\n",
       "      <td>0.969735</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.95875</td>\n",
       "      <td>5.898688</td>\n",
       "      <td>7.240266</td>\n",
       "      <td>0.784969</td>\n",
       "      <td>1.482563</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>1.217605</td>\n",
       "      <td>0.969735</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.95875</td>\n",
       "      <td>5.898688</td>\n",
       "      <td>7.240266</td>\n",
       "      <td>0.784969</td>\n",
       "      <td>1.482563</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>1.217605</td>\n",
       "      <td>0.969735</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.95875</td>\n",
       "      <td>5.898688</td>\n",
       "      <td>7.240266</td>\n",
       "      <td>0.784969</td>\n",
       "      <td>1.482563</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>1.217605</td>\n",
       "      <td>0.969735</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.95875</td>\n",
       "      <td>5.898688</td>\n",
       "      <td>7.240266</td>\n",
       "      <td>0.784969</td>\n",
       "      <td>1.482563</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>1.217605</td>\n",
       "      <td>0.969735</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.95875</td>\n",
       "      <td>5.898688</td>\n",
       "      <td>7.240266</td>\n",
       "      <td>0.784969</td>\n",
       "      <td>1.482563</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>1.217605</td>\n",
       "      <td>0.969735</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.95875</td>\n",
       "      <td>5.898688</td>\n",
       "      <td>7.240266</td>\n",
       "      <td>0.784969</td>\n",
       "      <td>1.482563</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>1.217605</td>\n",
       "      <td>0.969735</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.95125</td>\n",
       "      <td>5.903302</td>\n",
       "      <td>7.332629</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>0.841390</td>\n",
       "      <td>1.234782</td>\n",
       "      <td>0.972705</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.021010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.95125</td>\n",
       "      <td>5.903302</td>\n",
       "      <td>7.332629</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>0.841390</td>\n",
       "      <td>1.234782</td>\n",
       "      <td>0.972705</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.021010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.95125</td>\n",
       "      <td>5.903302</td>\n",
       "      <td>7.332629</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>0.841390</td>\n",
       "      <td>1.234782</td>\n",
       "      <td>0.972705</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.021010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.95125</td>\n",
       "      <td>5.903302</td>\n",
       "      <td>7.332629</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>0.841390</td>\n",
       "      <td>1.234782</td>\n",
       "      <td>0.972705</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.021010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.95125</td>\n",
       "      <td>5.903302</td>\n",
       "      <td>7.332629</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>0.841390</td>\n",
       "      <td>1.234782</td>\n",
       "      <td>0.972705</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.021010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.95125</td>\n",
       "      <td>5.903302</td>\n",
       "      <td>7.332629</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>0.841390</td>\n",
       "      <td>1.234782</td>\n",
       "      <td>0.972705</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.021010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.95125</td>\n",
       "      <td>5.903302</td>\n",
       "      <td>7.332629</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>0.841390</td>\n",
       "      <td>1.234782</td>\n",
       "      <td>0.972705</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.021010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.95125</td>\n",
       "      <td>5.903302</td>\n",
       "      <td>7.332629</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>0.841390</td>\n",
       "      <td>1.234782</td>\n",
       "      <td>0.972705</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.021010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.96625</td>\n",
       "      <td>6.001081</td>\n",
       "      <td>7.435053</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>1.550188</td>\n",
       "      <td>0.846168</td>\n",
       "      <td>1.245065</td>\n",
       "      <td>0.972394</td>\n",
       "      <td>0.950887</td>\n",
       "      <td>0.021507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>1.01250</td>\n",
       "      <td>6.030872</td>\n",
       "      <td>7.504851</td>\n",
       "      <td>0.827062</td>\n",
       "      <td>1.708625</td>\n",
       "      <td>0.909430</td>\n",
       "      <td>1.307144</td>\n",
       "      <td>0.968112</td>\n",
       "      <td>0.945867</td>\n",
       "      <td>0.022244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>1.01250</td>\n",
       "      <td>6.030872</td>\n",
       "      <td>7.504851</td>\n",
       "      <td>0.827062</td>\n",
       "      <td>1.708625</td>\n",
       "      <td>0.909430</td>\n",
       "      <td>1.307144</td>\n",
       "      <td>0.968112</td>\n",
       "      <td>0.945867</td>\n",
       "      <td>0.022244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>absolute_error</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>1.01250</td>\n",
       "      <td>6.030872</td>\n",
       "      <td>7.504851</td>\n",
       "      <td>0.827062</td>\n",
       "      <td>1.708625</td>\n",
       "      <td>0.909430</td>\n",
       "      <td>1.307144</td>\n",
       "      <td>0.968112</td>\n",
       "      <td>0.945867</td>\n",
       "      <td>0.022244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_samples_split  min_samples_leaf       criterion  MAE_train  \\\n",
       "0           7                  6                 4  absolute_error   0.551250   \n",
       "1           7                  5                 4  absolute_error   0.551250   \n",
       "2           7                  2                 4  absolute_error   0.551250   \n",
       "3           7                  3                 4  absolute_error   0.551250   \n",
       "4           7                  8                 4  absolute_error   0.551250   \n",
       "5           7                  7                 4  absolute_error   0.551250   \n",
       "6           7                  4                 4  absolute_error   0.551250   \n",
       "7           7                  9                 4  absolute_error   0.557500   \n",
       "8           7                 10                 4  absolute_error   0.569375   \n",
       "9           7                 10                 5  absolute_error   0.580625   \n",
       "10          7                  8                 5  absolute_error   0.580625   \n",
       "11          7                  2                 5  absolute_error   0.580625   \n",
       "12          7                  6                 5  absolute_error   0.580625   \n",
       "13          7                  7                 5  absolute_error   0.580625   \n",
       "14          7                  5                 5  absolute_error   0.580625   \n",
       "15          7                  4                 5  absolute_error   0.580625   \n",
       "16          7                  9                 5  absolute_error   0.580625   \n",
       "17          7                  3                 5  absolute_error   0.580625   \n",
       "18          6                  4                 4  absolute_error   0.581875   \n",
       "19          6                  5                 4  absolute_error   0.581875   \n",
       "20          6                  2                 4  absolute_error   0.581875   \n",
       "21          6                  3                 4  absolute_error   0.581875   \n",
       "22          6                  6                 4  absolute_error   0.581875   \n",
       "23          6                  7                 4  absolute_error   0.581875   \n",
       "24          6                  9                 4  absolute_error   0.581875   \n",
       "25          6                  8                 4  absolute_error   0.581875   \n",
       "26          6                 10                 4  absolute_error   0.593750   \n",
       "27          6                 10                 5  absolute_error   0.605000   \n",
       "28          6                  8                 5  absolute_error   0.605000   \n",
       "29          6                  9                 5  absolute_error   0.605000   \n",
       "\n",
       "    MAE_test  MAPE_train  MAPE_test  MSE_train  MSE_test  RMSE_train  \\\n",
       "0    0.91250    5.743865   7.147378   0.640781  1.289625    0.800488   \n",
       "1    0.91250    5.743865   7.147378   0.640781  1.289625    0.800488   \n",
       "2    0.91250    5.743865   7.147378   0.640781  1.289625    0.800488   \n",
       "3    0.91250    5.743865   7.147378   0.640781  1.289625    0.800488   \n",
       "4    0.91250    5.743865   7.147378   0.640781  1.289625    0.800488   \n",
       "5    0.91250    5.743865   7.147378   0.640781  1.289625    0.800488   \n",
       "6    0.91250    5.743865   7.147378   0.640781  1.289625    0.800488   \n",
       "7    0.89750    5.771117   7.068044   0.665844  1.298625    0.815992   \n",
       "8    0.91250    5.868897   7.170468   0.673906  1.324125    0.820918   \n",
       "9    0.95875    5.898688   7.240266   0.784969  1.482563    0.885985   \n",
       "10   0.95875    5.898688   7.240266   0.784969  1.482563    0.885985   \n",
       "11   0.95875    5.898688   7.240266   0.784969  1.482563    0.885985   \n",
       "12   0.95875    5.898688   7.240266   0.784969  1.482563    0.885985   \n",
       "13   0.95875    5.898688   7.240266   0.784969  1.482563    0.885985   \n",
       "14   0.95875    5.898688   7.240266   0.784969  1.482563    0.885985   \n",
       "15   0.95875    5.898688   7.240266   0.784969  1.482563    0.885985   \n",
       "16   0.95875    5.898688   7.240266   0.784969  1.482563    0.885985   \n",
       "17   0.95875    5.898688   7.240266   0.784969  1.482563    0.885985   \n",
       "18   0.95125    5.903302   7.332629   0.707937  1.524687    0.841390   \n",
       "19   0.95125    5.903302   7.332629   0.707937  1.524687    0.841390   \n",
       "20   0.95125    5.903302   7.332629   0.707937  1.524687    0.841390   \n",
       "21   0.95125    5.903302   7.332629   0.707937  1.524687    0.841390   \n",
       "22   0.95125    5.903302   7.332629   0.707937  1.524687    0.841390   \n",
       "23   0.95125    5.903302   7.332629   0.707937  1.524687    0.841390   \n",
       "24   0.95125    5.903302   7.332629   0.707937  1.524687    0.841390   \n",
       "25   0.95125    5.903302   7.332629   0.707937  1.524687    0.841390   \n",
       "26   0.96625    6.001081   7.435053   0.716000  1.550188    0.846168   \n",
       "27   1.01250    6.030872   7.504851   0.827062  1.708625    0.909430   \n",
       "28   1.01250    6.030872   7.504851   0.827062  1.708625    0.909430   \n",
       "29   1.01250    6.030872   7.504851   0.827062  1.708625    0.909430   \n",
       "\n",
       "    RMSE_test  R2_train   R2_test  R2_selisih  \n",
       "0    1.135617  0.975294  0.959142    0.016152  \n",
       "1    1.135617  0.975294  0.959142    0.016152  \n",
       "2    1.135617  0.975294  0.959142    0.016152  \n",
       "3    1.135617  0.975294  0.959142    0.016152  \n",
       "4    1.135617  0.975294  0.959142    0.016152  \n",
       "5    1.135617  0.975294  0.959142    0.016152  \n",
       "6    1.135617  0.975294  0.959142    0.016152  \n",
       "7    1.139572  0.974328  0.958857    0.015471  \n",
       "8    1.150706  0.974017  0.958049    0.015968  \n",
       "9    1.217605  0.969735  0.953029    0.016705  \n",
       "10   1.217605  0.969735  0.953029    0.016705  \n",
       "11   1.217605  0.969735  0.953029    0.016705  \n",
       "12   1.217605  0.969735  0.953029    0.016705  \n",
       "13   1.217605  0.969735  0.953029    0.016705  \n",
       "14   1.217605  0.969735  0.953029    0.016705  \n",
       "15   1.217605  0.969735  0.953029    0.016705  \n",
       "16   1.217605  0.969735  0.953029    0.016705  \n",
       "17   1.217605  0.969735  0.953029    0.016705  \n",
       "18   1.234782  0.972705  0.951695    0.021010  \n",
       "19   1.234782  0.972705  0.951695    0.021010  \n",
       "20   1.234782  0.972705  0.951695    0.021010  \n",
       "21   1.234782  0.972705  0.951695    0.021010  \n",
       "22   1.234782  0.972705  0.951695    0.021010  \n",
       "23   1.234782  0.972705  0.951695    0.021010  \n",
       "24   1.234782  0.972705  0.951695    0.021010  \n",
       "25   1.234782  0.972705  0.951695    0.021010  \n",
       "26   1.245065  0.972394  0.950887    0.021507  \n",
       "27   1.307144  0.968112  0.945867    0.022244  \n",
       "28   1.307144  0.968112  0.945867    0.022244  \n",
       "29   1.307144  0.968112  0.945867    0.022244  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ubah jadi DataFrame\n",
    "df_dt_absolute_error = pd.DataFrame(results)\n",
    "\n",
    "# Urutkan berdasarkan R2_test (model terbaik di atas)\n",
    "df_dt_absolute_error = df_dt_absolute_error.sort_values(by=\"R2_test\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_dt_absolute_error.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "abfa16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index ke 7 yang paling bagus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe99c",
   "metadata": {},
   "source": [
    "# Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fbd246d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "BaggingRegressor(\n",
      "    estimator=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    n_estimators=\u001b[32m10\u001b[39m,\n",
      "    *,\n",
      "    max_samples=\u001b[32m1.0\u001b[39m,\n",
      "    max_features=\u001b[32m1.0\u001b[39m,\n",
      "    bootstrap=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    bootstrap_features=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    oob_score=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    verbose=\u001b[32m0\u001b[39m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "A Bagging regressor.\n",
      "\n",
      "A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "regressors each on random subsets of the original dataset and then\n",
      "aggregate their individual predictions (either by voting or by averaging)\n",
      "to form a final prediction. Such a meta-estimator can typically be used as\n",
      "a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "tree), by introducing randomization into its construction procedure and\n",
      "then making an ensemble out of it.\n",
      "\n",
      "This algorithm encompasses several works from the literature. When random\n",
      "subsets of the dataset are drawn as random subsets of the samples, then\n",
      "this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "of the dataset are drawn as random subsets of the features, then the method\n",
      "is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "on subsets of both samples and features, then the method is known as\n",
      "Random Patches [4]_.\n",
      "\n",
      "Read more in the :ref:`User Guide <bagging>`.\n",
      "\n",
      ".. versionadded:: 0.15\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "estimator : object, default=None\n",
      "    The base estimator to fit on random subsets of the dataset.\n",
      "    If None, then the base estimator is a\n",
      "    :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
      "\n",
      "    .. versionadded:: 1.2\n",
      "       `base_estimator` was renamed to `estimator`.\n",
      "\n",
      "n_estimators : int, default=10\n",
      "    The number of base estimators in the ensemble.\n",
      "\n",
      "max_samples : int or float, default=1.0\n",
      "    The number of samples to draw from X to train each base estimator (with\n",
      "    replacement by default, see `bootstrap` for more details).\n",
      "\n",
      "    - If int, then draw `max_samples` samples.\n",
      "    - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "\n",
      "max_features : int or float, default=1.0\n",
      "    The number of features to draw from X to train each base estimator (\n",
      "    without replacement by default, see `bootstrap_features` for more\n",
      "    details).\n",
      "\n",
      "    - If int, then draw `max_features` features.\n",
      "    - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n",
      "\n",
      "bootstrap : bool, default=True\n",
      "    Whether samples are drawn with replacement. If False, sampling\n",
      "    without replacement is performed.\n",
      "\n",
      "bootstrap_features : bool, default=False\n",
      "    Whether features are drawn with replacement.\n",
      "\n",
      "oob_score : bool, default=False\n",
      "    Whether to use out-of-bag samples to estimate\n",
      "    the generalization error. Only available if bootstrap=True.\n",
      "\n",
      "warm_start : bool, default=False\n",
      "    When set to True, reuse the solution of the previous call to fit\n",
      "    and add more estimators to the ensemble, otherwise, just fit\n",
      "    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    The number of jobs to run in parallel for both :meth:`fit` and\n",
      "    :meth:`predict`. ``None`` means 1 unless in a\n",
      "    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "    processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Controls the random resampling of the original dataset\n",
      "    (sample wise and feature wise).\n",
      "    If the base estimator accepts a `random_state` attribute, a different\n",
      "    seed is generated for each instance in the ensemble.\n",
      "    Pass an int for reproducible output across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "verbose : int, default=0\n",
      "    Controls the verbosity when fitting and predicting.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "estimator_ : estimator\n",
      "    The base estimator from which the ensemble is grown.\n",
      "\n",
      "    .. versionadded:: 1.2\n",
      "       `base_estimator_` was renamed to `estimator_`.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "estimators_ : list of estimators\n",
      "    The collection of fitted sub-estimators.\n",
      "\n",
      "estimators_samples_ : list of arrays\n",
      "    The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "    estimator. Each subset is defined by an array of the indices selected.\n",
      "\n",
      "estimators_features_ : list of arrays\n",
      "    The subset of drawn features for each base estimator.\n",
      "\n",
      "oob_score_ : float\n",
      "    Score of the training dataset obtained using an out-of-bag estimate.\n",
      "    This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "oob_prediction_ : ndarray of shape (n_samples,)\n",
      "    Prediction computed with out-of-bag estimate on the training\n",
      "    set. If n_estimators is small it might be possible that a data point\n",
      "    was never left out during the bootstrap. In this case,\n",
      "    `oob_prediction_` might contain NaN. This attribute exists only\n",
      "    when ``oob_score`` is True.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "BaggingClassifier : A Bagging classifier.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      ".. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "\n",
      ".. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "       1996.\n",
      "\n",
      ".. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "       1998.\n",
      "\n",
      ".. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "       Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.svm import SVR\n",
      ">>> from sklearn.ensemble import BaggingRegressor\n",
      ">>> from sklearn.datasets import make_regression\n",
      ">>> X, y = make_regression(n_samples=100, n_features=4,\n",
      "...                        n_informative=2, n_targets=1,\n",
      "...                        random_state=0, shuffle=False)\n",
      ">>> regr = BaggingRegressor(estimator=SVR(),\n",
      "...                         n_estimators=10, random_state=0).fit(X, y)\n",
      ">>> regr.predict([[0, 0, 0, 0]])\n",
      "array([-2.8720])\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\asus\\anaconda3\\envs\\clustering_env_clean\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "?BaggingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "726227ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bagging Tree Regressor Train ===\n",
      "MAE: 0.2940000000000002\n",
      "MAPE: 3.2236727879803526\n",
      "MSE: 0.2288662500000001\n",
      "RMSE: 0.47839967600323485\n",
      "R2: 0.9911758157019616\n",
      "\n",
      "=== Bagging Tree Regressor Test ===\n",
      "MAE: 0.6427500000000002\n",
      "MAPE: 5.538919906704135\n",
      "MSE: 0.8269125000000006\n",
      "RMSE: 0.9093472933923543\n",
      "R2: 0.9738016999410911\n"
     ]
    }
   ],
   "source": [
    "# Buat model Bagging Regressor dengan base Decision Tree\n",
    "bagging_tree = BaggingRegressor()\n",
    "bagging_tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred_bagging = bagging_tree.predict(X_train_scaled)\n",
    "y_test_pred_bagging = bagging_tree.predict(X_test_scaled)\n",
    "\n",
    "# Evaluasi Train\n",
    "mse_train = mean_squared_error(y_train, y_train_pred_bagging)\n",
    "rmse_train = sqrt(mse_train)\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred_bagging)\n",
    "r2_train = r2_score(y_train, y_train_pred_bagging)\n",
    "mape_train = np.mean(np.abs((y_train - y_train_pred_bagging) / y_train)) * 100\n",
    "\n",
    "# Evaluasi Test\n",
    "mse_test = mean_squared_error(y_test, y_test_pred_bagging)\n",
    "rmse_test = sqrt(mse_test)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred_bagging)\n",
    "r2_test = r2_score(y_test, y_test_pred_bagging)\n",
    "mape_test = np.mean(np.abs((y_test - y_test_pred_bagging) / y_test)) * 100\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"=== Bagging Tree Regressor Train ===\")\n",
    "print(\"MAE:\", mae_train)\n",
    "print(\"MAPE:\", mape_train)\n",
    "print(\"MSE:\", mse_train)\n",
    "print(\"RMSE:\", rmse_train)\n",
    "print(\"R2:\", r2_train)\n",
    "\n",
    "print(\"\\n=== Bagging Tree Regressor Test ===\")\n",
    "print(\"MAE:\", mae_test)\n",
    "print(\"MAPE:\", mape_test)\n",
    "print(\"MSE:\", mse_test)\n",
    "print(\"RMSE:\", rmse_test)\n",
    "print(\"R2:\", r2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b7288",
   "metadata": {},
   "source": [
    "# tuning utk bagging regressor rule of thumb\n",
    "\n",
    "max_features = p (feature)\n",
    "\n",
    "min_samples_split: 2 sampai 0.01Ã—n\n",
    "\n",
    "min_samples_leaf: 1 sampai 0.01Ã—n\n",
    "\n",
    "max_depth: logâ‚‚(n) - 1  atau logâ‚‚(n) sampai logâ‚‚(n) + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be41cab",
   "metadata": {},
   "source": [
    "# Squared Erorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "425563f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Range hyperparameter base Decision Tree\n",
    "max_depth_range = range(6, 11)        # 6-10\n",
    "min_samples_split_range = range(2, 5) # 2-4\n",
    "min_samples_leaf_range = range(1, 5)  # 1-4\n",
    "max_features = 3                       # tetap 3 fitur\n",
    "\n",
    "# List untuk menyimpan hasil\n",
    "results = []\n",
    "\n",
    "# Loop semua kombinasi parameter\n",
    "for max_depth, min_samples_split, min_samples_leaf in product(\n",
    "    max_depth_range, min_samples_split_range, min_samples_leaf_range\n",
    "):\n",
    "    # Buat base tree\n",
    "    base_tree = DecisionTreeRegressor(\n",
    "        criterion='squared_error',  # Manhattan / MAE\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Buat Bagging Regressor\n",
    "    bagging_tree = BaggingRegressor(\n",
    "        estimator=base_tree,\n",
    "        n_estimators=100,  # bisa dituning juga\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Latih model\n",
    "    bagging_tree.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prediksi\n",
    "    y_train_pred = bagging_tree.predict(X_train_scaled)\n",
    "    y_test_pred = bagging_tree.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluasi\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    \n",
    "    # Simpan hasil\n",
    "    results.append({\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'max_features': max_features,\n",
    "        'MAE_train': mae_train,\n",
    "        'MAE_test': mae_test,\n",
    "        'MAPE_train': mape_train,\n",
    "        'MAPE_test': mape_test,\n",
    "        'MSE_train': mse_train,\n",
    "        'MSE_test': mse_test,\n",
    "        'RMSE_train': rmse_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'R2_train': r2_train,\n",
    "        'R2_test': r2_test,\n",
    "        'R2_selisih': r2_train - r2_test\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ec0c377f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.239283</td>\n",
       "      <td>0.618675</td>\n",
       "      <td>2.343333</td>\n",
       "      <td>5.391068</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.571380</td>\n",
       "      <td>0.317174</td>\n",
       "      <td>0.755897</td>\n",
       "      <td>0.996121</td>\n",
       "      <td>0.981897</td>\n",
       "      <td>0.014224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.226486</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>2.256303</td>\n",
       "      <td>5.384660</td>\n",
       "      <td>0.093208</td>\n",
       "      <td>0.575157</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.758391</td>\n",
       "      <td>0.996406</td>\n",
       "      <td>0.981778</td>\n",
       "      <td>0.014628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.229032</td>\n",
       "      <td>0.620469</td>\n",
       "      <td>2.266860</td>\n",
       "      <td>5.404001</td>\n",
       "      <td>0.095144</td>\n",
       "      <td>0.579403</td>\n",
       "      <td>0.308455</td>\n",
       "      <td>0.761185</td>\n",
       "      <td>0.996332</td>\n",
       "      <td>0.981643</td>\n",
       "      <td>0.014688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.248694</td>\n",
       "      <td>0.623957</td>\n",
       "      <td>2.413362</td>\n",
       "      <td>5.448210</td>\n",
       "      <td>0.106844</td>\n",
       "      <td>0.584935</td>\n",
       "      <td>0.326870</td>\n",
       "      <td>0.764810</td>\n",
       "      <td>0.995881</td>\n",
       "      <td>0.981468</td>\n",
       "      <td>0.014412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.251055</td>\n",
       "      <td>0.627189</td>\n",
       "      <td>2.585139</td>\n",
       "      <td>5.518237</td>\n",
       "      <td>0.117336</td>\n",
       "      <td>0.592387</td>\n",
       "      <td>0.342544</td>\n",
       "      <td>0.769667</td>\n",
       "      <td>0.995476</td>\n",
       "      <td>0.981232</td>\n",
       "      <td>0.014244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.248690</td>\n",
       "      <td>0.629487</td>\n",
       "      <td>2.578827</td>\n",
       "      <td>5.526584</td>\n",
       "      <td>0.116724</td>\n",
       "      <td>0.600151</td>\n",
       "      <td>0.341649</td>\n",
       "      <td>0.774694</td>\n",
       "      <td>0.995500</td>\n",
       "      <td>0.980986</td>\n",
       "      <td>0.014514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.265374</td>\n",
       "      <td>0.632883</td>\n",
       "      <td>2.701657</td>\n",
       "      <td>5.553376</td>\n",
       "      <td>0.127816</td>\n",
       "      <td>0.602012</td>\n",
       "      <td>0.357513</td>\n",
       "      <td>0.775894</td>\n",
       "      <td>0.995072</td>\n",
       "      <td>0.980927</td>\n",
       "      <td>0.014145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.251687</td>\n",
       "      <td>0.635153</td>\n",
       "      <td>2.603083</td>\n",
       "      <td>5.555458</td>\n",
       "      <td>0.117920</td>\n",
       "      <td>0.611392</td>\n",
       "      <td>0.343394</td>\n",
       "      <td>0.781915</td>\n",
       "      <td>0.995453</td>\n",
       "      <td>0.980630</td>\n",
       "      <td>0.014824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.287826</td>\n",
       "      <td>0.632948</td>\n",
       "      <td>2.678006</td>\n",
       "      <td>5.526792</td>\n",
       "      <td>0.138904</td>\n",
       "      <td>0.624600</td>\n",
       "      <td>0.372698</td>\n",
       "      <td>0.790316</td>\n",
       "      <td>0.994644</td>\n",
       "      <td>0.980211</td>\n",
       "      <td>0.014433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.275292</td>\n",
       "      <td>0.639311</td>\n",
       "      <td>2.834984</td>\n",
       "      <td>5.572843</td>\n",
       "      <td>0.141666</td>\n",
       "      <td>0.625981</td>\n",
       "      <td>0.376386</td>\n",
       "      <td>0.791189</td>\n",
       "      <td>0.994538</td>\n",
       "      <td>0.980168</td>\n",
       "      <td>0.014370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.273428</td>\n",
       "      <td>0.639294</td>\n",
       "      <td>2.818737</td>\n",
       "      <td>5.561080</td>\n",
       "      <td>0.140348</td>\n",
       "      <td>0.632266</td>\n",
       "      <td>0.374630</td>\n",
       "      <td>0.795152</td>\n",
       "      <td>0.994589</td>\n",
       "      <td>0.979969</td>\n",
       "      <td>0.014620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.297213</td>\n",
       "      <td>0.636561</td>\n",
       "      <td>2.926145</td>\n",
       "      <td>5.603694</td>\n",
       "      <td>0.153993</td>\n",
       "      <td>0.632898</td>\n",
       "      <td>0.392419</td>\n",
       "      <td>0.795549</td>\n",
       "      <td>0.994063</td>\n",
       "      <td>0.979948</td>\n",
       "      <td>0.014114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.273391</td>\n",
       "      <td>0.643773</td>\n",
       "      <td>2.820135</td>\n",
       "      <td>5.583790</td>\n",
       "      <td>0.140522</td>\n",
       "      <td>0.641842</td>\n",
       "      <td>0.374863</td>\n",
       "      <td>0.801150</td>\n",
       "      <td>0.994582</td>\n",
       "      <td>0.979665</td>\n",
       "      <td>0.014917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.285296</td>\n",
       "      <td>0.644859</td>\n",
       "      <td>2.910094</td>\n",
       "      <td>5.596904</td>\n",
       "      <td>0.150137</td>\n",
       "      <td>0.647015</td>\n",
       "      <td>0.387475</td>\n",
       "      <td>0.804373</td>\n",
       "      <td>0.994211</td>\n",
       "      <td>0.979501</td>\n",
       "      <td>0.014710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.311544</td>\n",
       "      <td>0.648519</td>\n",
       "      <td>3.092138</td>\n",
       "      <td>5.673508</td>\n",
       "      <td>0.172791</td>\n",
       "      <td>0.658203</td>\n",
       "      <td>0.415681</td>\n",
       "      <td>0.811297</td>\n",
       "      <td>0.993338</td>\n",
       "      <td>0.979147</td>\n",
       "      <td>0.014191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301102</td>\n",
       "      <td>0.659779</td>\n",
       "      <td>3.297685</td>\n",
       "      <td>5.843081</td>\n",
       "      <td>0.182491</td>\n",
       "      <td>0.661521</td>\n",
       "      <td>0.427190</td>\n",
       "      <td>0.813339</td>\n",
       "      <td>0.992964</td>\n",
       "      <td>0.979042</td>\n",
       "      <td>0.013922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301102</td>\n",
       "      <td>0.659779</td>\n",
       "      <td>3.297685</td>\n",
       "      <td>5.843081</td>\n",
       "      <td>0.182491</td>\n",
       "      <td>0.661521</td>\n",
       "      <td>0.427190</td>\n",
       "      <td>0.813339</td>\n",
       "      <td>0.992964</td>\n",
       "      <td>0.979042</td>\n",
       "      <td>0.013922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301102</td>\n",
       "      <td>0.659779</td>\n",
       "      <td>3.297685</td>\n",
       "      <td>5.843081</td>\n",
       "      <td>0.182491</td>\n",
       "      <td>0.661521</td>\n",
       "      <td>0.427190</td>\n",
       "      <td>0.813339</td>\n",
       "      <td>0.992964</td>\n",
       "      <td>0.979042</td>\n",
       "      <td>0.013922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301420</td>\n",
       "      <td>0.660402</td>\n",
       "      <td>3.299258</td>\n",
       "      <td>5.845949</td>\n",
       "      <td>0.182550</td>\n",
       "      <td>0.662412</td>\n",
       "      <td>0.427258</td>\n",
       "      <td>0.813887</td>\n",
       "      <td>0.992962</td>\n",
       "      <td>0.979013</td>\n",
       "      <td>0.013948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301420</td>\n",
       "      <td>0.660402</td>\n",
       "      <td>3.299258</td>\n",
       "      <td>5.845949</td>\n",
       "      <td>0.182550</td>\n",
       "      <td>0.662412</td>\n",
       "      <td>0.427258</td>\n",
       "      <td>0.813887</td>\n",
       "      <td>0.992962</td>\n",
       "      <td>0.979013</td>\n",
       "      <td>0.013948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301420</td>\n",
       "      <td>0.660402</td>\n",
       "      <td>3.299258</td>\n",
       "      <td>5.845949</td>\n",
       "      <td>0.182550</td>\n",
       "      <td>0.662412</td>\n",
       "      <td>0.427258</td>\n",
       "      <td>0.813887</td>\n",
       "      <td>0.992962</td>\n",
       "      <td>0.979013</td>\n",
       "      <td>0.013948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301017</td>\n",
       "      <td>0.664010</td>\n",
       "      <td>3.301861</td>\n",
       "      <td>5.872871</td>\n",
       "      <td>0.182491</td>\n",
       "      <td>0.666643</td>\n",
       "      <td>0.427190</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.992964</td>\n",
       "      <td>0.978879</td>\n",
       "      <td>0.014084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301017</td>\n",
       "      <td>0.664010</td>\n",
       "      <td>3.301861</td>\n",
       "      <td>5.872871</td>\n",
       "      <td>0.182491</td>\n",
       "      <td>0.666643</td>\n",
       "      <td>0.427190</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.992964</td>\n",
       "      <td>0.978879</td>\n",
       "      <td>0.014084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301017</td>\n",
       "      <td>0.664010</td>\n",
       "      <td>3.301861</td>\n",
       "      <td>5.872871</td>\n",
       "      <td>0.182491</td>\n",
       "      <td>0.666643</td>\n",
       "      <td>0.427190</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.992964</td>\n",
       "      <td>0.978879</td>\n",
       "      <td>0.014084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.305125</td>\n",
       "      <td>0.670220</td>\n",
       "      <td>3.328405</td>\n",
       "      <td>5.933571</td>\n",
       "      <td>0.184579</td>\n",
       "      <td>0.686838</td>\n",
       "      <td>0.429627</td>\n",
       "      <td>0.828757</td>\n",
       "      <td>0.992883</td>\n",
       "      <td>0.978240</td>\n",
       "      <td>0.014644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.305125</td>\n",
       "      <td>0.670220</td>\n",
       "      <td>3.328405</td>\n",
       "      <td>5.933571</td>\n",
       "      <td>0.184579</td>\n",
       "      <td>0.686838</td>\n",
       "      <td>0.429627</td>\n",
       "      <td>0.828757</td>\n",
       "      <td>0.992883</td>\n",
       "      <td>0.978240</td>\n",
       "      <td>0.014644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.305125</td>\n",
       "      <td>0.670220</td>\n",
       "      <td>3.328405</td>\n",
       "      <td>5.933571</td>\n",
       "      <td>0.184579</td>\n",
       "      <td>0.686838</td>\n",
       "      <td>0.429627</td>\n",
       "      <td>0.828757</td>\n",
       "      <td>0.992883</td>\n",
       "      <td>0.978240</td>\n",
       "      <td>0.014644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.324854</td>\n",
       "      <td>0.670941</td>\n",
       "      <td>3.474717</td>\n",
       "      <td>5.947719</td>\n",
       "      <td>0.202835</td>\n",
       "      <td>0.694688</td>\n",
       "      <td>0.450372</td>\n",
       "      <td>0.833480</td>\n",
       "      <td>0.992179</td>\n",
       "      <td>0.977991</td>\n",
       "      <td>0.014189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.324854</td>\n",
       "      <td>0.670941</td>\n",
       "      <td>3.474717</td>\n",
       "      <td>5.947719</td>\n",
       "      <td>0.202835</td>\n",
       "      <td>0.694688</td>\n",
       "      <td>0.450372</td>\n",
       "      <td>0.833480</td>\n",
       "      <td>0.992179</td>\n",
       "      <td>0.977991</td>\n",
       "      <td>0.014189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.324854</td>\n",
       "      <td>0.670941</td>\n",
       "      <td>3.474717</td>\n",
       "      <td>5.947719</td>\n",
       "      <td>0.202835</td>\n",
       "      <td>0.694688</td>\n",
       "      <td>0.450372</td>\n",
       "      <td>0.833480</td>\n",
       "      <td>0.992179</td>\n",
       "      <td>0.977991</td>\n",
       "      <td>0.014189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_samples_split  min_samples_leaf  max_features  MAE_train  \\\n",
       "0           8                  2                 1             3   0.239283   \n",
       "1          10                  2                 1             3   0.226486   \n",
       "2           9                  2                 1             3   0.229032   \n",
       "3           7                  2                 1             3   0.248694   \n",
       "4           9                  3                 1             3   0.251055   \n",
       "5          10                  3                 1             3   0.248690   \n",
       "6           7                  3                 1             3   0.265374   \n",
       "7           8                  3                 1             3   0.251687   \n",
       "8           6                  2                 1             3   0.287826   \n",
       "9           8                  4                 1             3   0.275292   \n",
       "10         10                  4                 1             3   0.273428   \n",
       "11          6                  3                 1             3   0.297213   \n",
       "12          9                  4                 1             3   0.273391   \n",
       "13          7                  4                 1             3   0.285296   \n",
       "14          6                  4                 1             3   0.311544   \n",
       "15          9                  2                 2             3   0.301102   \n",
       "16          9                  3                 2             3   0.301102   \n",
       "17          9                  4                 2             3   0.301102   \n",
       "18         10                  4                 2             3   0.301420   \n",
       "19         10                  2                 2             3   0.301420   \n",
       "20         10                  3                 2             3   0.301420   \n",
       "21          8                  4                 2             3   0.301017   \n",
       "22          8                  3                 2             3   0.301017   \n",
       "23          8                  2                 2             3   0.301017   \n",
       "24          7                  2                 2             3   0.305125   \n",
       "25          7                  3                 2             3   0.305125   \n",
       "26          7                  4                 2             3   0.305125   \n",
       "27          6                  2                 2             3   0.324854   \n",
       "28          6                  4                 2             3   0.324854   \n",
       "29          6                  3                 2             3   0.324854   \n",
       "\n",
       "    MAE_test  MAPE_train  MAPE_test  MSE_train  MSE_test  RMSE_train  \\\n",
       "0   0.618675    2.343333   5.391068   0.100600  0.571380    0.317174   \n",
       "1   0.618884    2.256303   5.384660   0.093208  0.575157    0.305300   \n",
       "2   0.620469    2.266860   5.404001   0.095144  0.579403    0.308455   \n",
       "3   0.623957    2.413362   5.448210   0.106844  0.584935    0.326870   \n",
       "4   0.627189    2.585139   5.518237   0.117336  0.592387    0.342544   \n",
       "5   0.629487    2.578827   5.526584   0.116724  0.600151    0.341649   \n",
       "6   0.632883    2.701657   5.553376   0.127816  0.602012    0.357513   \n",
       "7   0.635153    2.603083   5.555458   0.117920  0.611392    0.343394   \n",
       "8   0.632948    2.678006   5.526792   0.138904  0.624600    0.372698   \n",
       "9   0.639311    2.834984   5.572843   0.141666  0.625981    0.376386   \n",
       "10  0.639294    2.818737   5.561080   0.140348  0.632266    0.374630   \n",
       "11  0.636561    2.926145   5.603694   0.153993  0.632898    0.392419   \n",
       "12  0.643773    2.820135   5.583790   0.140522  0.641842    0.374863   \n",
       "13  0.644859    2.910094   5.596904   0.150137  0.647015    0.387475   \n",
       "14  0.648519    3.092138   5.673508   0.172791  0.658203    0.415681   \n",
       "15  0.659779    3.297685   5.843081   0.182491  0.661521    0.427190   \n",
       "16  0.659779    3.297685   5.843081   0.182491  0.661521    0.427190   \n",
       "17  0.659779    3.297685   5.843081   0.182491  0.661521    0.427190   \n",
       "18  0.660402    3.299258   5.845949   0.182550  0.662412    0.427258   \n",
       "19  0.660402    3.299258   5.845949   0.182550  0.662412    0.427258   \n",
       "20  0.660402    3.299258   5.845949   0.182550  0.662412    0.427258   \n",
       "21  0.664010    3.301861   5.872871   0.182491  0.666643    0.427190   \n",
       "22  0.664010    3.301861   5.872871   0.182491  0.666643    0.427190   \n",
       "23  0.664010    3.301861   5.872871   0.182491  0.666643    0.427190   \n",
       "24  0.670220    3.328405   5.933571   0.184579  0.686838    0.429627   \n",
       "25  0.670220    3.328405   5.933571   0.184579  0.686838    0.429627   \n",
       "26  0.670220    3.328405   5.933571   0.184579  0.686838    0.429627   \n",
       "27  0.670941    3.474717   5.947719   0.202835  0.694688    0.450372   \n",
       "28  0.670941    3.474717   5.947719   0.202835  0.694688    0.450372   \n",
       "29  0.670941    3.474717   5.947719   0.202835  0.694688    0.450372   \n",
       "\n",
       "    RMSE_test  R2_train   R2_test  R2_selisih  \n",
       "0    0.755897  0.996121  0.981897    0.014224  \n",
       "1    0.758391  0.996406  0.981778    0.014628  \n",
       "2    0.761185  0.996332  0.981643    0.014688  \n",
       "3    0.764810  0.995881  0.981468    0.014412  \n",
       "4    0.769667  0.995476  0.981232    0.014244  \n",
       "5    0.774694  0.995500  0.980986    0.014514  \n",
       "6    0.775894  0.995072  0.980927    0.014145  \n",
       "7    0.781915  0.995453  0.980630    0.014824  \n",
       "8    0.790316  0.994644  0.980211    0.014433  \n",
       "9    0.791189  0.994538  0.980168    0.014370  \n",
       "10   0.795152  0.994589  0.979969    0.014620  \n",
       "11   0.795549  0.994063  0.979948    0.014114  \n",
       "12   0.801150  0.994582  0.979665    0.014917  \n",
       "13   0.804373  0.994211  0.979501    0.014710  \n",
       "14   0.811297  0.993338  0.979147    0.014191  \n",
       "15   0.813339  0.992964  0.979042    0.013922  \n",
       "16   0.813339  0.992964  0.979042    0.013922  \n",
       "17   0.813339  0.992964  0.979042    0.013922  \n",
       "18   0.813887  0.992962  0.979013    0.013948  \n",
       "19   0.813887  0.992962  0.979013    0.013948  \n",
       "20   0.813887  0.992962  0.979013    0.013948  \n",
       "21   0.816482  0.992964  0.978879    0.014084  \n",
       "22   0.816482  0.992964  0.978879    0.014084  \n",
       "23   0.816482  0.992964  0.978879    0.014084  \n",
       "24   0.828757  0.992883  0.978240    0.014644  \n",
       "25   0.828757  0.992883  0.978240    0.014644  \n",
       "26   0.828757  0.992883  0.978240    0.014644  \n",
       "27   0.833480  0.992179  0.977991    0.014189  \n",
       "28   0.833480  0.992179  0.977991    0.014189  \n",
       "29   0.833480  0.992179  0.977991    0.014189  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ubah jadi DataFrame\n",
    "df_bagging_squared_error = pd.DataFrame(results)\n",
    "\n",
    "# Urutkan berdasarkan R2_test (model terbaik di atas)\n",
    "df_bagging_squared_error = df_bagging_squared_error.sort_values(by=\"R2_test\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_bagging_squared_error.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6f8f95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index ke 0 yang terbaik , meski MAPE kalah dikit utk yg paling rendah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20137989",
   "metadata": {},
   "source": [
    "# Criterion: Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "db97afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Range hyperparameter base Decision Tree\n",
    "max_depth_range = range(6, 11)        # 6-10\n",
    "min_samples_split_range = range(2, 5) # 2-4\n",
    "min_samples_leaf_range = range(1, 5)  # 1-4\n",
    "max_features = 3                       # tetap 3 fitur\n",
    "\n",
    "# List untuk menyimpan hasil\n",
    "results = []\n",
    "\n",
    "# Loop semua kombinasi parameter\n",
    "for max_depth, min_samples_split, min_samples_leaf in product(\n",
    "    max_depth_range, min_samples_split_range, min_samples_leaf_range\n",
    "):\n",
    "    # Buat base tree\n",
    "    base_tree = DecisionTreeRegressor(\n",
    "        criterion='absolute_error',  \n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Buat Bagging Regressor\n",
    "    bagging_tree = BaggingRegressor(\n",
    "        estimator=base_tree,\n",
    "        n_estimators=100,  # bisa dituning juga\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Latih model\n",
    "    bagging_tree.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prediksi\n",
    "    y_train_pred = bagging_tree.predict(X_train_scaled)\n",
    "    y_test_pred = bagging_tree.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluasi\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    \n",
    "    # Simpan hasil\n",
    "    results.append({\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'max_features': max_features,\n",
    "        'MAE_train': mae_train,\n",
    "        'MAE_test': mae_test,\n",
    "        'MAPE_train': mape_train,\n",
    "        'MAPE_test': mape_test,\n",
    "        'MSE_train': mse_train,\n",
    "        'MSE_test': mse_test,\n",
    "        'RMSE_train': rmse_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'R2_train': r2_train,\n",
    "        'R2_test': r2_test,\n",
    "        'R2_selisih': r2_train - r2_test\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "788cea16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.243953</td>\n",
       "      <td>0.591662</td>\n",
       "      <td>2.510791</td>\n",
       "      <td>5.146520</td>\n",
       "      <td>0.115936</td>\n",
       "      <td>0.497787</td>\n",
       "      <td>0.340494</td>\n",
       "      <td>0.705540</td>\n",
       "      <td>0.995530</td>\n",
       "      <td>0.984229</td>\n",
       "      <td>0.011301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.240916</td>\n",
       "      <td>0.599525</td>\n",
       "      <td>2.335725</td>\n",
       "      <td>5.292448</td>\n",
       "      <td>0.106946</td>\n",
       "      <td>0.507362</td>\n",
       "      <td>0.327026</td>\n",
       "      <td>0.712293</td>\n",
       "      <td>0.995877</td>\n",
       "      <td>0.983926</td>\n",
       "      <td>0.011951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.220041</td>\n",
       "      <td>0.597900</td>\n",
       "      <td>2.201334</td>\n",
       "      <td>5.222607</td>\n",
       "      <td>0.093221</td>\n",
       "      <td>0.510679</td>\n",
       "      <td>0.305321</td>\n",
       "      <td>0.714618</td>\n",
       "      <td>0.996406</td>\n",
       "      <td>0.983821</td>\n",
       "      <td>0.012585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.222369</td>\n",
       "      <td>0.598462</td>\n",
       "      <td>2.205167</td>\n",
       "      <td>5.210169</td>\n",
       "      <td>0.095612</td>\n",
       "      <td>0.513697</td>\n",
       "      <td>0.309212</td>\n",
       "      <td>0.716726</td>\n",
       "      <td>0.996314</td>\n",
       "      <td>0.983725</td>\n",
       "      <td>0.012589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.241603</td>\n",
       "      <td>0.594500</td>\n",
       "      <td>2.487665</td>\n",
       "      <td>5.153630</td>\n",
       "      <td>0.117095</td>\n",
       "      <td>0.523441</td>\n",
       "      <td>0.342191</td>\n",
       "      <td>0.723493</td>\n",
       "      <td>0.995485</td>\n",
       "      <td>0.983416</td>\n",
       "      <td>0.012069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.257034</td>\n",
       "      <td>0.602200</td>\n",
       "      <td>2.612148</td>\n",
       "      <td>5.230398</td>\n",
       "      <td>0.127529</td>\n",
       "      <td>0.526752</td>\n",
       "      <td>0.357112</td>\n",
       "      <td>0.725777</td>\n",
       "      <td>0.995083</td>\n",
       "      <td>0.983311</td>\n",
       "      <td>0.011772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.258959</td>\n",
       "      <td>0.616375</td>\n",
       "      <td>2.489140</td>\n",
       "      <td>5.398383</td>\n",
       "      <td>0.123208</td>\n",
       "      <td>0.529543</td>\n",
       "      <td>0.351010</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.995250</td>\n",
       "      <td>0.983223</td>\n",
       "      <td>0.012027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.275059</td>\n",
       "      <td>0.608013</td>\n",
       "      <td>2.807310</td>\n",
       "      <td>5.328637</td>\n",
       "      <td>0.147882</td>\n",
       "      <td>0.538073</td>\n",
       "      <td>0.384554</td>\n",
       "      <td>0.733535</td>\n",
       "      <td>0.994298</td>\n",
       "      <td>0.982953</td>\n",
       "      <td>0.011346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.272984</td>\n",
       "      <td>0.612450</td>\n",
       "      <td>2.796692</td>\n",
       "      <td>5.333854</td>\n",
       "      <td>0.146193</td>\n",
       "      <td>0.545249</td>\n",
       "      <td>0.382352</td>\n",
       "      <td>0.738410</td>\n",
       "      <td>0.994363</td>\n",
       "      <td>0.982725</td>\n",
       "      <td>0.011638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.273738</td>\n",
       "      <td>0.629275</td>\n",
       "      <td>2.723681</td>\n",
       "      <td>5.430119</td>\n",
       "      <td>0.143366</td>\n",
       "      <td>0.561384</td>\n",
       "      <td>0.378637</td>\n",
       "      <td>0.749256</td>\n",
       "      <td>0.994472</td>\n",
       "      <td>0.982214</td>\n",
       "      <td>0.012258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.629150</td>\n",
       "      <td>2.832526</td>\n",
       "      <td>5.432592</td>\n",
       "      <td>0.153729</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.392083</td>\n",
       "      <td>0.756241</td>\n",
       "      <td>0.994073</td>\n",
       "      <td>0.981881</td>\n",
       "      <td>0.012192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.292103</td>\n",
       "      <td>0.638575</td>\n",
       "      <td>2.928229</td>\n",
       "      <td>5.481224</td>\n",
       "      <td>0.165117</td>\n",
       "      <td>0.592024</td>\n",
       "      <td>0.406346</td>\n",
       "      <td>0.769431</td>\n",
       "      <td>0.993634</td>\n",
       "      <td>0.981243</td>\n",
       "      <td>0.012390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.309178</td>\n",
       "      <td>0.657075</td>\n",
       "      <td>2.776222</td>\n",
       "      <td>5.638187</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.612179</td>\n",
       "      <td>0.410265</td>\n",
       "      <td>0.782419</td>\n",
       "      <td>0.993510</td>\n",
       "      <td>0.980605</td>\n",
       "      <td>0.012905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.310887</td>\n",
       "      <td>0.651925</td>\n",
       "      <td>2.964032</td>\n",
       "      <td>5.577796</td>\n",
       "      <td>0.177220</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>0.420975</td>\n",
       "      <td>0.783657</td>\n",
       "      <td>0.993167</td>\n",
       "      <td>0.980543</td>\n",
       "      <td>0.012624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.345884</td>\n",
       "      <td>0.650462</td>\n",
       "      <td>3.810084</td>\n",
       "      <td>5.724558</td>\n",
       "      <td>0.238498</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.488362</td>\n",
       "      <td>0.802295</td>\n",
       "      <td>0.990804</td>\n",
       "      <td>0.979607</td>\n",
       "      <td>0.011198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.345884</td>\n",
       "      <td>0.650462</td>\n",
       "      <td>3.810084</td>\n",
       "      <td>5.724558</td>\n",
       "      <td>0.238498</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.488362</td>\n",
       "      <td>0.802295</td>\n",
       "      <td>0.990804</td>\n",
       "      <td>0.979607</td>\n",
       "      <td>0.011198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.345884</td>\n",
       "      <td>0.650462</td>\n",
       "      <td>3.810084</td>\n",
       "      <td>5.724558</td>\n",
       "      <td>0.238498</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.488362</td>\n",
       "      <td>0.802295</td>\n",
       "      <td>0.990804</td>\n",
       "      <td>0.979607</td>\n",
       "      <td>0.011198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.327178</td>\n",
       "      <td>0.662787</td>\n",
       "      <td>3.152768</td>\n",
       "      <td>5.573302</td>\n",
       "      <td>0.202494</td>\n",
       "      <td>0.651796</td>\n",
       "      <td>0.449993</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>0.992193</td>\n",
       "      <td>0.979350</td>\n",
       "      <td>0.012843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.345328</td>\n",
       "      <td>0.651675</td>\n",
       "      <td>3.805396</td>\n",
       "      <td>5.727102</td>\n",
       "      <td>0.237775</td>\n",
       "      <td>0.655308</td>\n",
       "      <td>0.487622</td>\n",
       "      <td>0.809511</td>\n",
       "      <td>0.990832</td>\n",
       "      <td>0.979238</td>\n",
       "      <td>0.011594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.345328</td>\n",
       "      <td>0.651675</td>\n",
       "      <td>3.805396</td>\n",
       "      <td>5.727102</td>\n",
       "      <td>0.237775</td>\n",
       "      <td>0.655308</td>\n",
       "      <td>0.487622</td>\n",
       "      <td>0.809511</td>\n",
       "      <td>0.990832</td>\n",
       "      <td>0.979238</td>\n",
       "      <td>0.011594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.345328</td>\n",
       "      <td>0.651675</td>\n",
       "      <td>3.805396</td>\n",
       "      <td>5.727102</td>\n",
       "      <td>0.237775</td>\n",
       "      <td>0.655308</td>\n",
       "      <td>0.487622</td>\n",
       "      <td>0.809511</td>\n",
       "      <td>0.990832</td>\n",
       "      <td>0.979238</td>\n",
       "      <td>0.011594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.354778</td>\n",
       "      <td>0.659975</td>\n",
       "      <td>3.864207</td>\n",
       "      <td>5.779245</td>\n",
       "      <td>0.250394</td>\n",
       "      <td>0.661549</td>\n",
       "      <td>0.500394</td>\n",
       "      <td>0.813357</td>\n",
       "      <td>0.990346</td>\n",
       "      <td>0.979041</td>\n",
       "      <td>0.011305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.354778</td>\n",
       "      <td>0.659975</td>\n",
       "      <td>3.864207</td>\n",
       "      <td>5.779245</td>\n",
       "      <td>0.250394</td>\n",
       "      <td>0.661549</td>\n",
       "      <td>0.500394</td>\n",
       "      <td>0.813357</td>\n",
       "      <td>0.990346</td>\n",
       "      <td>0.979041</td>\n",
       "      <td>0.011305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.354778</td>\n",
       "      <td>0.659975</td>\n",
       "      <td>3.864207</td>\n",
       "      <td>5.779245</td>\n",
       "      <td>0.250394</td>\n",
       "      <td>0.661549</td>\n",
       "      <td>0.500394</td>\n",
       "      <td>0.813357</td>\n",
       "      <td>0.990346</td>\n",
       "      <td>0.979041</td>\n",
       "      <td>0.011305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.352250</td>\n",
       "      <td>0.667250</td>\n",
       "      <td>3.847153</td>\n",
       "      <td>5.814022</td>\n",
       "      <td>0.245588</td>\n",
       "      <td>0.678421</td>\n",
       "      <td>0.495568</td>\n",
       "      <td>0.823663</td>\n",
       "      <td>0.990531</td>\n",
       "      <td>0.978506</td>\n",
       "      <td>0.012025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.352250</td>\n",
       "      <td>0.667250</td>\n",
       "      <td>3.847153</td>\n",
       "      <td>5.814022</td>\n",
       "      <td>0.245588</td>\n",
       "      <td>0.678421</td>\n",
       "      <td>0.495568</td>\n",
       "      <td>0.823663</td>\n",
       "      <td>0.990531</td>\n",
       "      <td>0.978506</td>\n",
       "      <td>0.012025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.352250</td>\n",
       "      <td>0.667250</td>\n",
       "      <td>3.847153</td>\n",
       "      <td>5.814022</td>\n",
       "      <td>0.245588</td>\n",
       "      <td>0.678421</td>\n",
       "      <td>0.495568</td>\n",
       "      <td>0.823663</td>\n",
       "      <td>0.990531</td>\n",
       "      <td>0.978506</td>\n",
       "      <td>0.012025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.366412</td>\n",
       "      <td>0.677937</td>\n",
       "      <td>3.955581</td>\n",
       "      <td>5.880770</td>\n",
       "      <td>0.266629</td>\n",
       "      <td>0.692430</td>\n",
       "      <td>0.516361</td>\n",
       "      <td>0.832124</td>\n",
       "      <td>0.989720</td>\n",
       "      <td>0.978062</td>\n",
       "      <td>0.011657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.366412</td>\n",
       "      <td>0.677937</td>\n",
       "      <td>3.955581</td>\n",
       "      <td>5.880770</td>\n",
       "      <td>0.266629</td>\n",
       "      <td>0.692430</td>\n",
       "      <td>0.516361</td>\n",
       "      <td>0.832124</td>\n",
       "      <td>0.989720</td>\n",
       "      <td>0.978062</td>\n",
       "      <td>0.011657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.366412</td>\n",
       "      <td>0.677937</td>\n",
       "      <td>3.955581</td>\n",
       "      <td>5.880770</td>\n",
       "      <td>0.266629</td>\n",
       "      <td>0.692430</td>\n",
       "      <td>0.516361</td>\n",
       "      <td>0.832124</td>\n",
       "      <td>0.989720</td>\n",
       "      <td>0.978062</td>\n",
       "      <td>0.011657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_samples_split  min_samples_leaf  max_features  MAE_train  \\\n",
       "0           9                  3                 1             3   0.243953   \n",
       "1           8                  2                 1             3   0.240916   \n",
       "2          10                  2                 1             3   0.220041   \n",
       "3           9                  2                 1             3   0.222369   \n",
       "4          10                  3                 1             3   0.241603   \n",
       "5           8                  3                 1             3   0.257034   \n",
       "6           7                  2                 1             3   0.258959   \n",
       "7           9                  4                 1             3   0.275059   \n",
       "8          10                  4                 1             3   0.272984   \n",
       "9           7                  3                 1             3   0.273738   \n",
       "10          8                  4                 1             3   0.279412   \n",
       "11          7                  4                 1             3   0.292103   \n",
       "12          6                  2                 1             3   0.309178   \n",
       "13          6                  3                 1             3   0.310887   \n",
       "14          9                  3                 2             3   0.345884   \n",
       "15          9                  2                 2             3   0.345884   \n",
       "16          9                  4                 2             3   0.345884   \n",
       "17          6                  4                 1             3   0.327178   \n",
       "18         10                  4                 2             3   0.345328   \n",
       "19         10                  2                 2             3   0.345328   \n",
       "20         10                  3                 2             3   0.345328   \n",
       "21          7                  2                 2             3   0.354778   \n",
       "22          7                  4                 2             3   0.354778   \n",
       "23          7                  3                 2             3   0.354778   \n",
       "24          8                  3                 2             3   0.352250   \n",
       "25          8                  4                 2             3   0.352250   \n",
       "26          8                  2                 2             3   0.352250   \n",
       "27          6                  2                 2             3   0.366412   \n",
       "28          6                  4                 2             3   0.366412   \n",
       "29          6                  3                 2             3   0.366412   \n",
       "\n",
       "    MAE_test  MAPE_train  MAPE_test  MSE_train  MSE_test  RMSE_train  \\\n",
       "0   0.591662    2.510791   5.146520   0.115936  0.497787    0.340494   \n",
       "1   0.599525    2.335725   5.292448   0.106946  0.507362    0.327026   \n",
       "2   0.597900    2.201334   5.222607   0.093221  0.510679    0.305321   \n",
       "3   0.598462    2.205167   5.210169   0.095612  0.513697    0.309212   \n",
       "4   0.594500    2.487665   5.153630   0.117095  0.523441    0.342191   \n",
       "5   0.602200    2.612148   5.230398   0.127529  0.526752    0.357112   \n",
       "6   0.616375    2.489140   5.398383   0.123208  0.529543    0.351010   \n",
       "7   0.608013    2.807310   5.328637   0.147882  0.538073    0.384554   \n",
       "8   0.612450    2.796692   5.333854   0.146193  0.545249    0.382352   \n",
       "9   0.629275    2.723681   5.430119   0.143366  0.561384    0.378637   \n",
       "10  0.629150    2.832526   5.432592   0.153729  0.571900    0.392083   \n",
       "11  0.638575    2.928229   5.481224   0.165117  0.592024    0.406346   \n",
       "12  0.657075    2.776222   5.638187   0.168317  0.612179    0.410265   \n",
       "13  0.651925    2.964032   5.577796   0.177220  0.614118    0.420975   \n",
       "14  0.650462    3.810084   5.724558   0.238498  0.643678    0.488362   \n",
       "15  0.650462    3.810084   5.724558   0.238498  0.643678    0.488362   \n",
       "16  0.650462    3.810084   5.724558   0.238498  0.643678    0.488362   \n",
       "17  0.662787    3.152768   5.573302   0.202494  0.651796    0.449993   \n",
       "18  0.651675    3.805396   5.727102   0.237775  0.655308    0.487622   \n",
       "19  0.651675    3.805396   5.727102   0.237775  0.655308    0.487622   \n",
       "20  0.651675    3.805396   5.727102   0.237775  0.655308    0.487622   \n",
       "21  0.659975    3.864207   5.779245   0.250394  0.661549    0.500394   \n",
       "22  0.659975    3.864207   5.779245   0.250394  0.661549    0.500394   \n",
       "23  0.659975    3.864207   5.779245   0.250394  0.661549    0.500394   \n",
       "24  0.667250    3.847153   5.814022   0.245588  0.678421    0.495568   \n",
       "25  0.667250    3.847153   5.814022   0.245588  0.678421    0.495568   \n",
       "26  0.667250    3.847153   5.814022   0.245588  0.678421    0.495568   \n",
       "27  0.677937    3.955581   5.880770   0.266629  0.692430    0.516361   \n",
       "28  0.677937    3.955581   5.880770   0.266629  0.692430    0.516361   \n",
       "29  0.677937    3.955581   5.880770   0.266629  0.692430    0.516361   \n",
       "\n",
       "    RMSE_test  R2_train   R2_test  R2_selisih  \n",
       "0    0.705540  0.995530  0.984229    0.011301  \n",
       "1    0.712293  0.995877  0.983926    0.011951  \n",
       "2    0.714618  0.996406  0.983821    0.012585  \n",
       "3    0.716726  0.996314  0.983725    0.012589  \n",
       "4    0.723493  0.995485  0.983416    0.012069  \n",
       "5    0.725777  0.995083  0.983311    0.011772  \n",
       "6    0.727697  0.995250  0.983223    0.012027  \n",
       "7    0.733535  0.994298  0.982953    0.011346  \n",
       "8    0.738410  0.994363  0.982725    0.011638  \n",
       "9    0.749256  0.994472  0.982214    0.012258  \n",
       "10   0.756241  0.994073  0.981881    0.012192  \n",
       "11   0.769431  0.993634  0.981243    0.012390  \n",
       "12   0.782419  0.993510  0.980605    0.012905  \n",
       "13   0.783657  0.993167  0.980543    0.012624  \n",
       "14   0.802295  0.990804  0.979607    0.011198  \n",
       "15   0.802295  0.990804  0.979607    0.011198  \n",
       "16   0.802295  0.990804  0.979607    0.011198  \n",
       "17   0.807339  0.992193  0.979350    0.012843  \n",
       "18   0.809511  0.990832  0.979238    0.011594  \n",
       "19   0.809511  0.990832  0.979238    0.011594  \n",
       "20   0.809511  0.990832  0.979238    0.011594  \n",
       "21   0.813357  0.990346  0.979041    0.011305  \n",
       "22   0.813357  0.990346  0.979041    0.011305  \n",
       "23   0.813357  0.990346  0.979041    0.011305  \n",
       "24   0.823663  0.990531  0.978506    0.012025  \n",
       "25   0.823663  0.990531  0.978506    0.012025  \n",
       "26   0.823663  0.990531  0.978506    0.012025  \n",
       "27   0.832124  0.989720  0.978062    0.011657  \n",
       "28   0.832124  0.989720  0.978062    0.011657  \n",
       "29   0.832124  0.989720  0.978062    0.011657  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ubah jadi DataFrame\n",
    "df_bagging_absolute_error = pd.DataFrame(results)\n",
    "\n",
    "# Urutkan berdasarkan R2_test (model terbaik di atas)\n",
    "df_bagging_absolute_error = df_bagging_absolute_error.sort_values(by=\"R2_test\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_bagging_absolute_error.head(30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c73af7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index ke 0 yang terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "211e2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kesimpulan dari Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39286231",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f159424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "RandomForestRegressor(\n",
      "    n_estimators=\u001b[32m100\u001b[39m,\n",
      "    *,\n",
      "    criterion=\u001b[33m'squared_error'\u001b[39m,\n",
      "    max_depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    min_samples_split=\u001b[32m2\u001b[39m,\n",
      "    min_samples_leaf=\u001b[32m1\u001b[39m,\n",
      "    min_weight_fraction_leaf=\u001b[32m0.0\u001b[39m,\n",
      "    max_features=\u001b[32m1.0\u001b[39m,\n",
      "    max_leaf_nodes=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    min_impurity_decrease=\u001b[32m0.0\u001b[39m,\n",
      "    bootstrap=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    oob_score=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    verbose=\u001b[32m0\u001b[39m,\n",
      "    warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    ccp_alpha=\u001b[32m0.0\u001b[39m,\n",
      "    max_samples=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    monotonic_cst=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "A random forest regressor.\n",
      "\n",
      "A random forest is a meta estimator that fits a number of decision tree\n",
      "regressors on various sub-samples of the dataset and uses averaging to\n",
      "improve the predictive accuracy and control over-fitting.\n",
      "Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
      "`splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
      "The sub-sample size is controlled with the `max_samples` parameter if\n",
      "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      "each tree.\n",
      "\n",
      "This estimator has native support for missing values (NaNs). During training,\n",
      "the tree grower learns at each split point whether samples with missing values\n",
      "should go to the left or right child, based on the potential gain. When predicting,\n",
      "samples with missing values are assigned to the left or right child consequently.\n",
      "If no missing values were encountered for a given feature during training, then\n",
      "samples with missing values are mapped to whichever child has the most samples.\n",
      "\n",
      "For a comparison between tree-based ensemble models see the example\n",
      ":ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
      "\n",
      "Read more in the :ref:`User Guide <forest>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "n_estimators : int, default=100\n",
      "    The number of trees in the forest.\n",
      "\n",
      "    .. versionchanged:: 0.22\n",
      "       The default value of ``n_estimators`` changed from 10 to 100\n",
      "       in 0.22.\n",
      "\n",
      "criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n",
      "    The function to measure the quality of a split. Supported criteria\n",
      "    are \"squared_error\" for the mean squared error, which is equal to\n",
      "    variance reduction as feature selection criterion and minimizes the L2\n",
      "    loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
      "    mean squared error with Friedman's improvement score for potential\n",
      "    splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
      "    the L1 loss using the median of each terminal node, and \"poisson\" which\n",
      "    uses reduction in Poisson deviance to find splits.\n",
      "    Training using \"absolute_error\" is significantly slower\n",
      "    than when using \"squared_error\".\n",
      "\n",
      "    .. versionadded:: 0.18\n",
      "       Mean Absolute Error (MAE) criterion.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "       Poisson criterion.\n",
      "\n",
      "max_depth : int, default=None\n",
      "    The maximum depth of the tree. If None, then nodes are expanded until\n",
      "    all leaves are pure or until all leaves contain less than\n",
      "    min_samples_split samples.\n",
      "\n",
      "min_samples_split : int or float, default=2\n",
      "    The minimum number of samples required to split an internal node:\n",
      "\n",
      "    - If int, then consider `min_samples_split` as the minimum number.\n",
      "    - If float, then `min_samples_split` is a fraction and\n",
      "      `ceil(min_samples_split * n_samples)` are the minimum\n",
      "      number of samples for each split.\n",
      "\n",
      "    .. versionchanged:: 0.18\n",
      "       Added float values for fractions.\n",
      "\n",
      "min_samples_leaf : int or float, default=1\n",
      "    The minimum number of samples required to be at a leaf node.\n",
      "    A split point at any depth will only be considered if it leaves at\n",
      "    least ``min_samples_leaf`` training samples in each of the left and\n",
      "    right branches.  This may have the effect of smoothing the model,\n",
      "    especially in regression.\n",
      "\n",
      "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "    - If float, then `min_samples_leaf` is a fraction and\n",
      "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "      number of samples for each node.\n",
      "\n",
      "    .. versionchanged:: 0.18\n",
      "       Added float values for fractions.\n",
      "\n",
      "min_weight_fraction_leaf : float, default=0.0\n",
      "    The minimum weighted fraction of the sum total of weights (of all\n",
      "    the input samples) required to be at a leaf node. Samples have\n",
      "    equal weight when sample_weight is not provided.\n",
      "\n",
      "max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
      "    The number of features to consider when looking for the best split:\n",
      "\n",
      "    - If int, then consider `max_features` features at each split.\n",
      "    - If float, then `max_features` is a fraction and\n",
      "      `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      "      split.\n",
      "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "    - If \"log2\", then `max_features=log2(n_features)`.\n",
      "    - If None or 1.0, then `max_features=n_features`.\n",
      "\n",
      "    .. note::\n",
      "        The default of 1.0 is equivalent to bagged trees and more\n",
      "        randomness can be achieved by setting smaller values, e.g. 0.3.\n",
      "\n",
      "    .. versionchanged:: 1.1\n",
      "        The default of `max_features` changed from `\"auto\"` to 1.0.\n",
      "\n",
      "    Note: the search for a split does not stop until at least one\n",
      "    valid partition of the node samples is found, even if it requires to\n",
      "    effectively inspect more than ``max_features`` features.\n",
      "\n",
      "max_leaf_nodes : int, default=None\n",
      "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "    Best nodes are defined as relative reduction in impurity.\n",
      "    If None then unlimited number of leaf nodes.\n",
      "\n",
      "min_impurity_decrease : float, default=0.0\n",
      "    A node will be split if this split induces a decrease of the impurity\n",
      "    greater than or equal to this value.\n",
      "\n",
      "    The weighted impurity decrease equation is the following::\n",
      "\n",
      "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                            - N_t_L / N_t * left_impurity)\n",
      "\n",
      "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "    if ``sample_weight`` is passed.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "\n",
      "bootstrap : bool, default=True\n",
      "    Whether bootstrap samples are used when building trees. If False, the\n",
      "    whole dataset is used to build each tree.\n",
      "\n",
      "oob_score : bool or callable, default=False\n",
      "    Whether to use out-of-bag samples to estimate the generalization score.\n",
      "    By default, :func:`~sklearn.metrics.r2_score` is used.\n",
      "    Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
      "    custom metric. Only available if `bootstrap=True`.\n",
      "\n",
      "    For an illustration of out-of-bag (OOB) error estimation, see the example\n",
      "    :ref:`sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "    context. ``-1`` means using all processors. See :term:`Glossary\n",
      "    <n_jobs>` for more details.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Controls both the randomness of the bootstrapping of the samples used\n",
      "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
      "    features to consider when looking for the best split at each node\n",
      "    (if ``max_features < n_features``).\n",
      "    See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "verbose : int, default=0\n",
      "    Controls the verbosity when fitting and predicting.\n",
      "\n",
      "warm_start : bool, default=False\n",
      "    When set to ``True``, reuse the solution of the previous call to fit\n",
      "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "    new forest. See :term:`Glossary <warm_start>` and\n",
      "    :ref:`tree_ensemble_warm_start` for details.\n",
      "\n",
      "ccp_alpha : non-negative float, default=0.0\n",
      "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "    subtree with the largest cost complexity that is smaller than\n",
      "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "    :ref:`minimal_cost_complexity_pruning` for details. See\n",
      "    :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
      "    for an example of such pruning.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\n",
      "max_samples : int or float, default=None\n",
      "    If bootstrap is True, the number of samples to draw from X\n",
      "    to train each base estimator.\n",
      "\n",
      "    - If None (default), then draw `X.shape[0]` samples.\n",
      "    - If int, then draw `max_samples` samples.\n",
      "    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
      "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\n",
      "monotonic_cst : array-like of int of shape (n_features), default=None\n",
      "    Indicates the monotonicity constraint to enforce on each feature.\n",
      "      - 1: monotonically increasing\n",
      "      - 0: no constraint\n",
      "      - -1: monotonically decreasing\n",
      "\n",
      "    If monotonic_cst is None, no constraints are applied.\n",
      "\n",
      "    Monotonicity constraints are not supported for:\n",
      "      - multioutput regressions (i.e. when `n_outputs_ > 1`),\n",
      "      - regressions trained on data with missing values.\n",
      "\n",
      "    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "\n",
      "    .. versionadded:: 1.4\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n",
      "    The child estimator template used to create the collection of fitted\n",
      "    sub-estimators.\n",
      "\n",
      "    .. versionadded:: 1.2\n",
      "       `base_estimator_` was renamed to `estimator_`.\n",
      "\n",
      "estimators_ : list of DecisionTreeRegressor\n",
      "    The collection of fitted sub-estimators.\n",
      "\n",
      "feature_importances_ : ndarray of shape (n_features,)\n",
      "    The impurity-based feature importances.\n",
      "    The higher, the more important the feature.\n",
      "    The importance of a feature is computed as the (normalized)\n",
      "    total reduction of the criterion brought by that feature.  It is also\n",
      "    known as the Gini importance.\n",
      "\n",
      "    Warning: impurity-based feature importances can be misleading for\n",
      "    high cardinality features (many unique values). See\n",
      "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_outputs_ : int\n",
      "    The number of outputs when ``fit`` is performed.\n",
      "\n",
      "oob_score_ : float\n",
      "    Score of the training dataset obtained using an out-of-bag estimate.\n",
      "    This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "    Prediction computed with out-of-bag estimate on the training set.\n",
      "    This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "estimators_samples_ : list of arrays\n",
      "    The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "    estimator. Each subset is defined by an array of the indices selected.\n",
      "\n",
      "    .. versionadded:: 1.4\n",
      "\n",
      "See Also\n",
      "--------\n",
      "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      "sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
      "    tree regressors.\n",
      "sklearn.ensemble.HistGradientBoostingRegressor : A Histogram-based Gradient\n",
      "    Boosting Regression Tree, very fast for big datasets (n_samples >=\n",
      "    10_000).\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The default values for the parameters controlling the size of the trees\n",
      "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "unpruned trees which can potentially be very large on some data sets. To\n",
      "reduce memory consumption, the complexity and size of the trees should be\n",
      "controlled by setting those parameter values.\n",
      "\n",
      "The features are always randomly permuted at each split. Therefore,\n",
      "the best found split may vary, even with the same training data,\n",
      "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "of the criterion is identical for several splits enumerated during the\n",
      "search of the best split. To obtain a deterministic behaviour during\n",
      "fitting, ``random_state`` has to be fixed.\n",
      "\n",
      "The default value ``max_features=1.0`` uses ``n_features``\n",
      "rather than ``n_features / 3``. The latter was originally suggested in\n",
      "[1], whereas the former was more recently justified empirically in [2].\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      ".. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      "       trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.ensemble import RandomForestRegressor\n",
      ">>> from sklearn.datasets import make_regression\n",
      ">>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "...                        random_state=0, shuffle=False)\n",
      ">>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      ">>> regr.fit(X, y)\n",
      "RandomForestRegressor(...)\n",
      ">>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "[-8.32987858]\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\asus\\anaconda3\\envs\\clustering_env_clean\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "?RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9d50531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Regressor Train ===\n",
      "MAE: 0.2216187499999999\n",
      "MAPE: 2.3757828201838116\n",
      "MSE: 0.10430314374999967\n",
      "RMSE: 0.32295997236499707\n",
      "R2: 0.995978480167719\n",
      "\n",
      "=== Random Forest Regressor Test ===\n",
      "MAE: 0.5898749999999986\n",
      "MAPE: 5.139579508864249\n",
      "MSE: 0.507632025\n",
      "RMSE: 0.7124829997971882\n",
      "R2: 0.9839171664348265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Buat model Random Forest\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Latih model\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred_rf = rf.predict(X_train_scaled)\n",
    "y_test_pred_rf = rf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluasi Train\n",
    "mse_train_rf = mean_squared_error(y_train, y_train_pred_rf)\n",
    "rmse_train_rf = sqrt(mse_train_rf)\n",
    "mae_train_rf = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "r2_train_rf = r2_score(y_train, y_train_pred_rf)\n",
    "mape_train_rf = np.mean(np.abs((y_train - y_train_pred_rf) / y_train)) * 100\n",
    "\n",
    "# Evaluasi Test\n",
    "mse_test_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "rmse_test_rf = sqrt(mse_test_rf)\n",
    "mae_test_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "r2_test_rf = r2_score(y_test, y_test_pred_rf)\n",
    "mape_test_rf = np.mean(np.abs((y_test - y_test_pred_rf) / y_test)) * 100\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"=== Random Forest Regressor Train ===\")\n",
    "print(\"MAE:\", mae_train_rf)\n",
    "print(\"MAPE:\", mape_train_rf)\n",
    "print(\"MSE:\", mse_train_rf)\n",
    "print(\"RMSE:\", rmse_train_rf)\n",
    "print(\"R2:\", r2_train_rf)\n",
    "\n",
    "print(\"\\n=== Random Forest Regressor Test ===\")\n",
    "print(\"MAE:\", mae_test_rf)\n",
    "print(\"MAPE:\", mape_test_rf)\n",
    "print(\"MSE:\", mse_test_rf)\n",
    "print(\"RMSE:\", rmse_test_rf)\n",
    "print(\"R2:\", r2_test_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6d811b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9b59b",
   "metadata": {},
   "source": [
    " n_estimators = 10 Ã— number_of_features (p)\n",
    "\n",
    " max_features =  \n",
    " \n",
    " -- Dataset kecil, fitur sedikit (pâ‰¤10) â†’ gunakan all features (max_features=p atau 1.0).\n",
    "\n",
    "---Dataset besar, fitur banyak (p>10) â†’ bisa dicoba:\n",
    "\n",
    "    ---max_features=\"sqrt\" â†’ âˆšp fitur per split\n",
    "\n",
    "    ---max_features=\"log2\" â†’ logâ‚‚(p) fitur per split\n",
    "\n",
    "    ---atau max_features=p/3 â†’ sekitar sepertiga fitur per split\n",
    "\n",
    "\n",
    " min_samples_leaf = mirip Decision Tree 1Â sampaiÂ 0.01Ã—n\n",
    "\n",
    " max_depth: log2â€‹(n)Â sampaiÂ log2â€‹(n)/2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bd1288a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# Rule of thumb range\n",
    "n_estimators = [30]                        # 10 * p\n",
    "max_features = 1.0                          # semua fitur\n",
    "min_samples_leaf_range = [1, 2]            # 1 sampai 0.01*n\n",
    "max_depth_range = [4, 5, 6, 7]             # log2(n)/2 sampai log2(n)\n",
    "\n",
    "# List untuk menyimpan hasil\n",
    "results_rf = []\n",
    "\n",
    "# Loop semua kombinasi\n",
    "for min_samples_leaf, max_depth in product(\n",
    "    min_samples_leaf_range,\n",
    "    max_depth_range\n",
    "):\n",
    "    # Buat model Random Forest\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_estimators[0],\n",
    "        max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Latih model\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prediksi\n",
    "    y_train_pred = rf.predict(X_train_scaled)\n",
    "    y_test_pred = rf.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluasi\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    \n",
    "    # Simpan hasil\n",
    "    results_rf.append({\n",
    "        'max_features': max_features,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'max_depth': max_depth,\n",
    "        'n_estimators': n_estimators[0],\n",
    "        'MAE_train': mae_train,\n",
    "        'MAE_test': mae_test,\n",
    "        'MAPE_train': mape_train,\n",
    "        'MAPE_test': mape_test,\n",
    "        'MSE_train': mse_train,\n",
    "        'MSE_test': mse_test,\n",
    "        'RMSE_train': rmse_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'R2_train': r2_train,\n",
    "        'R2_test': r2_test,\n",
    "        'R2_selisih': r2_train - r2_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "50eba96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_features</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>R2_selisih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>0.310368</td>\n",
       "      <td>0.665133</td>\n",
       "      <td>2.770114</td>\n",
       "      <td>6.144179</td>\n",
       "      <td>0.158917</td>\n",
       "      <td>0.671285</td>\n",
       "      <td>0.398644</td>\n",
       "      <td>0.819320</td>\n",
       "      <td>0.993873</td>\n",
       "      <td>0.978732</td>\n",
       "      <td>0.015140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>0.274648</td>\n",
       "      <td>0.685444</td>\n",
       "      <td>2.567562</td>\n",
       "      <td>6.158051</td>\n",
       "      <td>0.129295</td>\n",
       "      <td>0.698832</td>\n",
       "      <td>0.359577</td>\n",
       "      <td>0.835962</td>\n",
       "      <td>0.995015</td>\n",
       "      <td>0.977860</td>\n",
       "      <td>0.017155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>0.318106</td>\n",
       "      <td>0.697849</td>\n",
       "      <td>3.323722</td>\n",
       "      <td>6.318841</td>\n",
       "      <td>0.194478</td>\n",
       "      <td>0.751037</td>\n",
       "      <td>0.440997</td>\n",
       "      <td>0.866624</td>\n",
       "      <td>0.992502</td>\n",
       "      <td>0.976206</td>\n",
       "      <td>0.016296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>0.340780</td>\n",
       "      <td>0.696152</td>\n",
       "      <td>3.483729</td>\n",
       "      <td>6.354409</td>\n",
       "      <td>0.209358</td>\n",
       "      <td>0.756536</td>\n",
       "      <td>0.457557</td>\n",
       "      <td>0.869791</td>\n",
       "      <td>0.991928</td>\n",
       "      <td>0.976031</td>\n",
       "      <td>0.015897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.397670</td>\n",
       "      <td>0.721887</td>\n",
       "      <td>3.478786</td>\n",
       "      <td>6.542483</td>\n",
       "      <td>0.252561</td>\n",
       "      <td>0.782107</td>\n",
       "      <td>0.502555</td>\n",
       "      <td>0.884368</td>\n",
       "      <td>0.990262</td>\n",
       "      <td>0.975221</td>\n",
       "      <td>0.015041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.406064</td>\n",
       "      <td>0.728651</td>\n",
       "      <td>3.978652</td>\n",
       "      <td>6.652413</td>\n",
       "      <td>0.278394</td>\n",
       "      <td>0.848584</td>\n",
       "      <td>0.527631</td>\n",
       "      <td>0.921186</td>\n",
       "      <td>0.989266</td>\n",
       "      <td>0.973115</td>\n",
       "      <td>0.016151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>0.535763</td>\n",
       "      <td>0.833086</td>\n",
       "      <td>4.731694</td>\n",
       "      <td>7.278064</td>\n",
       "      <td>0.458792</td>\n",
       "      <td>1.153096</td>\n",
       "      <td>0.677342</td>\n",
       "      <td>1.073823</td>\n",
       "      <td>0.982311</td>\n",
       "      <td>0.963468</td>\n",
       "      <td>0.018843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>0.532529</td>\n",
       "      <td>0.842788</td>\n",
       "      <td>4.997587</td>\n",
       "      <td>7.422883</td>\n",
       "      <td>0.477297</td>\n",
       "      <td>1.184638</td>\n",
       "      <td>0.690867</td>\n",
       "      <td>1.088411</td>\n",
       "      <td>0.981597</td>\n",
       "      <td>0.962468</td>\n",
       "      <td>0.019129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_features  min_samples_leaf  max_depth  n_estimators  MAE_train  \\\n",
       "0           1.0                 1          6            30   0.310368   \n",
       "1           1.0                 1          7            30   0.274648   \n",
       "2           1.0                 2          7            30   0.318106   \n",
       "3           1.0                 2          6            30   0.340780   \n",
       "4           1.0                 1          5            30   0.397670   \n",
       "5           1.0                 2          5            30   0.406064   \n",
       "6           1.0                 1          4            30   0.535763   \n",
       "7           1.0                 2          4            30   0.532529   \n",
       "\n",
       "   MAE_test  MAPE_train  MAPE_test  MSE_train  MSE_test  RMSE_train  \\\n",
       "0  0.665133    2.770114   6.144179   0.158917  0.671285    0.398644   \n",
       "1  0.685444    2.567562   6.158051   0.129295  0.698832    0.359577   \n",
       "2  0.697849    3.323722   6.318841   0.194478  0.751037    0.440997   \n",
       "3  0.696152    3.483729   6.354409   0.209358  0.756536    0.457557   \n",
       "4  0.721887    3.478786   6.542483   0.252561  0.782107    0.502555   \n",
       "5  0.728651    3.978652   6.652413   0.278394  0.848584    0.527631   \n",
       "6  0.833086    4.731694   7.278064   0.458792  1.153096    0.677342   \n",
       "7  0.842788    4.997587   7.422883   0.477297  1.184638    0.690867   \n",
       "\n",
       "   RMSE_test  R2_train   R2_test  R2_selisih  \n",
       "0   0.819320  0.993873  0.978732    0.015140  \n",
       "1   0.835962  0.995015  0.977860    0.017155  \n",
       "2   0.866624  0.992502  0.976206    0.016296  \n",
       "3   0.869791  0.991928  0.976031    0.015897  \n",
       "4   0.884368  0.990262  0.975221    0.015041  \n",
       "5   0.921186  0.989266  0.973115    0.016151  \n",
       "6   1.073823  0.982311  0.963468    0.018843  \n",
       "7   1.088411  0.981597  0.962468    0.019129  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ubah jadi DataFrame\n",
    "df_rf = pd.DataFrame(results_rf)\n",
    "\n",
    "# Urutkan berdasarkan R2_test (model terbaik di atas)\n",
    "df_rf = df_rf.sort_values(by=\"R2_test\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_rf.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9459bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index ke 0 yg terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "efacc670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utk ini gak pakai boosting karena dataset dibawha 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f5caa17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dari beberapa tunning algoritma maka Bagging Regresor terbaik\n",
    "\n",
    "\n",
    "#Max depth 9, min_samples split 3, min_samples_leaf 1, max_features 3, criterion absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1715243e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train ===\n",
      "MAE : 0.2440\n",
      "MAPE: 2.51%\n",
      "MSE : 0.1159\n",
      "RMSE: 0.3405\n",
      "R2  : 0.9955\n",
      "\n",
      "=== Test ===\n",
      "MAE : 0.5917\n",
      "MAPE: 5.15%\n",
      "MSE : 0.4978\n",
      "RMSE: 0.7055\n",
      "R2  : 0.9842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Buat base estimator dengan hyperparameter spesifik\n",
    "base_tree = DecisionTreeRegressor(\n",
    "    max_depth=9,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=3,\n",
    "    criterion='absolute_error',  # bisa juga 'mae'\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Buat model Bagging dengan base estimator\n",
    "bagging_tree = BaggingRegressor(\n",
    "    estimator=base_tree,\n",
    "    n_estimators=100,  # jumlah pohon, bisa disesuaikan\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "bagging_tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_train_pred_bagging = bagging_tree.predict(X_train_scaled)\n",
    "y_test_pred_bagging = bagging_tree.predict(X_test_scaled)\n",
    "\n",
    "# Evaluasi\n",
    "def evaluate(y_true, y_pred, dataset_name='Dataset'):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f\"=== {dataset_name} ===\")\n",
    "    print(f\"MAE : {mae:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"MSE : {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2  : {r2:.4f}\\n\")\n",
    "\n",
    "evaluate(y_train, y_train_pred_bagging, 'Train')\n",
    "evaluate(y_test, y_test_pred_bagging, 'Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c1bb49d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil disimpan!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Simpan model Bagging Regressor yang sudah fit\n",
    "joblib.dump(bagging_tree, \"bagging_tree_terbaik.joblib\")\n",
    "print(\"Model berhasil disimpan!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a66a49ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_X.joblib']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Setelah fit scaler ke X_train\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Simpan scaler\n",
    "joblib.dump(scaler, \"scaler_X.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering_env_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
